"""
Known education-relevant benchmarks — a curated seed list.

This file provides a manually curated set of well-known benchmarks that are
relevant to education. These are always included in reports, even if the
HuggingFace scraper doesn't find them. You can add/remove entries here.
"""

from scraper import BenchmarkEntry

KNOWN_BENCHMARKS: list[BenchmarkEntry] = [
    # ── 1  General reasoning ─────────────────────────────────────────────
    BenchmarkEntry(
        name="MMLU (Massive Multitask Language Understanding)",
        source_url="https://huggingface.co/datasets/cais/mmlu",
        source_type="dataset",
        description="57 subjects across STEM, humanities, social sciences, and more. Tests breadth and depth of knowledge.",
        framework_ids=["1", "3.1"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["general", "knowledge", "multi-subject"],
    ),
    BenchmarkEntry(
        name="MMLU-Pro",
        source_url="https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro",
        source_type="dataset",
        description="Harder, expert-level version of MMLU with 10 answer choices and more reasoning-focused questions.",
        framework_ids=["1", "3.1"],
        tool_types=["ai_tutor"],
        tags=["general", "reasoning", "expert"],
    ),
    BenchmarkEntry(
        name="ARC (AI2 Reasoning Challenge)",
        source_url="https://huggingface.co/datasets/allenai/ai2_arc",
        source_type="dataset",
        description="Grade-school science questions requiring reasoning. Challenge set focuses on questions that simple retrieval fails on.",
        framework_ids=["1", "3.1"],
        tool_types=["ai_tutor", "pal"],
        tags=["reasoning", "science", "grade-school"],
    ),
    BenchmarkEntry(
        name="HellaSwag",
        source_url="https://huggingface.co/datasets/Rowan/hellaswag",
        source_type="dataset",
        description="Commonsense reasoning via sentence completion. Tests understanding of everyday situations.",
        framework_ids=["1"],
        tool_types=["ai_tutor"],
        tags=["commonsense", "reasoning"],
    ),
    BenchmarkEntry(
        name="GPQA (Graduate-Level Google-Proof QA)",
        source_url="https://huggingface.co/datasets/Idavidrein/gpqa",
        source_type="dataset",
        description="Expert-level science questions that are 'Google-proof' — require deep reasoning, not just retrieval.",
        framework_ids=["1", "3.1"],
        tool_types=["ai_tutor"],
        tags=["expert", "reasoning", "graduate-level"],
    ),
    BenchmarkEntry(
        name="BIG-Bench Hard (BBH)",
        source_url="https://huggingface.co/datasets/maveriq/bigbenchhard",
        source_type="dataset",
        description="23 challenging tasks from BIG-Bench where LLMs previously underperformed — logic, math, language understanding.",
        framework_ids=["1"],
        tool_types=["ai_tutor"],
        tags=["reasoning", "hard", "diverse"],
    ),

    # ── 2.3  Pedagogical interactions ────────────────────────────────────
    BenchmarkEntry(
        name="TutorEval",
        source_url="https://huggingface.co/papers/2402.08070",
        source_type="paper",
        description="Evaluates LLMs as tutors across multiple dimensions: pedagogical quality, accuracy, helpfulness in tutoring dialogues.",
        framework_ids=["2.2", "2.3"],
        tool_types=["ai_tutor"],
        tags=["tutoring", "pedagogy", "dialogue"],
    ),
    BenchmarkEntry(
        name="MathDial",
        source_url="https://huggingface.co/datasets/eth-nlped/mathdial",
        source_type="dataset",
        description="Math tutoring dialogues grounded in student misconceptions. Tests ability to identify and address student errors through dialogue.",
        framework_ids=["2.3", "4.2"],
        tool_types=["ai_tutor"],
        tags=["math", "tutoring", "dialogue", "misconception"],
    ),

    # ── 3.1  Content knowledge ───────────────────────────────────────────
    BenchmarkEntry(
        name="GSM8K",
        source_url="https://huggingface.co/datasets/openai/gsm8k",
        source_type="dataset",
        description="8.5K grade-school math word problems requiring multi-step reasoning. Widely used for math ability benchmarking.",
        framework_ids=["3.1", "1"],
        tool_types=["ai_tutor", "pal"],
        tags=["math", "grade-school", "word-problems"],
    ),
    BenchmarkEntry(
        name="MATH",
        source_url="https://huggingface.co/datasets/lighteval/MATH",
        source_type="dataset",
        description="12.5K competition-level math problems across 7 subjects (algebra, geometry, number theory, etc.).",
        framework_ids=["3.1"],
        tool_types=["ai_tutor"],
        tags=["math", "competition", "advanced"],
    ),
    BenchmarkEntry(
        name="SciQ",
        source_url="https://huggingface.co/datasets/allenai/sciq",
        source_type="dataset",
        description="13.7K science exam questions with supporting passages. Covers physics, chemistry, biology.",
        framework_ids=["3.1"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["science", "exam", "multiple-choice"],
    ),
    BenchmarkEntry(
        name="OpenBookQA",
        source_url="https://huggingface.co/datasets/allenai/openbookqa",
        source_type="dataset",
        description="Elementary science questions modelled after open-book exams — requires combining a core science fact with broad commonsense.",
        framework_ids=["3.1", "1"],
        tool_types=["ai_tutor"],
        tags=["science", "open-book", "commonsense"],
    ),
    BenchmarkEntry(
        name="MedQA",
        source_url="https://huggingface.co/datasets/bigbio/med_qa",
        source_type="dataset",
        description="USMLE-style medical exam questions. Tests medical content knowledge at professional level.",
        framework_ids=["3.1"],
        tool_types=["ai_tutor"],
        tags=["medical", "professional", "exam"],
    ),
    BenchmarkEntry(
        name="MAmmoTH",
        source_url="https://huggingface.co/datasets/TIGER-Lab/MathInstruct",
        source_type="dataset",
        description="Large-scale math instruction dataset combining CoT and PoT (program-of-thought) rationales across diverse math topics.",
        framework_ids=["3.1", "2.2"],
        tool_types=["ai_tutor", "pal"],
        tags=["math", "instruction", "chain-of-thought"],
    ),

    # ── 4.1  Scoring and grading ─────────────────────────────────────────
    BenchmarkEntry(
        name="ASAP (Automated Student Assessment Prize)",
        source_url="https://huggingface.co/datasets/mpalaval/ASAP_Essay_dataset",
        source_type="dataset",
        description="Essay scoring dataset from Kaggle competition. 8 prompt sets with human-scored essays on different rubrics.",
        framework_ids=["4.1"],
        tool_types=["teacher_support"],
        tags=["essay", "scoring", "grading", "rubric"],
    ),
    BenchmarkEntry(
        name="ASAP-SAS (Short Answer Scoring)",
        source_url="https://huggingface.co/datasets/marianna13/ASAP-SAS",
        source_type="dataset",
        description="Short answer scoring dataset — human-graded short responses to content-area questions.",
        framework_ids=["4.1"],
        tool_types=["teacher_support", "pal"],
        tags=["short-answer", "scoring", "grading"],
    ),

    # ── 4.2  Feedback with reasoning ─────────────────────────────────────
    BenchmarkEntry(
        name="FEEDBACKQA",
        source_url="https://huggingface.co/datasets/McGill-NLP/feedbackqa",
        source_type="dataset",
        description="QA dataset with interactive feedback. Tests ability to provide helpful feedback on incorrect answers.",
        framework_ids=["4.2"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["feedback", "QA", "interactive"],
    ),

    # ── 5  Ethics and bias ───────────────────────────────────────────────
    BenchmarkEntry(
        name="BBQ (Bias Benchmark for QA)",
        source_url="https://huggingface.co/datasets/heegyu/bbq",
        source_type="dataset",
        description="Tests social biases in QA across 11 categories (age, gender, race, disability, etc.).",
        framework_ids=["5"],
        tool_types=["ai_tutor", "pal", "teacher_support"],
        tags=["bias", "fairness", "social"],
    ),
    BenchmarkEntry(
        name="TruthfulQA",
        source_url="https://huggingface.co/datasets/truthfulqa/truthful_qa",
        source_type="dataset",
        description="Tests whether models generate truthful answers. Critical for education where misinformation is harmful.",
        framework_ids=["5", "3.1"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["truthfulness", "misinformation", "safety"],
    ),
    BenchmarkEntry(
        name="RealToxicityPrompts",
        source_url="https://huggingface.co/datasets/allenai/real-toxicity-prompts",
        source_type="dataset",
        description="100K prompts for measuring toxic language generation. Relevant to keeping educational content safe.",
        framework_ids=["5"],
        tool_types=["ai_tutor", "pal", "teacher_support"],
        tags=["toxicity", "safety", "content-moderation"],
    ),
    BenchmarkEntry(
        name="CrowS-Pairs",
        source_url="https://huggingface.co/datasets/BigScienceBiasEval/crows_pairs_multilingual",
        source_type="dataset",
        description="Stereotype detection benchmark covering 9 bias categories. Multilingual version available.",
        framework_ids=["5", "6.2"],
        tool_types=["ai_tutor", "pal", "teacher_support"],
        tags=["bias", "stereotypes", "multilingual"],
    ),

    # ── 6.1  Multimodal capabilities ─────────────────────────────────────
    BenchmarkEntry(
        name="MathVista",
        source_url="https://huggingface.co/datasets/AI4Math/MathVista",
        source_type="dataset",
        description="Math reasoning with visual context — charts, geometry, scientific figures. 6,141 examples from 28 datasets.",
        framework_ids=["6.1", "3.1"],
        tool_types=["ai_tutor", "pal"],
        tags=["multimodal", "math", "visual"],
    ),
    BenchmarkEntry(
        name="MMMU (Massive Multi-discipline Multimodal Understanding)",
        source_url="https://huggingface.co/datasets/MMMU/MMMU",
        source_type="dataset",
        description="11.5K expert-level multimodal questions across 30 subjects. Tests college-level understanding with images.",
        framework_ids=["6.1", "3.1", "1"],
        tool_types=["ai_tutor", "pal"],
        tags=["multimodal", "expert", "multi-subject"],
    ),
    BenchmarkEntry(
        name="ScienceQA",
        source_url="https://huggingface.co/datasets/derek-thomas/ScienceQA",
        source_type="dataset",
        description="21K multimodal science questions with lectures and explanations. Spans diverse science topics with images.",
        framework_ids=["6.1", "3.1", "2.2"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["multimodal", "science", "explanations"],
    ),

    # ── 6.2  Multilingual capabilities ───────────────────────────────────
    BenchmarkEntry(
        name="EXAMS",
        source_url="https://huggingface.co/datasets/mhardalov/exams",
        source_type="dataset",
        description="Multilingual high-school exam questions across 16 languages and 26 subjects. Tests cross-lingual educational knowledge.",
        framework_ids=["6.2", "3.1"],
        tool_types=["pal", "teacher_support"],
        tags=["multilingual", "exams", "high-school"],
    ),
    BenchmarkEntry(
        name="MGSM (Multilingual Grade School Math)",
        source_url="https://huggingface.co/datasets/juletxara/mgsm",
        source_type="dataset",
        description="GSM8K translated into 10 languages. Tests math reasoning across languages.",
        framework_ids=["6.2", "3.1"],
        tool_types=["ai_tutor", "pal"],
        tags=["multilingual", "math", "grade-school"],
    ),
    BenchmarkEntry(
        name="BELEBELE",
        source_url="https://huggingface.co/datasets/facebook/belebele",
        source_type="dataset",
        description="Reading comprehension benchmark in 122 languages. Tests multilingual understanding for education accessibility.",
        framework_ids=["6.2", "1"],
        tool_types=["pal"],
        tags=["multilingual", "reading-comprehension", "122-languages"],
    ),
    BenchmarkEntry(
        name="MMMLU (Multilingual MMLU)",
        source_url="https://huggingface.co/datasets/openai/MMMLU",
        source_type="dataset",
        description="Professional translations of MMLU into 14 languages. Evaluates whether educational knowledge transfers across languages.",
        framework_ids=["6.2", "3.1", "1"],
        tool_types=["ai_tutor", "pal"],
        tags=["multilingual", "knowledge", "multi-subject"],
    ),
    BenchmarkEntry(
        name="C-Eval",
        source_url="https://huggingface.co/datasets/ceval/ceval-exam",
        source_type="dataset",
        description="Chinese educational knowledge across 52 disciplines spanning middle school through professional levels.",
        framework_ids=["6.2", "3.1"],
        tool_types=["pal", "teacher_support"],
        tags=["multilingual", "chinese", "curriculum-aligned"],
    ),

    # ── 2.1  Pedagogical knowledge ───────────────────────────────────────
    BenchmarkEntry(
        name="EduBench",
        source_url="https://huggingface.co/datasets/DirectionAI/EduBench",
        source_type="dataset",
        description="Educational benchmark featuring Student-Oriented Scenarios (QA, Error Correction) across subjects and difficulty levels. Chinese and English.",
        framework_ids=["2.1", "2.2", "3.1"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["education", "pedagogy", "multi-subject", "bilingual"],
    ),
    BenchmarkEntry(
        name="FairytaleQA",
        source_url="https://huggingface.co/datasets/WorkInTheDark/FairytaleQA",
        source_type="dataset",
        description="10,580 QA pairs from 278 children's stories. Designed for educational question generation evaluation targeting narrative comprehension skills.",
        framework_ids=["2.2", "3.1"],
        tool_types=["teacher_support", "ai_tutor"],
        tags=["reading", "question-generation", "narrative", "children"],
    ),

    # ── 2.3  Pedagogical interactions (additional) ───────────────────────
    BenchmarkEntry(
        name="Socratic Benchmark",
        source_url="https://huggingface.co/datasets/koutch/socratic_benchmark",
        source_type="dataset",
        description="Tutor-student programming interactions with bug identification and Socratic dialogue. 3 versions covering code debugging pedagogy.",
        framework_ids=["2.3", "4.2"],
        tool_types=["ai_tutor"],
        tags=["Socratic", "tutoring", "programming", "dialogue", "feedback"],
    ),
    BenchmarkEntry(
        name="PACT Socratic Coding Tutor",
        source_url="https://huggingface.co/datasets/AndreiSobo/PACT-Socratic-Coding-Tutor",
        source_type="dataset",
        description="227 high-quality synthetic examples for fine-tuning LLMs on Socratic pedagogy in CS education, pairing student errors with Socratic hints.",
        framework_ids=["2.3", "2.2"],
        tool_types=["ai_tutor"],
        tags=["Socratic", "coding", "CS-education", "hints"],
    ),
    BenchmarkEntry(
        name="Engagement Socratic Rated",
        source_url="https://huggingface.co/datasets/Jennny/engagement-socratic-rated",
        source_type="dataset",
        description="9.3K Socratic dialogue examples with engagement quality ratings. Evaluates educational dialogue quality.",
        framework_ids=["2.3"],
        tool_types=["ai_tutor"],
        tags=["Socratic", "engagement", "rated", "dialogue"],
    ),
    BenchmarkEntry(
        name="FreedomIntelligence Socratic",
        source_url="https://huggingface.co/datasets/FreedomIntelligence/Socratic",
        source_type="dataset",
        description="34.4K multi-turn Socratic conversations between human and AI. Apache 2.0 licensed.",
        framework_ids=["2.3"],
        tool_types=["ai_tutor"],
        tags=["Socratic", "multi-turn", "dialogue"],
    ),

    # ── 3.1  Content knowledge (additional) ──────────────────────────────
    BenchmarkEntry(
        name="AGIEval",
        source_url="https://huggingface.co/datasets/baber/agieval",
        source_type="dataset",
        description="Performance on human-centric standardized exams (SAT, LSAT, GRE, GMAT, civil service, law, math competitions).",
        framework_ids=["3.1", "1"],
        tool_types=["ai_tutor", "pal"],
        tags=["exams", "standardized", "multi-subject"],
    ),
    BenchmarkEntry(
        name="TheoremQA",
        source_url="https://huggingface.co/datasets/wenhu/TheoremQA",
        source_type="dataset",
        description="800 questions requiring application of mathematical theorems. Covers math, physics, EE, CS, and finance.",
        framework_ids=["3.1"],
        tool_types=["ai_tutor"],
        tags=["theorems", "STEM", "advanced"],
    ),
    BenchmarkEntry(
        name="IFEval (Instruction-Following Eval)",
        source_url="https://huggingface.co/datasets/google/IFEval",
        source_type="dataset",
        description="Tests whether models can follow specific, verifiable instructions. Relevant to pedagogical formatting requirements.",
        framework_ids=["1", "2.2"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["instruction-following", "formatting"],
    ),
    BenchmarkEntry(
        name="WinoGrande",
        source_url="https://huggingface.co/datasets/allenai/winogrande",
        source_type="dataset",
        description="Large-scale Winograd Schema Challenge testing commonsense reasoning via pronoun resolution.",
        framework_ids=["1"],
        tool_types=["ai_tutor"],
        tags=["commonsense", "reasoning"],
    ),

    # ── 3.2  Content alignment ───────────────────────────────────────────
    BenchmarkEntry(
        name="RACE (Reading Comprehension)",
        source_url="https://huggingface.co/datasets/ehovy/race",
        source_type="dataset",
        description="28K passages and 100K questions from English learner exams. Grade-leveled content (middle school and high school).",
        framework_ids=["3.2", "3.1"],
        tool_types=["pal", "teacher_support"],
        tags=["reading", "grade-leveled", "curriculum"],
    ),
    BenchmarkEntry(
        name="Nigeria Learning Outcomes Mapping",
        source_url="https://huggingface.co/datasets/electricsheepafrica/nigeria-education-learning-outcomes-mapping",
        source_type="dataset",
        description="100K curriculum learning outcomes mapped to Bloom's taxonomy and assessment methods. Nigerian curriculum.",
        framework_ids=["3.2", "2.1"],
        tool_types=["teacher_support"],
        tags=["curriculum", "blooms-taxonomy", "learning-outcomes", "africa"],
    ),

    # ── 4.2  Feedback with reasoning (additional) ────────────────────────
    BenchmarkEntry(
        name="BEA 2019 GEC Shared Task",
        source_url="https://huggingface.co/datasets/wi_locness",
        source_type="dataset",
        description="Grammatical error correction dataset from English learner writing samples. Relevant to automated writing feedback.",
        framework_ids=["4.2", "4.1"],
        tool_types=["teacher_support", "ai_tutor"],
        tags=["grammar", "error-correction", "writing", "feedback"],
    ),

    # ── 5  Ethics and bias (additional) ──────────────────────────────────
    BenchmarkEntry(
        name="WinoBias",
        source_url="https://huggingface.co/datasets/wino_bias",
        source_type="dataset",
        description="Gender bias in coreference resolution related to professional roles. Relevant to gender equity in educational interactions.",
        framework_ids=["5"],
        tool_types=["ai_tutor", "pal", "teacher_support"],
        tags=["bias", "gender", "coreference"],
    ),
    BenchmarkEntry(
        name="BOLD (Bias in Open-Ended Language Generation)",
        source_url="https://huggingface.co/datasets/AlexaAI/bold",
        source_type="dataset",
        description="Tests for biased text generation across demographic groups. Relevant to equitable educational content generation.",
        framework_ids=["5"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["bias", "generation", "demographic"],
    ),
    BenchmarkEntry(
        name="HolisticBias",
        source_url="https://huggingface.co/datasets/facebook/holistic_bias",
        source_type="dataset",
        description="Nearly 600 descriptor terms across 13 demographic axes. Most comprehensive bias evaluation available.",
        framework_ids=["5"],
        tool_types=["ai_tutor", "pal", "teacher_support"],
        tags=["bias", "comprehensive", "demographic"],
    ),
    BenchmarkEntry(
        name="DecodingTrust",
        source_url="https://huggingface.co/datasets/AI-Secure/DecodingTrust",
        source_type="dataset",
        description="Multi-dimensional trustworthiness evaluation across toxicity, stereotype bias, robustness, fairness, privacy, and ethics.",
        framework_ids=["5"],
        tool_types=["ai_tutor", "pal", "teacher_support"],
        tags=["trustworthiness", "multi-dimensional", "safety"],
    ),

    # ── 6.1  Multimodal capabilities (additional) ────────────────────────
    BenchmarkEntry(
        name="AI2D (AI2 Diagrams)",
        source_url="https://huggingface.co/datasets/lmms-lab/ai2d",
        source_type="dataset",
        description="Questions about scientific/educational diagrams. Directly relevant to visual STEM education.",
        framework_ids=["6.1", "3.1"],
        tool_types=["ai_tutor", "pal"],
        tags=["multimodal", "diagrams", "science"],
    ),
    BenchmarkEntry(
        name="ChartQA",
        source_url="https://huggingface.co/datasets/ahmed-masry/ChartQA",
        source_type="dataset",
        description="Questions requiring understanding of charts and graphs. Relevant to data literacy education.",
        framework_ids=["6.1", "3.1"],
        tool_types=["ai_tutor", "teacher_support"],
        tags=["multimodal", "charts", "data-literacy"],
    ),

    # ── 6.2  Multilingual (additional) ───────────────────────────────────
    BenchmarkEntry(
        name="XCOPA",
        source_url="https://huggingface.co/datasets/xcopa",
        source_type="dataset",
        description="Causal reasoning across 11 languages. Tests whether educational AI can reason effectively in multiple languages.",
        framework_ids=["6.2", "1"],
        tool_types=["ai_tutor", "pal"],
        tags=["multilingual", "causal-reasoning", "11-languages"],
    ),
    BenchmarkEntry(
        name="KMMLU (Korean MMLU)",
        source_url="https://huggingface.co/datasets/HAERAE-HUB/KMMLU",
        source_type="dataset",
        description="Korean-language knowledge evaluation across academic subjects.",
        framework_ids=["6.2", "3.1"],
        tool_types=["pal"],
        tags=["multilingual", "korean", "knowledge"],
    ),
]
