# Critical Thinking & Higher-order Skills

**54 papers matched** (23 directly addressing this concern)

## Executive Summary

The literature reveals a significant and well-documented concern that AI tools, particularly LLMs like ChatGPT, risk undermining the development of critical thinking and higher-order cognitive skills in K-12 education. This concern manifests primarily through 'cognitive offloading' or 'metacognitive laziness'—where students delegate complex reasoning tasks to AI rather than engaging in the effortful thinking required for deep learning. Multiple empirical studies demonstrate that while AI can improve task performance and efficiency (e.g., essay scores, code generation), these gains often come at the expense of genuine understanding, knowledge transfer, and the development of analytical, evaluative, and creative capacities aligned with higher levels of Bloom's Taxonomy. The evidence suggests AI tools frequently provide direct answers rather than scaffolding the reasoning process, potentially creating dependency and limiting students' exposure to productive cognitive struggle.

However, the literature also identifies promising pedagogical approaches that can harness AI while preserving or enhancing critical thinking. These include: deliberate prompt engineering to elicit Socratic questioning rather than direct answers (extraheric AI, Pedagogical Chain-of-Thought); structured frameworks that create uncertainty to trigger analytical reasoning; hybrid human-AI systems where AI provides hints rather than solutions; and explicit instructional designs that position AI as a tool for metacognitive reflection rather than task completion. The key finding is that the impact on critical thinking is highly dependent on implementation design—AI can either replace or enhance higher-order thinking depending on how it is pedagogically integrated. LMICs face particular risks due to limited teacher capacity to design such sophisticated interventions, inadequate infrastructure for nuanced AI deployment, and pressure to use AI for efficiency rather than learning quality.

## Key Findings

### AI usage leads to cognitive offloading where students delegate higher-order thinking to the system, reducing engagement in analytical reasoning, synthesis, and evaluation

*Evidence type: empirical | 18 papers*

- Beware of Metacognitive Laziness: Effects of Generative Artificial Intelligence on Learning Motivation, Processes, and Performance
- extraheric AI: A conceptual framework for human-AI interaction that fosters higher-order thinking skills
- The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding
- Challenges and opportunities for classroom-based formative assessment and AI
- Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications

### AI tools improve surface-level task performance (scores, productivity) but do not enhance knowledge transfer, deep understanding, or application to novel contexts

*Evidence type: empirical | 7 papers*

- Beware of Metacognitive Laziness
- GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers
- Are Large Language Models Good Essay Graders?
- Does Multiple Choice Have a Future in the Age of Generative AI?

### When students use AI as intended (for hints, scaffolding, Socratic questioning), critical thinking can be maintained or enhanced, but only with explicit pedagogical design

*Evidence type: empirical | 12 papers*

- A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI
- Dialogic Pedagogy for Large Language Models
- LLMs can Find Mathematical Reasoning Mistakes by Pedagogical Chain-of-Thought
- Generative AI as a Tool for Enhancing Reflective Learning in Students
- Strategies for Creating Uncertainty in the AI Era to Trigger Students' Critical Thinking

### Current AI systems primarily support lower-level cognitive skills (remembering, understanding) and struggle to effectively scaffold higher-order skills (analyzing, evaluating, creating) aligned with Bloom's Taxonomy

*Evidence type: empirical | 9 papers*

- Assessing AI-Generated Questions' Alignment with Cognitive Frameworks
- Classifications of the Summative Assessment for Revised Bloom's Taxonomy by using Deep Learning
- A Novel Psychometrics-Based Approach to Developing Professional Competency Benchmark
- Docimological Quality Analysis of LLM-Generated Multiple Choice Questions

### Teachers lack training and frameworks to design AI-integrated learning that preserves critical thinking, leading to implementation that prioritizes efficiency over cognitive depth

*Evidence type: empirical | 8 papers*

- Exploring User Perspectives on ChatGPT
- How can we learn and use AI at the same time?: Participatory Design of GenAI with High School Students
- Using ChatGPT for Science Learning: A Study on Pre-service Teachers' Lesson Planning
- Potentials of ChatGPT in Computer Programming: Insights from Programming Instructors

### AI grading systems can reliably assess lower-order skills but struggle to evaluate critical thinking, argumentation quality, and reasoning processes without extensive human calibration

*Evidence type: empirical | 6 papers*

- Beyond Holistic Scores: Automatic Trait-Based Quality Scoring of Argumentative Essays
- Automated Evaluation for Student Argumentative Writing: A Survey
- AI-Enabled grading with near-domain data
- Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions

## Evidence For This Risk

- Multiple controlled experiments demonstrate that students using ChatGPT show significantly lower knowledge transfer to novel problems despite achieving higher scores on immediate tasks (Beware of Metacognitive Laziness study: no improvement in knowledge gain/transfer tests despite 20% better essay scores)
- Process mining analysis reveals students using AI engage less in metacognitive processes (monitoring, evaluation, planning) compared to those receiving human expert guidance, with AI groups showing closed loops centered on AI interaction rather than distributed metacognitive engagement
- Student self-reports consistently indicate concerns about over-reliance, reduced critical thinking, and becoming 'lazy' when using AI (21.4% of studies in systematic review; multiple qualitative studies with K-12 and undergraduate students)
- When AI provides direct answers rather than hints, students skip intermediate reasoning steps and fail to develop problem-solving strategies, as documented in mathematics and programming education studies
- GSM-Plus benchmark shows AI performance drops 20% when problems require deeper reasoning or are slightly modified, while human performance remains stable—suggesting AI shortcuts surface-level pattern matching rather than genuine understanding
- Eye-tracking and temporal analysis studies show students spend less time on deliberative reasoning and more time on AI interaction when AI is available, indicating cognitive offloading
- Teachers report observing decreased student engagement with challenging tasks and increased expectation for immediate answers when AI tools are introduced without pedagogical guardrails
- Automated assessment systems aligned with Bloom's Taxonomy show AI-generated questions cluster at lower cognitive levels (remember, understand) with poor discrimination at higher levels (analyze, evaluate, create)

## Mitigating Evidence

- Deliberate prompt engineering strategies (Socratic questioning, hint-based systems, uncertainty creation) can maintain or enhance critical thinking while leveraging AI efficiency
- Studies using 'extraheric AI' framework show that when AI poses questions rather than provides answers, students demonstrate improved metacognitive awareness and reasoning quality
- Enhanced Cognitive Scaffolding approach with progressive autonomy (high AI support initially, gradually reduced) shows learning gains without dependency when properly implemented
- Hybrid human-AI systems where teachers use AI-generated materials as starting points for discussion (rather than endpoints) maintain critical thinking while reducing preparation time
- Formative assessment integrated with AI (where AI provides process feedback, not just answers) can support self-regulated learning and metacognition when designed with pedagogical intent
- Student agency frameworks show that when students are explicitly taught to critically evaluate AI outputs and use AI as one information source among many, over-reliance risks diminish
- Evidence from programming education shows that when AI is positioned as 'debugging partner' rather than 'code generator,' students maintain learning of core concepts
- Classroom implementations requiring students to justify AI-generated content and demonstrate understanding through peer explanation show preservation of higher-order skills
- Cultural contexts where collaborative learning is norm (vs. individual competition) show better integration of AI as collective thinking tool rather than shortcut

## What Is Being Measured

- Knowledge transfer tasks (applying learned concepts to novel problems without AI support)
- Self-reported intrinsic motivation and perceived competence (IMI scale)
- Process mining of learning behaviors (time spent on different cognitive activities, transitions between tasks)
- Pre-post knowledge gain tests comparing intervention vs. control groups
- Bloom's Taxonomy alignment of questions/assessments generated by AI vs. humans
- Essay quality traits (argument structure, coherence, critical analysis) in AI-assisted vs. non-AI writing
- Metacognitive strategy use (planning, monitoring, evaluation) through protocol analysis and trace data
- Item difficulty and discrimination parameters using Item Response Theory for AI vs. human-created assessments
- Student engagement metrics (time-on-task, interaction patterns, help-seeking behavior)
- Qualitative analysis of student work products for evidence of reasoning vs. surface-level reproduction
- Teacher and student perceptions through surveys, interviews, and focus groups
- Performance on diagnostic questions requiring multi-step reasoning, counterintuitive thinking, or integration of concepts

## Gaps — What Is NOT Being Measured

- Long-term longitudinal effects of AI use on critical thinking development across school years or courses
- Transfer of critical thinking skills across domains (e.g., from AI-assisted mathematics to science reasoning)
- Fine-grained cognitive load measures during AI interaction to distinguish productive vs. unproductive cognitive offloading
- Development of epistemic cognition (understanding of knowledge sources, evaluation criteria, justification standards) with/without AI
- Creativity and divergent thinking outcomes beyond immediate task performance
- Metacognitive calibration (accuracy of self-assessment) and how AI affects students' ability to judge their own understanding
- Sociocultural factors affecting AI use patterns (family educational background, peer norms, cultural attitudes toward technology and authority)
- Differential impacts by prior achievement level, learning disabilities, language proficiency
- How teacher instructional moves mediate AI effects on thinking (vs. just measuring AI presence/absence)
- Authentic problem-solving in messy, ill-defined contexts (most studies use well-structured tasks)
- Development of argumentation skills specifically—distinguishing claims from evidence, evaluating source credibility, considering counterarguments
- How AI affects disposition toward critical thinking (curiosity, open-mindedness, intellectual humility) beyond skill measurement
- Cost-effectiveness and scalability trade-offs between AI efficiency and learning quality in resource-constrained settings

## Context Factors

- Age/grade level: Elementary students show more susceptibility to AI over-reliance than older students with established metacognitive strategies
- Subject domain: Mathematics and programming show different patterns than essay writing; procedural domains vs. conceptual understanding
- Task complexity and structure: Well-defined tasks (computation) vs. ill-defined tasks (argumentation, design)
- Pedagogical design: Direct answer provision vs. hint-based systems vs. Socratic questioning approaches
- Prior achievement: Low-performing students may benefit from AI scaffolding but also face greater dependency risks
- Teacher training and beliefs: Teachers' pedagogical knowledge and attitudes toward technology shape implementation quality
- Implementation context: Formative vs. summative assessment; in-class vs. homework; individual vs. collaborative use
- AI system design: Generic chatbots vs. purpose-built educational tools; degree of transparency and explainability
- Instructional scaffolding: Whether students receive explicit guidance on how to use AI as thinking tool vs. answer source
- Assessment alignment: Whether tests measure AI-assisted performance or independent capability
- Language and cultural context: English vs. non-English materials; Western vs. non-Western educational cultures
- Socioeconomic context: Access to devices, connectivity, home support for learning; opportunity costs of 'productive struggle'
- Time pressure: High-stakes testing environments vs. low-stakes practice contexts
- Peer dynamics: Competitive vs. collaborative classroom cultures; social norms around AI use

## Notable Studies

### Beware of Metacognitive Laziness: Effects of Generative Artificial Intelligence on Learning Motivation, Processes, and Performance

**Design:** Randomized controlled experiment with 117 university students comparing ChatGPT, human expert, checklist tools, and control groups on essay writing task with knowledge gain/transfer tests
**Sample:** 117 university students, writing course context, China
**Key result:** ChatGPT group achieved 20% higher essay scores but showed no improvement in knowledge gain or transfer tests compared to control; process mining revealed less metacognitive engagement (monitoring, evaluation, planning) in AI group

### extraheric AI: A conceptual framework for human-AI interaction that fosters higher-order thinking skills

**Design:** Conceptual framework development based on cognitive load theory and Bloom's Taxonomy, proposing interaction strategies where AI poses questions rather than provides answers
**Sample:** Framework paper with design principles; no empirical sample
**Key result:** Framework demonstrates how AI can be designed to draw forth higher-order thinking through question-posing and alternative perspectives rather than direct answers, preventing cognitive offloading

### Strategies for Creating Uncertainty in the AI Era to Trigger Students' Critical Thinking: Pedagogical Design, Assessment Rubric, and Exam System

**Design:** Design-based research developing MindMosaicAIExam system that deliberately uses AI limitations and controlled behavior to create uncertainty requiring critical evaluation
**Sample:** System design and theoretical framework; testing with university students
**Key result:** System architecture where AI is instructed to provide plausible but incorrect explanations or incomplete answers, forcing students to reason about outputs; includes rubric for evaluating critical thinking through reasoning artifacts

### GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers

**Design:** Benchmark creation with 10,552 math problem variations testing whether LLMs rely on shortcuts vs. genuine reasoning; tests 25 LLMs with 4 prompting techniques
**Sample:** Grade school math problems (GSM8K extended); 25 different LLMs tested
**Key result:** LLM accuracy drops up to 20% when problems slightly modified while human performance stable; AI shows surface-level pattern matching rather than deep mathematical understanding

### Using ChatGPT for Science Learning: A Study on Pre-service Teachers' Lesson Planning

**Design:** Analysis of 29 pre-service teachers' lesson plans integrating ChatGPT into science education, evaluated using TPACK-based rubric and qualitative interviews
**Sample:** 29 pre-service elementary teachers, Korea, science education context
**Key result:** Teachers envisioned broad applicability but scored low on 'ChatGPT function selection' (3.0/4); expressed concerns about student over-dependence and need for systematic methods to prevent cognitive offloading

### Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted

**Design:** Developed AnaQuest prompting technique using students' formative assessment responses to inform MCQ generation; evaluated using Item Response Theory comparing AI vs. instructor-created questions
**Sample:** 586 college students' exam responses; HCI course context
**Key result:** AI-generated questions with student misconception hints showed better psychometric properties (discrimination, difficulty) than baseline ChatGPT, closely resembling instructor-created items

## Implications for LMICs

LMICs face compounded risks regarding critical thinking erosion due to: (1) Larger class sizes and lower teacher-student ratios making individualized scaffolding of AI use infeasible; (2) Limited teacher professional development capacity to train educators in sophisticated AI-integrated pedagogy; (3) Pressure to use AI for efficiency gains (grading, content generation) rather than learning quality due to resource constraints; (4) Weaker digital literacy among both teachers and students, reducing ability to critically evaluate AI outputs; (5) Infrastructure limitations (connectivity, devices) pushing toward offline or asynchronous AI use that lacks real-time teacher mediation; (6) Assessment systems emphasizing rote learning and examination performance over higher-order skills, making AI shortcuts more attractive; (7) Language barriers where AI trained primarily on English may reinforce surface-level translation rather than conceptual understanding in local languages; (8) Absence of culturally-appropriate pedagogical materials that scaffold critical thinking through AI; (9) Risk that AI access becomes marker of privilege, with well-resourced students receiving 'AI with scaffolding' while under-resourced students receive 'AI as answer key'; (10) Policy environments lacking regulatory frameworks or quality standards for educational AI, allowing deployment without evaluation of learning impacts. However, LMICs may also benefit from 'leapfrogging' to AI-integrated pedagogies if properly designed—avoiding entrenchment of traditional teacher-centered methods. The key is ensuring AI enhances rather than replaces already-scarce opportunities for student-centered, inquiry-based learning that develops higher-order thinking.

## Recommendations

- Design AI interactions that require student justification, explanation, and critique of AI outputs rather than passive acceptance (e.g., 'extraheric AI' framework where AI poses questions)
- Implement progressive autonomy scaffolding where AI support is high initially but systematically reduced as students develop competence, preventing long-term dependency
- Develop teacher professional development focused specifically on orchestrating AI use to preserve critical thinking—including exemplar lessons, rubrics for evaluating AI-mediated thinking, and classroom discussion protocols
- Create assessment systems that explicitly test knowledge/skills without AI access, ensuring students develop independent capability alongside AI-assisted work
- Adopt 'uncertainty creation' strategies where AI is deliberately configured to provide incomplete or multiple perspectives requiring student evaluation rather than single correct answers
- Build in structured reflection activities where students document their reasoning process, compare their approach to AI's, and evaluate strengths/limitations of AI-generated solutions
- Use AI for formative feedback that guides process (hints, questions, alternative framings) rather than summative answers, with human teachers handling final evaluation
- Implement 'dialogic pedagogy' frameworks where AI serves as conversation partner for exploring ideas rather than authoritative answer source
- Establish clear classroom norms and policies about when/how AI should be used, explicitly teaching students to recognize tasks requiring independent vs. AI-assisted thinking
- Prioritize development of AI literacy curricula that teach students to critically evaluate AI capabilities, limitations, biases, and appropriate use contexts
- Conduct ongoing longitudinal assessment of students' independent problem-solving, reasoning, and critical thinking abilities to detect early signs of skill erosion
- Design AI systems with built-in scaffolding features (progressive hint systems, Socratic questioning, requirement to show work) rather than relying on teachers to retrofit generic chatbots
- For LMICs specifically: Develop low-bandwidth AI tools designed for offline/asynchronous use with pedagogical guardrails built in; create teacher guides with minimal-training protocols for AI integration; establish national standards for educational AI that prioritize learning quality over efficiency

## Top Papers

1. **Beware of Metacognitive Laziness: Effects of Generative Artificial Intelligence on Learning Motivation, Processes, and Performance**
   Rigorous RCT with process mining showing concrete evidence of cognitive offloading and knowledge transfer failure despite performance gains

2. **extraheric AI: A conceptual framework for human-AI interaction that fosters higher-order thinking skills**
   Provides actionable framework grounded in learning theory for designing AI that enhances rather than replaces critical thinking

3. **The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox**
   Synthesizes theory and evidence on the fundamental tension between AI convenience and cognitive development with proposed resolution

4. **GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers**
   Demonstrates empirically that LLMs rely on shortcuts rather than reasoning, validating concerns about surface-level learning

5. **A Practical Guide for Supporting Formative Assessment and Feedback Using Generative AI**
   Offers concrete, theory-grounded strategies for using AI to support rather than replace higher-order thinking in assessment

6. **Strategies for Creating Uncertainty in the AI Era to Trigger Students' Critical Thinking**
   Innovative approach using AI limitations deliberately to create cognitive demand and develops assessment rubric for critical thinking

7. **Using ChatGPT for Science Learning: A Study on Pre-service Teachers' Lesson Planning**
   Reveals teacher preparation gaps and provides evidence of how educators conceptualize (and struggle with) preserving critical thinking when integrating AI

8. **Exploring User Perspectives on ChatGPT: Applications, Perceptions, and Implications for AI-Integrated Education**
   Large-scale qualitative analysis of real user concerns showing consensus about over-reliance and critical thinking erosion across stakeholder groups


---

# Critical Thinking & Higher-order Skills

**11 papers matched** (5 directly addressing this concern)

## Executive Summary

The literature reveals a significant and well-documented concern that AI tools reduce opportunities for students to develop critical thinking, analysis, evaluation, and creative problem-solving skills. Five papers directly address this risk through empirical studies or systematic reviews, while six others encounter it as a secondary finding or theoretical consideration. The evidence suggests a dual mechanism of harm: first, AI systems enable cognitive offloading where students bypass effortful thinking by accepting AI-generated solutions without question; second, when AI is integrated into educational tasks, it fundamentally changes the cognitive demands, often reducing tasks to surface-level pattern recognition rather than deep analytical engagement. This manifests across multiple educational levels and contexts, from primary school mathematics to university-level writing.

The most robust evidence comes from systematic reviews and mixed-methods studies examining student behavior with AI dialogue systems like ChatGPT. Researchers consistently find that over-reliance on AI correlates with reduced engagement in critical analysis, decreased effort in constructing logical arguments, and diminished capacity for independent problem-solving. However, the literature also reveals important nuances: the impact varies significantly based on how AI is implemented pedagogically, the specific cognitive level targeted (per Bloom's taxonomy), and whether students receive explicit instruction in AI literacy. Notably, several studies demonstrate that AI can support higher-order thinking when used as a scaffolding tool rather than a replacement for cognitive effort, suggesting that pedagogical design is the critical mediating factor.

## Key Findings

### Over-reliance on AI dialogue systems leads to reduced critical thinking, analytical reasoning, and decision-making abilities in students

*Evidence type: empirical | 3 papers*

- The effects of over-reliance on AI dialogue systems on students' cognitive abilities: a systematic review
- Impact of ChatGPT on Student's Education: A Comprehensive analysis of positive and negative effects
- Embracing AI in English Composition: Insights and Innovations in Hybrid Pedagogical Practices

### AI tools reduce student engagement with course materials and discourage independent research and problem-solving

*Evidence type: empirical | 2 papers*

- Impact of ChatGPT on Student's Education: A Comprehensive analysis of positive and negative effects
- The effects of over-reliance on AI dialogue systems on students' cognitive abilities: a systematic review

### Students prefer fast, optimal AI-generated solutions over slower, effortful cognitive processes, even when aware of ethical issues

*Evidence type: empirical | 1 papers*

- The effects of over-reliance on AI dialogue systems on students' cognitive abilities: a systematic review

### AI-generated content exhibits lower originality in claims and arguments compared to human-generated content, particularly at higher cognitive levels

*Evidence type: empirical | 2 papers*

- Argument Rarity-based Originality Assessment for AI-Assisted Writing
- Co-Designing Interdisciplinary Design Projects with AI

### Current LLMs can generate questions at different Bloom's taxonomy levels but struggle to create genuinely higher-order cognitive challenges

*Evidence type: empirical | 3 papers*

- Automated Educational Question Generation at Different Bloom's Skill Levels Using Large Language Models
- Generating AI Literacy MCQs: A Multi-Agent LLM Approach
- BloomNet: A Robust Transformer based model for Bloom's Learning Outcome Classification

### Intelligent tutoring systems with AI can improve prediction accuracy but primarily focus on lower-order skills like recall and comprehension

*Evidence type: empirical | 1 papers*

- Enhancement of Prediction Model for Students' Performance in Intelligent Tutoring System

### Quality and originality represent a trade-off in AI-assisted writing, with higher-quality AI outputs exhibiting substantially lower claim originality

*Evidence type: empirical | 1 papers*

- Argument Rarity-based Originality Assessment for AI-Assisted Writing

## Evidence For This Risk

- Systematic review of 14 studies found that over-reliance on AI dialogue systems impacts cognitive abilities including decision-making, critical thinking, and analytical reasoning across educational contexts
- Survey of 150 students found 44.7% reported AI impacted their thinking skills, 36% confirmed it discouraged independent problem-solving, and 56% only sometimes verified AI-generated information
- Computational analysis revealed AI-generated essays had claim rarity approximately one-fifth that of human essays, demonstrating LLMs reproduce argumentative forms but lack content originality
- Strong negative correlation (r = -0.67) between text quality and claim rarity indicates a fundamental quality-originality trade-off where AI optimizes for typical patterns at the expense of original thinking
- Students increasingly favor 'fast and optimal solutions over slow ones constrained by practicality,' demonstrating preference for cognitive shortcuts over effortful reasoning
- 46% of surveyed students reported over-dependence on AI for learning tasks, with reduced creativity and innovation noted as consequences
- Teachers observed that students using AI become 'too dependent on AI-generated information,' undermining capacity for independent critical thinking and logical argument construction
- AI-assisted project plans scored significantly lower on interdisciplinary integration compared to manual planning (p = .623), suggesting surface-level connections rather than deep conceptual synthesis

## Mitigating Evidence

- AI-assisted planning can support higher-order skills when properly scaffolded: project plans scored significantly higher on Design Thinking Application (p = .005, d = 0.81) and Coherence & Flow (p = .035, d = 0.64)
- 58% of students reported ChatGPT positively affected their academic performance, suggesting potential benefits when used appropriately
- Adaptive scheduling algorithms that optimize practice difficulty can enhance memory retention and learning efficiency without reducing cognitive engagement
- Teachers can successfully integrate AI tools while maintaining focus on critical thinking when explicit pedagogical frameworks guide implementation
- Custom GPT models with pedagogical grounding (e.g., Design Innovation framework, curriculum alignment) can structure complex interdisciplinary planning without replacing teacher expertise
- AI tools can serve as 'structural aids' and provide a 'strong backbone to be refined manually,' supporting rather than replacing critical thinking when positioned as scaffolding
- Explicit AI literacy education and transparent guidelines can help students develop metacognitive awareness about appropriate AI use

## What Is Being Measured

- Student self-reported impact on thinking skills and problem-solving behavior (survey-based)
- Verification rates for AI-generated information (frequency of fact-checking)
- Engagement levels with course materials (self-report and behavioral tracking)
- Argument originality measured through structural rarity, claim rarity, evidence rarity, and cognitive depth metrics
- Bloom's taxonomy classification accuracy for AI-generated questions (Remember, Understand, Apply, Analyze, Evaluate, Create)
- Rubric-based assessment of interdisciplinary project plans across six dimensions including Design Thinking Application
- Cognitive load and practice efficiency using computational memory models
- Question quality across ten pedagogical criteria including cognitive level appropriateness
- Confidence parameters in knowledge state predictions (P(C) in Bayesian Knowledge Tracing)
- Over-reliance indicators through user acceptance of AI recommendations without critical evaluation

## Gaps — What Is NOT Being Measured

- Longitudinal developmental trajectories of critical thinking skills with sustained AI use over months or years
- Transfer effects: whether reduced critical thinking in AI-assisted tasks generalizes to non-AI contexts
- Cognitive mechanisms: specific brain regions or processing pathways affected by AI offloading
- Threshold effects: at what point does AI assistance become cognitively harmful vs. beneficial
- Domain-specific impacts: whether critical thinking degradation varies by subject area (STEM vs. humanities)
- Intervention effectiveness: which pedagogical strategies successfully prevent cognitive skill erosion
- Metacognitive development: students' ability to recognize when they should vs. shouldn't use AI
- Creative synthesis abilities independent of originality metrics (non-pattern-based innovation)
- Collaborative critical thinking when AI is present in group work contexts
- Recovery trajectories: whether critical thinking skills rebound after AI tools are removed
- Age-specific vulnerabilities: differential impacts across developmental stages (primary vs. secondary vs. tertiary)
- Cultural and linguistic factors affecting critical thinking development with AI in non-English contexts

## Context Factors

- Educational level: Impact varies from primary school (basic recall) to university (complex synthesis), with primary students potentially more vulnerable
- Subject domain: STEM tasks may be more susceptible to procedural shortcuts than open-ended humanities tasks requiring interpretation
- AI tool type: Conversational AI (ChatGPT) vs. specialized tutoring systems vs. writing assistants have different cognitive impacts
- Pedagogical framing: Whether AI is positioned as replacement vs. scaffolding tool fundamentally changes outcomes
- Task cognitive level: Bloom's taxonomy level targeted (lower-order Remember/Understand vs. higher-order Analyze/Evaluate/Create)
- Implementation context: Adaptive vs. fixed scheduling, with vs. without explicit AI literacy instruction
- Assessment design: Whether evaluation focuses on quality (which AI excels at) vs. originality (where AI is weaker)
- Student characteristics: Prior knowledge, metacognitive skills, and tendency toward cognitive shortcuts
- Verification behavior: Whether students are prompted to fact-check or simply accept AI outputs
- Language and culture: Questions in local languages/cultures show particularly poor AI performance, potentially affecting equity
- Time pressure: Students under time constraints more likely to accept AI solutions without critical evaluation
- Feedback mechanisms: Whether students receive information about the quality-originality trade-off in AI outputs

## Notable Studies

### The effects of over-reliance on AI dialogue systems on students' cognitive abilities: a systematic review

**Design:** Systematic review using PRISMA guidelines, analyzing 14 studies from ProQuest, IEEE Xplore, ScienceDirect, and Web of Science examining relationships between ethical AI issues and cognitive offloading
**Sample:** 14 studies spanning educational and research contexts, primarily higher education students
**Key result:** Over-reliance stemming from ethical issues (hallucinations, biases, privacy concerns) impacts cognitive abilities as users prefer efficient cognitive shortcuts over effortful reasoning, even amid AI's ethical problems

### Impact of ChatGPT on Student's Education: A Comprehensive analysis of positive and negative effects

**Design:** Mixed-methods survey study with quantitative questions on usage patterns, verification behavior, and self-reported cognitive impacts
**Sample:** 150 college students who use ChatGPT as educational tool
**Key result:** 44.7% of students reported AI impacted their thinking skills; 36% reported it discouraged independent research; only 34.7% always verify AI-generated information; 46% showed neutral stance on over-dependence

### Argument Rarity-based Originality Assessment for AI-Assisted Writing

**Design:** Computational framework (AROA) using density estimation to measure structural rarity, claim rarity, evidence rarity, and cognitive depth; comparison of 1,375 human essays vs. 1,000 AI essays
**Sample:** 1,375 human student essays and 1,000 AI-generated essays on same topics
**Key result:** Strong negative correlation (r = -0.67) between quality and claim rarity; AI essays achieved comparable structural complexity but claim rarity was approximately one-fifth that of humans, revealing quality-originality trade-off

### Co-Designing Interdisciplinary Design Projects with AI

**Design:** Workshop with 33 in-service secondary teachers comparing AI-assisted (IDPplanner GPT tool) vs. manual project planning using six-dimensional pedagogical rubric including Design Thinking Application
**Sample:** 33 in-service secondary school teachers in Singapore, creating interdisciplinary design projects
**Key result:** AI-assisted plans scored significantly higher on Design Thinking Application (p = .005, d = 0.81) and Coherence but lower on Interdisciplinary Integration, suggesting AI supports structure but may limit deep conceptual connections

### Embracing AI in English Composition: Insights and Innovations in Hybrid Pedagogical Practices

**Design:** Mixed-methods study across three college composition courses tracking student attitudes, usage patterns, and writing artifacts; development of best practices guide for AI integration
**Sample:** 28 students across three sections of college English Composition I and II courses, Fall 2023
**Key result:** Shift from initial anxiety to practical application; students could effectively use AI when given explicit pedagogical framing, but required guidance to avoid using AI as shortcut rather than thinking tool

## Implications for LMICs

The critical thinking concern has particularly acute implications for low- and middle-income countries. First, the IndoMMLU study reveals that current LLMs perform poorly on local languages and cultures, with none of the 24 evaluated models performing well on Indonesian local knowledge—yet these same tools are being widely adopted in LMIC educational systems. This creates a dual vulnerability: students may over-rely on AI that lacks cultural competency, potentially eroding both critical thinking skills AND cultural knowledge transmission. Second, LMICs often have larger class sizes and fewer teachers, making AI tools attractive for scaling education, but this economic pressure may lead to implementation without adequate pedagogical safeguards. Third, the finding that AI reduces tasks to 'surface-level pattern matching' is especially concerning in contexts where rote learning already predominates; AI may reinforce rather than disrupt this tendency. Fourth, the 'quality-originality trade-off' identified in the AROA study suggests AI optimizes for typical patterns—in LMIC contexts, this could mean perpetuating dominant (often Western) knowledge frameworks at the expense of local, original perspectives. Finally, the evidence that only 34.7% of students verify AI information is concerning in contexts with limited digital literacy infrastructure and where misinformation may have more severe consequences.

## Recommendations

- Implement explicit AI literacy curricula that teach students metacognitive strategies for recognizing when to use vs. avoid AI, particularly emphasizing the quality-originality trade-off
- Design assessments that specifically target originality and higher-order thinking rather than quality alone, using frameworks like AROA's multi-dimensional originality assessment
- Adopt pedagogical frameworks that position AI as scaffolding rather than replacement, following models like IDPplanner that integrate curriculum alignment and design thinking principles
- Require students to document their thinking process independently before consulting AI, then explicitly compare their reasoning to AI outputs as a critical thinking exercise
- Develop rubrics that evaluate cognitive depth and argumentative originality separately from surface quality, making these explicit evaluation criteria
- Create structured verification protocols where students must fact-check and cite sources for AI-generated content, building this into assessment criteria
- Implement adaptive learning systems that optimize for cognitive challenge level (optimal difficulty thresholds) rather than task completion efficiency
- Design interdisciplinary projects that require deep conceptual integration beyond AI's current capability for surface-level connections
- Provide formative feedback that specifically addresses when students are relying on typical patterns vs. developing original arguments
- Conduct regular assessments of students' critical thinking skills independent of AI-assisted work to monitor potential degradation
- In LMIC contexts, prioritize development of culturally-grounded AI literacy materials and assessment tools in local languages before widespread AI adoption
- Establish institutional policies requiring transparent disclosure of AI use and pedagogical justification for AI integration in curriculum design

## Top Papers

1. **The effects of over-reliance on AI dialogue systems on students' cognitive abilities: a systematic review**
   Most comprehensive systematic review directly examining the over-reliance mechanism and its impact on critical thinking, decision-making, and analytical reasoning across educational contexts

2. **Argument Rarity-based Originality Assessment for AI-Assisted Writing**
   Provides novel computational framework for measuring the quality-originality trade-off, demonstrating that AI optimizes for typical patterns at expense of original thinking with strong empirical evidence (r = -0.67 correlation)

3. **Impact of ChatGPT on Student's Education: A Comprehensive analysis of positive and negative effects**
   Offers direct empirical evidence from student surveys showing reduced critical thinking (44.7% impacted), minimal verification behavior (only 34.7% always verify), and over-dependence patterns

4. **Co-Designing Interdisciplinary Design Projects with AI**
   Demonstrates both promise and limitations of AI for higher-order thinking: supports structure and design thinking application but reduces genuine interdisciplinary integration, with rigorous experimental comparison
