# Pedagogical Knowledge: How Well Do LLMs Understand Teaching?

## Executive Summary

Research into the pedagogical knowledge of large language models (LLMs) represents one of the most consequential — and contested — areas of AI-in-education benchmarking. Across **43 papers** reviewed in this category, a fundamental tension emerges: while LLMs demonstrate increasing competency in generating pedagogically-informed content such as lesson plans, feedback, and assessments, they frequently lack the deep understanding of learning science principles required for nuanced instructional decision-making. Leading benchmarks such as **The Pedagogy Benchmark** (920 questions drawn from Chilean teacher certification exams) and **EduGuardBench** show models achieving **28–89% accuracy** on pedagogical knowledge tests, with reasoning-enabled models like GPT-4 and Gemini 2.5 Pro substantially outperforming smaller alternatives. However, these quantitative scores frequently mask qualitative deficiencies: teachers consistently report that AI-generated materials require significant human refinement.

Perhaps the most striking finding across this body of work concerns cognitive offloading. A field experiment with approximately 1,000 high school students found that those using GPT-4 for practice solved **48% more problems correctly** but scored **17% lower on unassisted tests** — suggesting that AI-supported practice may actually impede the development of independent problem-solving skills. This single statistic encapsulates the central challenge: pedagogical AI that improves surface-level performance while undermining genuine learning is not merely unhelpful — it is actively harmful. The field is now moving beyond simple content generation towards complex multi-agent systems and frameworks grounded in learning science, but critical gaps remain in longitudinal evidence, cross-cultural validation, and deployment in authentic classroom settings — particularly in low- and middle-income countries (LMICs).

## Key Themes

### Benchmarking pedagogical competency — beyond content knowledge

A growing cluster of **11 papers** focusses on developing specialised datasets and evaluation frameworks to assess LLMs' understanding of teaching principles, instructional design, and educational theory. The Pedagogy Benchmark, which draws on authentic Chilean teacher certification questions, is notable as the **first large-scale benchmark explicitly testing pedagogical knowledge** rather than subject-matter expertise. It reveals significant performance gaps between surface-level recall and deep pedagogical reasoning — a distinction that matters enormously for practical deployment. Alongside this, **EduGuardBench** takes a dual approach, assessing both role-playing fidelity and teaching competence across nine scenarios while simultaneously testing adversarial safety against jailbreaking and academic misconduct requests. It identifies harmful teaching behaviours such as "Incompetence", "Indolence", and "Offensiveness" — and reveals a counter-intuitive finding where mid-sized models prove most vulnerable to adversarial attacks, not the smallest ones. Other benchmarks, including the **psychometrics-based professional competency framework** and **EduBench**, are expanding the evaluation landscape, though most remain concentrated in specific subjects or cultural contexts.

### Teacher-facing authoring tools and multi-agent architectures

Two closely related themes — **teacher-facing AI authoring tools** (9 papers) and **multi-agent systems for educational task orchestration** (7 papers) — represent the field's shift from single-model content generation toward more sophisticated architectures. Systems such as **FACET**, **IDPplanner**, and **EduPlanner** employ multiple specialised LLM agents — a planner, an evaluator, an optimiser, and sometimes a student simulator — that collaborate to design, deliver, and refine instruction with greater pedagogical sophistication than any single agent could achieve. The paper on **Enabling Multi-Agent Systems as Learning Designers** demonstrates that embedding the Knowledge-Learning-Instruction (KLI) framework into multi-agent workflows produces lesson plans rated higher than human-designed ones on curriculum alignment and pedagogical soundness. This is a significant finding, though it must be tempered by the observation that most evaluations remain offline — assessing lesson plan artefacts rather than their impact on actual students in real classrooms.

### Pedagogically-grounded intelligent tutoring systems

Eight papers explore the integration of learning science principles — Socratic dialogue, scaffolding, formative assessment — into LLM-based intelligent tutoring systems (ITS). The most comprehensive effort is **LearnLM-Tutor**, developed by Google DeepMind, which combines learning science principles with LLM capabilities across seven diverse evaluation benchmarks and includes real-world deployment data from Arizona State University. It demonstrates that **pedagogically-grounded tutoring achieves measurably better student outcomes** than baseline models. Complementing this, the **Inquizzitor framework** — described in "A Theory of Adaptive Scaffolding" — integrates Evidence-Centred Design with Social Cognitive Theory to create a theoretically rigorous approach to adaptive tutoring. These systems represent genuine progress, yet a persistent challenge remains: models still struggle with pedagogical concepts requiring high inference, such as metacognition and formative assessment design.

### Teacher perceptions and the irreducibility of human expertise

Across **8 papers** examining how teachers evaluate and adapt AI-generated pedagogical content, a consistent finding emerges: **human expertise remains essential** for contextualising and validating AI outputs. Studies on teacher co-design processes — including work on culturally relevant pedagogy and interdisciplinary project design — reveal that teachers value the efficiency gains AI tools provide while simultaneously identifying significant limitations in contextual adaptation, cultural appropriateness, and pedagogical nuance. Teachers in a Korean study stressed the importance of systematic support to prevent AI over-dependence, while educators evaluating ChatGPT for computer programming instruction reported that dependency on the tool limits student creativity and critical thinking. These qualitative findings provide an important corrective to benchmark scores alone.

### Automated assessment of teaching practice

Six papers explore LLMs as tools for evaluating teacher competencies and instructional quality — a potentially transformative application for scalable professional development. The **TalkMoves Application** achieves **79.3% F1 score** in classifying pedagogically significant teaching practices from classroom transcripts, demonstrating that automated analysis of classroom discourse is becoming practically viable. However, research on "The Promises and Pitfalls of Using Language Models to Measure Instruction Quality" highlights persistent challenges in inter-rater reliability between human experts and LLM evaluators. This is an area where **Retrieval-Augmented Generation (RAG)** shows particular promise — injecting domain-specific pedagogical knowledge into model prompts to improve assessment accuracy.

## What Is Being Measured

The field currently evaluates pedagogical knowledge across several dimensions. At the most basic level, researchers measure **accuracy on multiple-choice pedagogical knowledge tests**, including teacher certification examinations. Beyond recall, studies assess the **alignment of AI-generated lesson plans** with curriculum standards using established rubrics such as TPACK (Technological Pedagogical Content Knowledge), Quality Matters K-12 Standards, and the CIDDP framework — which evaluates clarity, integrity, depth, practicality, and pertinence. The **quality of instructional feedback** is scored against rubrics measuring clarity, specificity, and actionability. For tutoring systems, researchers track **student learning gains** through pre-post test differences and correctness prediction, as well as the coherence of multi-turn tutoring dialogues and the ability to generate appropriate scaffolding and Socratic questions. Teacher satisfaction is captured through Likert scales and qualitative feedback. A notable methodological trend is the use of **stronger LLMs as automated evaluators** to score weaker models' outputs — though this introduces its own biases.

## Critical Gaps

The most significant gap is the near-total absence of **longitudinal evidence on student learning outcomes** from AI-assisted instruction. Most studies evaluate AI-generated artefacts in isolation rather than during authentic teaching — and the few that do involve actual classroom deployment typically run for days or weeks rather than months or academic years. Equally concerning is the lack of research on **differential effectiveness across diverse student populations**, including variation by socioeconomic status, language background, and disability. This gap is particularly acute for LMIC contexts, where the pedagogical benchmarks and rubrics used in current research — predominantly drawn from Chilean, American, and Korean educational frameworks — may not transfer meaningfully.

The field also lacks measurement of **teachers' own professional growth** through AI tool usage, the **cost-effectiveness and scalability** of complex multi-agent systems in resource-constrained settings, and the **reliability of AI performance across different curriculum frameworks and cultural contexts**. Student emotional and motivational responses during extended interaction with pedagogical AI remain largely unexplored, as does the transparency of AI's own pedagogical decision-making — how and why a system selects a particular instructional strategy. These gaps represent substantial opportunities for investment.

## Cognitive Offloading

This category contains some of the most direct evidence on cognitive offloading in the broader benchmarking literature, with **8 papers** addressing the concern. The most consequential finding comes from a field experiment with approximately 1,000 high school students: those using GPT-4 — which was the leading commercial model at the time of the study — for practice solved **48% more problems correctly** during AI-assisted sessions but scored **17% lower on subsequent unassisted tests**. This suggests that AI-supported practice may substitute for rather than support genuine learning.

Corroborating evidence comes from multiple directions. A study on essay writing found that **83% of students using ChatGPT could not recall any text they had written**, suggesting minimal cognitive engagement during the task. EduGuardBench identifies "Indolence" as a specific harmful teaching behaviour where AI tutors dismiss student questions rather than scaffolding thinking. Teachers across several studies reported that ChatGPT dependency limits student creativity and critical thinking.

Encouragingly, some frameworks are beginning to address this directly. The **FACET framework** incorporates motivational modelling to prevent students from disengaging cognitively when AI provides extensive support. Multiple papers recommend **strategic AI availability** — such as providing hints only after a student has made an attempt — alongside explicit metacognitive prompts and validation requirements. The evidence here is clear: designing pedagogical AI without attention to cognitive offloading risks undermining the very learning outcomes these tools are intended to support.

## Notable Benchmarks and Datasets

**The Pedagogy Benchmark** stands out as the first large-scale evaluation tool explicitly focussed on pedagogical knowledge, comprising 920 multiple-choice questions from Chilean teacher certification exams covering cross-domain pedagogical knowledge and special educational needs and disabilities (SEND). It includes inference cost analysis alongside performance metrics — a practical consideration often overlooked.

**EduGuardBench** is unique in integrating both pedagogical quality assessment and ethical safety testing, using scenario-based evaluation with real-world teaching contexts and persona-based adversarial attacks. The **LearnLM Evaluation Framework** is the most comprehensive multi-dimensional effort, combining seven diverse benchmarks — quantitative, qualitative, automatic, and human — with actual student learning outcome data from university deployment.

The **CIDDP Rubric** and **TPACK-based Lesson Plan Rubric** apply established pedagogical theory to evaluate AI-generated instructional materials, while the **Quality Matters K-12 Standards** bring a widely adopted industry standard to bear on AI outputs. The **PBL-STEM Dataset**, with over 500 project-based learning projects featuring multimodal data, represents the first benchmark for evaluating multimodal LLMs on complex, real-world STEM assessment tasks.

## Methodological Trends

The dominant methodological approach combines **automated rubric-based scoring** — often using stronger LLMs such as GPT-4 or Claude 3.7 Sonnet as judges — with **qualitative teacher feedback** to capture nuanced pedagogical quality. Multi-agent system architectures with specialised roles have become a standard design pattern. RAG is increasingly used to inject domain-specific pedagogical knowledge into model prompts, and **few-shot and chain-of-thought prompting** have become standard approaches for improving pedagogical reasoning.

A notable limitation is that most studies remain **small-scale pilots** (N=10–30 teachers) using mixed-methods analysis rather than large-scale randomised controlled trials (RCTs). Evaluation against human expert baselines — experienced teachers and curriculum designers — is becoming more common, but inter-rater reliability remains problematic. The translation of learning science frameworks such as Bloom's taxonomy, the KLI framework, and Culturally Relevant Pedagogy into structured prompts represents an important methodological innovation, though it raises questions about whether complex pedagogical principles can be adequately operationalised in prompt instructions.

Critically, **most studies evaluate AI artefacts offline** — lesson plans, dialogue transcripts, assessment items — rather than tracking their impact during authentic classroom deployment with K-12 students. This limits the field's ability to draw conclusions about real-world pedagogical effectiveness.

## Recommendations

We recommend that funders and development partners **prioritise investment in longitudinal classroom trials** comparing student learning under AI-assisted versus traditional instruction — with particular attention to differential effects across student populations, subjects, and educational contexts in LMICs *(by 2027)*. The field should move beyond offline artefact evaluation towards measuring what ultimately matters: whether students learn more, and more equitably.

Alongside this, the field should **develop standardised evaluation protocols** that assess not merely pedagogical knowledge recall but applied instructional decision-making in realistic teaching scenarios. Current benchmarks, while valuable, remain concentrated in a handful of cultural contexts — we recommend expanding benchmark development to include teacher certification frameworks from sub-Saharan Africa, South and Southeast Asia, and other LMIC regions *(ongoing)*.

We recommend that education ministries and implementing organisations **establish clear guidelines for teacher validation of AI-generated pedagogical content**, including practical rubrics for evaluating contextual appropriateness and pedagogical soundness. Investment in **comprehensive teacher professional development** — focussed on critical evaluation of AI tools, prompt engineering for educational applications, and integration of AI within broader instructional strategies — should be a priority for any programme deploying pedagogical AI.

To address cognitive offloading, the field should **develop and test specific design mechanisms** — strategic AI availability, explicit metacognitive prompts, and validation requirements — that maintain productive struggle while leveraging AI's capabilities. This means positioning AI as a collaborative tool that augments teacher expertise rather than replacing it, and ensuring that AI handles routine tasks while teachers retain high-stakes pedagogical decisions.

Finally, we recommend creating **open-source repositories of validated pedagogical prompts and multi-agent architectures** to lower barriers for educational AI development while ensuring quality — particularly for organisations working in resource-constrained LMIC settings *(by December 2026)*.

## Key Papers

- **Towards Responsible Development of Generative AI for Education (LearnLM-Tutor)** — The most comprehensive evaluation framework in this space, combining seven diverse benchmarks with real-world deployment data and demonstrating measurable learning gains from pedagogically-grounded LLM tutoring.

- **EduGuardBench** — The only benchmark integrating pedagogical quality assessment with adversarial safety testing; reveals critical vulnerabilities and a counter-intuitive scaling paradox in educational AI models.

- **Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems** — Provides field-experimental evidence of cognitive offloading with approximately 1,000 students, showing a 17% decrease in independent test scores despite improved AI-assisted practice performance.

- **Benchmarking the Pedagogical Knowledge of Large Language Models (The Pedagogy Benchmark)** — The first large-scale benchmark explicitly testing pedagogical knowledge with 920 questions and cost-performance analysis, establishing baseline capabilities and limitations.

- **Enabling Multi-Agent Systems as Learning Designers** — Demonstrates that embedding learning science frameworks into multi-agent systems produces lesson plans rated higher than human-designed ones on key pedagogical dimensions.

- **A Theory of Adaptive Scaffolding for LLM-Based Pedagogical Agents (Inquizzitor)** — Integrates Evidence-Centred Design with Social Cognitive Theory to create a theoretically rigorous framework for operationalising learning science in LLM agents.

- **TalkMoves Application** — Achieves 79.3% F1 in classifying pedagogically significant teaching practices from classroom transcripts, demonstrating a practical pathway for scalable teacher professional development feedback.