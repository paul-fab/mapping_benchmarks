# Multimodal Capabilities: Benchmarking AI Systems That See, Hear, and Reason Across K-12 Education

## Executive Summary

Multimodal AI — systems that process and reason across text, images, diagrams, audio, and video — represents one of the most active frontiers in educational technology research, with **128 papers** identified in this category. The field is producing increasingly ambitious benchmarks, from MDK12-Bench's **141,000 questions across six K-12 disciplines** to KidsArtBench's expert-annotated evaluation of children's artwork across nine rubric-aligned dimensions. Yet a striking paradox emerges from this body of work: while researchers are investing heavily in measuring whether AI models can *solve* visual mathematics problems and *interpret* educational diagrams, almost none are measuring whether students *learn better* when supported by these multimodal systems. The overwhelming focus remains on predictive accuracy metrics — AUC scores, classification accuracy, next-item correctness — rather than learning science outcomes such as conceptual understanding, long-term retention, or the development of spatial reasoning skills.

The dominant research strand, accounting for **28 or more papers**, evaluates how well large multimodal models (LMMs) interpret mathematical diagrams, geometric figures, and scientific visualisations alongside textual problem statements. These benchmarks span elementary through secondary mathematics across multiple languages — English, Chinese, Vietnamese, French, and others — and reveal that even models considered leading at the time of testing, including GPT-4V and Gemini, struggle with **fine-grained visual discrimination**, spatial reasoning, and integrating multiple images with text. A secondary cluster of **15 or more papers** explores knowledge tracing systems that predict student performance by modelling learning trajectories, with several recent studies incorporating multimodal signals such as code submissions, facial expressions, and speech. Critically, the research is overwhelmingly concentrated in East Asian and Western educational contexts, with very limited coverage of African, Latin American, or South Asian settings — a significant gap given the educational priorities of low- and middle-income countries (LMICs).

This report examines what is being measured, what is being missed, and what the education sector should prioritise to ensure that multimodal AI serves genuine learning rather than simply demonstrating technical capability.

---

## Key Themes

### Visual-mathematical reasoning: the largest and most revealing strand

The single largest concentration of research — spanning at least **28 papers** — focusses on benchmarking LMMs against diagram-based mathematics and science problems. Datasets such as **CMM-Math** (28,069 Chinese K-12 mathematics problems), **VisioMath** (1,800 problems where answer options are visually similar diagrams), and **VisScience** (3,000 problems across mathematics, physics, and chemistry) have become the field's primary evaluation tools. These benchmarks consistently reveal fundamental limitations in current models. The VisioMath study was the first to systematically demonstrate that LMMs fail at **fine-grained visual discrimination** in educational contexts — when presented with multiple similar diagrams as answer choices, models rely on shallow positional heuristics rather than grounded semantic understanding. CMM-Math uniquely tests multi-image reasoning, where visual information appears in both questions and answer options, reflecting the reality of actual educational materials.

These findings matter for LMICs because they suggest that multimodal AI-EdTech products claiming to support diagram-heavy subjects — geometry, physics, chemistry — may be far less capable than their text-based performance implies. The **Multilingual Performance** study on physics concept inventories across 32 languages further revealed significant performance disparities, with models showing bias towards English and high-resource languages, raising serious equity concerns for global educational deployment.

### Comprehensive multimodal benchmarking across disciplines

Beyond mathematics, a cluster of approximately **eight papers** has developed large-scale benchmarks spanning multiple K-12 subjects. **MDK12-Bench** stands out as the most ambitious, covering 141,000 questions across mathematics, physics, chemistry, biology, geography, and computer science, annotated with **6,225 knowledge points** and difficulty labels. Its dynamic evaluation framework uses six bootstrapping strategies — including textual paraphrasing and visual augmentation — to combat data contamination, addressing the growing concern that foundation models may have been trained on test items. **EXAMS-V** provides unprecedented linguistic diversity with 20,932 questions across **11 languages from seven language families**, testing whether models can handle diverse education systems and cultural contexts. These benchmarks are essential infrastructure, but their overwhelming focus on multiple-choice accuracy leaves open the question of whether they capture the kinds of reasoning that matter most for learning.

### Speech, handwriting, and accessibility: multimodal inputs beyond images

A smaller but significant strand of research — approximately **nine papers** — addresses audio and handwriting modalities. The study on handwriting recognition in **Indonesian classrooms** evaluated vision-language models and LLMs on **14,000 or more handwritten student samples**, demonstrating both technical feasibility and the practical deployment challenges that arise in underrepresented educational contexts. Speech-enabled systems, including a spoken dialogue system for basic mathematics and the PhysicsAssistant robot tutor, explore how audio input can complement visual and textual modalities. The research on **automated evaluation of children's speech fluency for low-resource languages** is particularly relevant for LMIC contexts, where foundational literacy and numeracy (FLN) assessment at scale remains a pressing challenge.

### Knowledge tracing meets multimodal signals

Approximately **15 papers** explore knowledge tracing — predicting student performance from historical interaction sequences. While traditional knowledge tracing relies on response correctness data alone, several recent studies incorporate multimodal signals: **ES-KT-24** introduces the first public knowledge tracing dataset from game-based learning, with 15,032 users and **7.7 million interactions** including gameplay video. Other work integrates facial expressions, speech prosody, and code submissions into performance prediction models. The study on the Ken Utilisation Layer is notable for demonstrating that biologically inspired memory mechanisms can match deep learning performance with a **99% reduction in memory footprint** — an important finding for deployment in resource-constrained settings typical of LMICs.

### Creative assessment and beyond-STEM applications

Two papers push multimodal evaluation into creative domains. **KidsArtBench** provides the first public benchmark for multi-dimensional children's art assessment, with **1,046 artworks** from children aged 5–15 annotated by 12 expert educators across nine rubric-aligned dimensions including realism, imagination, and colour use. **MusicScaffold** addresses creative music education, deliberately positioning AI as scaffold rather than generator. These studies are significant because they demonstrate that multimodal AI evaluation need not be confined to STEM subjects — though the field remains overwhelmingly focussed there.

---

## What Is Being Measured

The field's measurement landscape is dominated by **technical performance metrics** applied to model capabilities rather than student outcomes. Across the 128 papers, the most commonly reported measures include multi-choice question answering accuracy across text, images, and diagrams; visual diagram parsing accuracy for geometry, physics, and chemistry; and knowledge state prediction accuracy (typically reported as AUC). Benchmarks such as MDK12-Bench assess performance across grade levels and difficulty tiers, while VisioMath specifically tests the ability to distinguish between visually similar answer options — a capability that proves surprisingly difficult for current models.

Beyond accuracy, researchers are measuring **cross-lingual consistency** (whether models perform equitably across languages), computational efficiency (inference latency, memory usage, token costs), and dynamic robustness under visual and textual perturbations. The ES-KT-24 dataset enables measurement of learning signals from gameplay videos, while eye-tracking studies measure dwell time, text coverage, and words per minute correlated with comprehension. The SCB-Dataset supports measurement of **19 classes of classroom behaviours** from images, enabling automated observation research.

Notably, some studies do measure learning-adjacent outcomes: user satisfaction, perceived usefulness, response time, and attempt counts. The KARL flashcard study stands out for including an **online user study** measuring actual student recall and cards learned — not just offline prediction accuracy. However, such studies remain the exception rather than the norm.

---

## Critical Gaps

The most consequential gap across this entire body of research is the near-total absence of measurement of **actual student learning gains** when using multimodal AI tools. Pre- and post-testing of conceptual understanding, long-term retention studies, and transfer-of-learning experiments are virtually nonexistent. The field treats multimodal AI as an evaluation challenge — *can models solve visual mathematics problems?* — rather than a pedagogical intervention question — *do students learn better with multimodal AI support?*

Several specific gaps demand investment. First, **cognitive load measurement** is almost entirely absent. Given that multimedia learning theory — grounded in decades of research by Sweller, Mayer, and others — identifies split-attention and modality effects as critical design variables, the failure to measure working memory load across different multimodal presentation formats represents a serious oversight. Second, **developmental trajectories** are ignored: benchmarks aggregate K-12 but developmental differences in multimodal processing between a six-year-old and a sixteen-year-old warrant separate evaluation tracks. Third, **cross-cultural validity** beyond East Asian and Western contexts is almost untested — a critical gap for LMIC stakeholders.

The field also lacks longitudinal multimodal datasets tracking the same students over weeks or months, accessibility evaluations for learners with visual, auditory, or motor disabilities, and cost-benefit analyses comparing computational expense against educational gain. Teacher adoption barriers and professional development needs for multimodal AI systems remain largely unexplored, as do the equity implications of differential device and connectivity access.

---

## Cognitive Offloading

Only **eight of 128 papers** explicitly address whether multimodal AI helps or hinders genuine learning — a troubling ratio given the stakes. The findings that do exist, however, are instructive. **ViScratch**, which uses gameplay videos and LLMs to provide automated feedback in Scratch programming, found that some students became dependent on LLM code generation, **'gaming' the system by clicking through without genuine engagement**. A study of ChatGPT usage in science investigation found that students over-relied on AI for both question-asking and answer-evaluation, struggling with effective questioning even when AI use was unrestricted.

More constructively, **HypoCompass** (a debugging tutor) deliberately offloads code writing to the LLM while requiring students to focus on hypothesis construction, preventing over-reliance on direct solutions. **MusicScaffold** positions AI explicitly as scaffold rather than generator, preserving productive struggle and autonomous creativity. Research on student choice in adaptive learning found that learner agency in pedagogical decisions — when supported by adaptive AI — improved outcomes compared to full automation, suggesting that co-regulation outperforms full AI control.

The embodied agent research found that while visual representations increase engagement, they also increase cognitive load and may cause distraction — a finding with direct implications for system design. Critically, no studies test whether students who use diagram-interpreting AI subsequently **lose the ability to reason spatially without AI support**. This is a significant concern given that visual reasoning is a transferable skill central to STEM learning.

---

## Notable Benchmarks and Datasets

The field has produced a substantial evaluation infrastructure. **MDK12-Bench** is the largest and most systematic, with 141,000 multimodal questions, a six-layer knowledge taxonomy of 6,225 knowledge points, and a dynamic evaluation framework using six bootstrapping strategies to combat data contamination. **CMM-Math** provides the largest non-English multimodal mathematics benchmark at **28,069 problems**, uniquely testing multi-image reasoning. **VisioMath** is the first benchmark specifically designed to test fine-grained visual discrimination among diagram-based answer choices.

For multilingual evaluation, **EXAMS-V** offers 20,932 questions across 11 languages from seven language families, while the **PhysPort Concept Inventories** study tested multimodal AI across physics assessments in **32 languages**. In knowledge tracing, **EdNet** provides massive-scale authentic data from 627,000 users and 73 million responses, while **ES-KT-24** introduces game-based learning video data. For classroom observation, the **SCB-Dataset** provides 13,330 images with 122,977 labels across 19 behaviour classes — the first comprehensive, publicly available dataset for automated classroom observation.

Beyond STEM, **KidsArtBench** establishes the first benchmark for children's creative work assessment, and **MalAlgoQA** evaluates LLMs' ability to identify misconceptions and counterfactual reasoning — a capability critical for effective tutoring systems.

---

## Methodological Trends

The dominant technical approach combines **pre-trained vision encoders** (CLIP, ResNet, Vision Transformers) with large language models through attention mechanisms, often fine-tuned using parameter-efficient methods such as LoRA. Multi-task learning frameworks that jointly predict correctness, response choices, and knowledge states are increasingly common. A notable trend is the use of **dynamic evaluation methodologies** — bootstrapping through textual paraphrasing and visual augmentation — to mitigate data contamination concerns, reflecting growing awareness that foundation models may have encountered benchmark items during training.

Prompt engineering strategies for multimodal LLMs have become sophisticated, incorporating role-playing instructions, rubric-aligned prompts, and chain-of-thought reasoning with visual grounding. Several studies employ **Wizard-of-Oz methods**, where human operators perform critical functions such as speech recognition to evaluate system design under ideal conditions before full automation — a pragmatic approach for systems that are not yet reliable enough for independent deployment.

A welcome methodological development is the emergence of **hybrid human-AI evaluation**, combining automated metrics with expert human ratings. However, the overwhelming reliance on offline benchmark performance rather than online user studies measuring actual learning outcomes remains the field's most significant methodological limitation.

---

## Recommendations

We recommend that funders and researchers **shift evaluation focus from pure model accuracy to learning science outcomes** *(by end of 2026)*. The field urgently needs pre- and post-studies measuring whether multimodal AI tools improve conceptual understanding, spatial reasoning development, and long-term retention — not just immediate test scores. This represents the single highest-priority investment opportunity.

The field should **address the image-text alignment crisis** identified across multiple benchmarks. Current LMMs fail to reliably ground reasoning in visual cues when multiple images are present. This is the most critical technical limitation for educational deployment and requires fundamental architectural improvements, not simply more training data.

We recommend the development of **grade-specific multimodal benchmarks** that account for developmental differences in multimodal processing between early primary and secondary learners. Alongside this, **cross-cultural multimodal benchmarks** should be expanded beyond East Asian and Western contexts to incorporate African, Latin American, and South Asian educational content *(ongoing, with initial benchmarks targeted by mid-2027)*.

Researchers should **investigate cognitive offloading and over-reliance systematically**. No studies currently test whether students who use diagram-interpreting AI subsequently lose the ability to reason spatially without AI support. This is crucial given that visual reasoning is a transferable skill central to STEM learning.

The field should **prioritise accessibility in design and evaluation**, reporting disaggregated results for diverse learner populations including those with visual impairments, non-native speakers, and neurodiverse students. Equity and fairness audits should become standard practice.

Finally, we recommend that all multimodal AI-EdTech research **report computational costs** — inference time, memory requirements, and connectivity needs — to enable realistic cost-benefit analysis for school deployment in LMIC contexts, where infrastructure constraints are the norm rather than the exception.

---

## Key Papers

- **MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams** — Establishes the largest and most systematic K-12 multimodal benchmark with 141,000 questions, structured knowledge taxonomy, and dynamic evaluation framework. Essential reading for anyone designing multimodal educational assessments.

- **VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs** — The first study to systematically demonstrate that LMMs fail at fine-grained visual discrimination in educational contexts, revealing fundamental image-text alignment problems that affect all multimodal reasoning applications.

- **From Handwriting to Feedback: Evaluating VLMs and LLMs for AI-Powered Assessment in Indonesian Classrooms** — Addresses multimodal assessment in an underrepresented LMIC context with real handwritten student work, demonstrating both technical feasibility and practical deployment challenges.

- **CMM-Math: A Chinese Multimodal Math Dataset** — The largest non-English multimodal mathematics benchmark at 28,069 problems, uniquely testing multi-image reasoning critical for real educational materials.

- **Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories** — The most comprehensive evaluation of multimodal LLMs on validated K-12 physics assessments across 32 languages, revealing critical visual reasoning weaknesses and language equity concerns.

- **KARL: Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students** — The first content-aware flashcard scheduler with both offline validation and an online user study demonstrating improved learning outcomes — a rare example of measuring actual learning, not just prediction accuracy.

- **KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs** — The first comprehensive benchmark for multimodal creative work assessment with expert-validated pedagogical rubrics, pushing evaluation beyond STEM.

- **ViScratch: Using Large Language Models and Gameplay Videos for Automated Feedback in Scratch** — Demonstrates practical multimodal debugging support while also surfacing evidence of student over-reliance — one of the few papers engaging with cognitive offloading concerns.

- **Open-ended Knowledge Tracing for Computer Science Education** — The first study to predict actual student code rather than just correctness, enabling analysis of specific misconceptions and error patterns in open-ended responses.

- **SCB-Dataset: A Dataset for Detecting Student and Teacher Classroom Behavior** — Provides the first large-scale, publicly available benchmark for automated classroom observation with 19 behaviour classes, enabling AI development for teacher professional development.