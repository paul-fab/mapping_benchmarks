# Multilingual Capabilities: How AI Systems Perform Across Languages in Education

## Executive Summary

The ability of large language models (LLMs) to operate effectively across multiple languages is arguably the single most consequential factor determining whether AI-EdTech can deliver equitable educational benefits in low- and middle-income countries (LMICs). Our analysis of **15 papers** focussed on multilingual capabilities in educational contexts reveals a consistent and concerning finding: LLMs exhibit **systematic performance gaps of 10–40%** between English and other languages, with degradation most severe for low-resource languages and non-Latin scripts. This means that learners who stand to benefit most from AI-powered educational tools — those in multilingual LMIC settings — are precisely those receiving the lowest-quality AI support.

The research base is growing in sophistication. Multiple studies now construct **authentic multilingual benchmarks** drawn from real national examinations and curricula, moving beyond the earlier — and deeply flawed — practice of simply translating English-language datasets. Landmark benchmarks such as EXAMS (covering 16 languages and 24 subjects) and IndoMMLU (nearly 15,000 questions from Indonesian education) provide granular evidence of where models succeed and fail. A particularly striking finding from the IndoMMLU study is that earlier-generation models such as GPT-3.5 achieved only **primary school-level performance** when tested on Indonesian educational content, despite performing competently on equivalent English-language assessments.

However, the field remains heavily focussed on measuring answer correctness rather than the quality of educational interaction. Almost no research examines long-term learning outcomes for students using multilingual AI tutors, the impact of language-dependent performance gaps on student motivation, or how code-switching — a natural feature of multilingual classrooms — is handled by these systems. These gaps represent urgent priorities for the education research and funding community.

## Key Themes

### Language-dependent performance gaps are large, consistent, and consequential

Across **10 of the 15 papers** reviewed, the finding is unambiguous: LLMs perform substantially better in English than in other languages, and the gap widens as language resource levels decrease. The study *Multilingual Performance Biases of Large Language Models in Education* provides foundational evidence, evaluating five major LLMs across nine languages on four educational tasks and documenting systematic disparities. The paper *Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs* further demonstrates that these gaps are not limited to comprehension tasks — they extend to **mathematical reasoning**, where one might expect language to play a smaller role.

The Korean mathematics study (*On the robustness of ChatGPT in teaching Korean Mathematics*) reveals that even well-resourced languages can present significant challenges when paired with domain-specific tasks. Using 586 questions from the Korean College Scholastic Ability Test, the study shows that models struggle with both the linguistic demands of Korean mathematical terminology and the interpretation of diagrams presented in non-English contexts. This means the performance gap is not merely about translation quality; it reflects deeper limitations in how models process knowledge across linguistic boundaries.

### Multimodal reasoning compounds multilingual challenges

Seven papers examine what happens when language barriers intersect with **multimodal reasoning** — tasks requiring models to interpret diagrams, graphs, tables, or images alongside text. The findings are stark. The EXAMS-V benchmark, comprising **20,932 multimodal questions across 11 languages and 20 disciplines**, demonstrates that vision-language models experience compounded performance degradation when visual reasoning is combined with non-English text. Models that perform adequately on text-only questions in a given language often fail dramatically when visual elements are introduced.

The study *Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories* tested GPT-4o — one of the more capable models available at the time of the study — across physics assessments in **17 languages**. Even this relatively advanced model struggled with non-English scientific content requiring visual interpretation. For education in LMICs, where STEM instruction frequently relies on diagrams and locally produced visual materials, this represents a significant barrier to effective AI deployment.

### The shift from translated to authentic benchmarks

A methodological turning point is evident across **eight papers**: the move from translating English benchmarks towards creating **authentic multilingual evaluation frameworks** built from actual educational materials in target languages. The EXAMS dataset exemplifies this approach, drawing on **over 24,000 high school examination questions** from 16 languages and eight language families. Similarly, IndoMMLU assembles **14,981 questions** from Indonesian primary school through university entrance examinations, with **46% focussing on Indonesian language proficiency and knowledge of nine local languages and cultures**.

This shift matters enormously. Translated benchmarks introduce noise, lose cultural context, and may inadvertently test translation quality rather than educational reasoning. Authentic benchmarks reveal challenges that translation-based approaches miss entirely — particularly around **culture-specific knowledge**, local curriculum expectations, and the distinctive ways that concepts are framed in different educational traditions.

### Cultural and local knowledge gaps persist

Five papers document that LLMs show limited knowledge of **culture-specific content and local educational contexts**, even when they are technically proficient in a given language. The IndoMMLU study is particularly revealing: models that could handle general Indonesian-language questions failed on items requiring knowledge of local languages, regional customs, or Indonesia-specific civic education. This finding has profound implications for LMICs where curricula are deliberately designed to reflect national and local identities. An AI tutor that cannot engage with locally relevant content is, at best, a limited tool and, at worst, a vehicle for cultural homogenisation.

### Accessibility for diverse learners remains underexplored

A smaller but important thread — represented by four papers — examines how multilingual AI systems serve learners with specific needs. The study *LLM-Powered AI Tutors with Personas for d/Deaf and Hard-of-Hearing Online Learners* highlights that current systems offer inadequate **sign language support** and limited multimodal accessibility beyond text and speech. The work on *Automated evaluation of children's speech fluency for low-resource languages* addresses a critical gap in mother-tongue literacy assessment for Tamil and Malay speakers, demonstrating both the potential and the current limitations of automated speech recognition for young learners in low-resource language contexts.

## What Is Being Measured

The field is currently focussed on a relatively narrow — though important — set of metrics. The dominant measure is **accuracy or correctness** of LLM responses on standardised assessments across languages, with studies typically reporting performance gaps between English and target languages as percentage-point differences. Cross-lingual transfer capabilities are tested by training in one language and evaluating in another, while multimodal evaluations compare text-only versus text-plus-image performance by language.

Beyond answer correctness, researchers are measuring **automatic speech recognition (ASR) accuracy** for children in low-resource languages, **essay quality assessment** metrics in non-English contexts (grammar, coherence, content), **readability classification accuracy** for educational texts, and **OCR quality** from multilingual materials. Domain-specific terminology understanding — how well models handle STEM concepts or grammatical terms across languages — is emerging as an important dimension, alongside consistency of responses when identical content is presented in different languages.

## Critical Gaps

What the field is **not** measuring represents a significant opportunity for future investment. Most critically, there is virtually no research on **long-term learning outcomes** for students using multilingual AI tutors. We know models perform worse in certain languages, but we do not know how this translates into differential learning experiences over weeks or months of use. The impact of language-dependent performance gaps on **student motivation and self-efficacy** — particularly for learners in non-English contexts who may receive lower-quality AI responses — remains entirely unexplored.

The quality of **explanations and reasoning processes** across languages is another major gap. Current benchmarks overwhelmingly assess whether a model produces the correct final answer, not whether the explanation offered is pedagogically sound, age-appropriate, or culturally relevant. Relatedly, **hallucination rates and factual accuracy across languages** beyond English are poorly documented — a significant concern given evidence that models may confabulate more freely when operating in languages with less training data.

Practical implementation questions are also underexamined. **Code-switching** — the natural mixing of languages that characterises multilingual classrooms across LMICs — is barely studied in the context of AI-EdTech. Neither is the **cognitive load** experienced by students interacting with AI in their second or third language versus their mother tongue. Teacher perspectives, including **workload implications** when implementing multilingual AI tools, are largely absent from the literature.

## Notable Benchmarks and Datasets

Several evaluation tools stand out for their rigour and relevance. **EXAMS** represents the gold standard for authentic multilingual educational benchmarking, with over **24,000 high school exam questions** across 16 languages, eight language families, and 24 subjects — all drawn from actual national examinations rather than translations. Its multimodal extension, **EXAMS-V**, adds **20,932 questions** requiring joint reasoning over text, images, tables, and diagrams across 11 languages and 20 disciplines, making it the most comprehensive multimodal multilingual educational benchmark available.

**IndoMMLU** fills a critical gap for Southeast Asian educational assessment, with **14,981 questions** stratified from primary school through university entrance level, and nearly half its content focussed on Indonesian language proficiency and local cultural knowledge. For spoken language assessment, the **Tamil and Malay Children's Speech Fluency Dataset** represents a pioneering effort to evaluate automated fluency assessment for very low-resource languages, addressing an essential need in mother-tongue literacy programmes. The **Korean Mathematics CSAT Dataset**, with 586 validated questions and difficulty ratings across 11 criteria, provides granular insight into mathematical reasoning challenges in non-English contexts.

## Methodological Trends

The methodological landscape is maturing in encouraging ways. The most significant trend is the creation of **authentic multilingual benchmarks** from actual educational assessments, displacing the earlier reliance on translated English datasets. Researchers are increasingly employing **multi-stage evaluation pipelines** — ASR or OCR followed by content extraction and then automated evaluation — that mirror the real-world complexities of processing multilingual educational content.

Comparative analysis of multiple commercial and open-source models on identical tasks is now standard practice, with studies typically evaluating models from OpenAI, Google, Anthropic, and Meta side by side. **Fine-grained analysis** by language family, script type, and resource level allows researchers to distinguish between challenges that are language-specific and those that are structural. Error analysis is becoming more sophisticated, distinguishing content errors from language errors, parsing failures, and cultural misalignments — a level of granularity essential for identifying actionable improvements.

Prompting strategies are also under investigation, with zero-shot and few-shot evaluation used alongside experiments in **cross-lingual transfer** — training in one language and evaluating in others. Several studies combine automated metrics with human expert judgement, reflecting a growing recognition that purely automated evaluation may miss important dimensions of educational quality.

## Recommendations

We recommend that funders and development partners **prioritise investment in authentic multilingual educational benchmarks** created in target languages rather than relying on translations. This means funding partnerships with national examination bodies, curriculum developers, and local researchers to build evaluation frameworks that reflect actual educational contexts in LMICs *(ongoing, with initial frameworks by December 2026)*.

The field should **establish transparent reporting requirements** for multilingual LLM performance in educational applications. At minimum, developers deploying AI-EdTech tools should be required to publish disaggregated results by language, script type, and educational domain — enabling educators and policymakers to make informed decisions about where these tools can be trusted.

We recommend that AI-EdTech developers **implement mandatory language-specific validation** before deploying tools in new linguistic contexts, verifying that performance meets minimum quality thresholds and identifying specific failure modes. This is not merely a technical recommendation; it is an equity imperative.

Investment is urgently needed in **training data collection for underrepresented languages**, particularly for low-resource languages, non-Latin scripts, and culture-specific knowledge domains relevant to LMIC curricula. Alongside this, we recommend the development of **specialised evaluation frameworks** that assess explanation quality, reasoning processes, and pedagogical appropriateness across languages — moving beyond the current overreliance on answer correctness.

Finally, we recommend **longitudinal studies examining learning outcomes** when students use AI tutors across different language contexts. Understanding the real-world equity implications of multilingual performance gaps — not just the technical measurements — should be a research priority for the next two to three years.

## Key Papers

- **Multilingual Performance Biases of Large Language Models in Education** — Comprehensive evaluation of five major LLMs across nine languages on four educational tasks, providing foundational evidence of systematic performance gaps and establishing the case for language-specific validation before deployment.

- **EXAMS: A Multi-subject High School Examinations Dataset for Cross-lingual and Multilingual Question Answering** — Establishes the gold-standard methodology for authentic multilingual educational benchmarking with over 24,000 questions across 16 languages drawn from real national examinations.

- **IndoMMLU: Large Language Models Only Pass Primary School Exams in Indonesia** — Demonstrates critical limitations of frontier LLMs in low-resource languages and local knowledge domains, showing GPT-3.5 achieving only primary school-level performance on Indonesian educational content.

- **EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models** — Extends multilingual evaluation to multimodal reasoning with 20,932 questions requiring integrated text-image understanding across 11 languages, revealing compounded challenges at the intersection of visual reasoning and multilingual processing.

- **Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories** — Systematic analysis of GPT-4o across physics assessments in 17 languages, demonstrating that even relatively capable models struggle with non-English scientific content requiring visual interpretation.