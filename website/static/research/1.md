# General Reasoning: Benchmarking AI's Fundamental Cognitive Abilities for K-12 Education

## Executive Summary

General reasoning — encompassing mathematical problem-solving, reading comprehension, multi-step logical inference, and multimodal interpretation — represents the most extensively benchmarked dimension of AI performance in K-12 education. Our analysis covers **109 papers** that collectively reveal a field making rapid progress in measuring what AI systems can do, yet largely failing to measure what matters most: whether these systems actually help students learn.

The dominant finding across this body of research is sobering. Even the most capable models available during the study periods — including GPT-4 and its multimodal variants — typically achieved only **50–70% accuracy** on K-12 tasks, significantly below human performance benchmarks of **80–95%**. More concerning still, several critical studies demonstrate that models frequently rely on **shallow heuristics and pattern matching** rather than genuine conceptual understanding. The GSM-PLUS benchmark, for instance, revealed **20% accuracy drops** when problems were only slightly rephrased — suggesting that what looks like reasoning may often be sophisticated memorisation. This has profound implications for any deployment in educational settings, particularly in low- and middle-income countries (LMICs) where teacher oversight may be limited and the consequences of unreliable AI support are most acute.

The research landscape is heavily weighted towards mathematical reasoning and dominated by English and Chinese language contexts. Multilingual evaluation remains sparse — with only a handful of benchmarks covering languages such as Bangla, Vietnamese, Indonesian, and Korean — and LMIC-focussed research is notably underrepresented. Perhaps most critically, the field has invested heavily in measuring **answer correctness** while largely neglecting the pedagogical dimensions that determine whether AI tools genuinely support learning: scaffolding quality, cognitive load, metacognitive development, and the risk of cognitive offloading.

## Key Themes

### Comprehensive K-12 benchmarks are emerging — but unevenly

Approximately 15 papers in this category focus on building large-scale benchmarks that span multiple subjects and grade levels. **E-EVAL**, the first comprehensive Chinese K-12 education evaluation benchmark, covers 4,351 questions across nine subjects from primary through high school. **MDK12-Bench** is substantially larger, with **141,000 real-world exam questions** across six disciplines, structured knowledge points, and difficulty labels — making it the most extensive multimodal K-12 benchmark to date. The **VNHSGE** dataset brings Vietnamese high school graduation examination content into the evaluation landscape.

These benchmarks represent significant progress. However, the geographical and linguistic distribution is strikingly uneven. Chinese-language benchmarks dominate, alongside English-language datasets, while coverage of languages spoken across sub-Saharan Africa, South Asia, and Southeast Asia remains minimal. For education funders and policymakers concerned with equity, this means that the evidence base for AI performance in most LMIC contexts is thin to non-existent.

### Mathematical reasoning dominates — and exposes fundamental weaknesses

With over **30 papers** focussed on mathematical reasoning across the two analysis passes, this is the single largest theme in the general reasoning category. Benchmarks range from elementary-level word problems (GSM8K, with 8,500 grade school problems) through to competition-level mathematics (the MATH dataset, with 12,500 problems). Specialised datasets such as **CMATH** (Chinese elementary mathematics), **BanglaMATH** (Bangla mathematical reasoning for grades 6–8), and **CMMaTH** (Chinese multimodal mathematics) extend coverage across languages and formats.

The most important finding from this body of work is not about accuracy levels — it is about the fragility of apparent reasoning. The **GSM-PLUS** benchmark systematically generated 10,552 variations of existing problems through numerical changes, added distractors, and rephrasing. The result was a consistent **20% accuracy drop** across models, strongly suggesting reliance on superficial patterns rather than genuine mathematical understanding. The **SVAMP** dataset reinforced this finding, demonstrating that some models could achieve high accuracy without even processing the question text — a damning indictment of what passes for "reasoning" in benchmark evaluations.

### Multimodal reasoning remains a critical weakness

Approximately 12 papers evaluate models' ability to integrate visual information — diagrams, graphs, geometric figures, scientific notation — with textual problem descriptions. This is particularly relevant for K-12 education, where visual representations are central to mathematics, science, and geography instruction. The findings are consistently discouraging. **VCBench**, which tests elementary-level mathematical reasoning requiring interpretation of multiple images, found that even the strongest models of the period achieved **below 50% accuracy**. **VisioMath**, where answer options are presented as visually similar diagrams, revealed fundamental weaknesses in fine-grained visual discrimination. **GeoQA**, with 5,010 geometric problems requiring diagram understanding, remains one of the largest multimodal geometry datasets and continues to challenge current systems.

This matters enormously for LMIC deployment. Many educational contexts rely heavily on visual and diagrammatic content — from geometric reasoning in mathematics curricula to scientific diagrams in physics and biology. If AI systems cannot reliably interpret these elements, their utility as tutoring or assessment tools is severely constrained.

### Knowledge tracing sits at the intersection of tradition and innovation

A substantial cluster of approximately **15 papers** addresses knowledge tracing — the task of predicting student performance and tracking knowledge states over time. This field shows a fascinating methodological tension. Traditional approaches grounded in psychometric theory — Item Response Theory (IRT), Bayesian Knowledge Tracing (BKT) — offer interpretability and theoretical grounding. Modern deep learning approaches — including Deep Knowledge Tracing (DKT), attention-based models, and transformer architectures — often achieve higher predictive accuracy but sacrifice explainability.

The **FoundationalASSIST** dataset represents a significant advance, providing 1.7 million K-12 student interactions with full question text, actual student responses, and Common Core alignment. It is the first English-language educational dataset enabling research on predicting actual student responses — not just correctness — at scale. The **PSI-KT** framework demonstrates that it is possible to combine interpretability with strong performance, suggesting that the field need not choose between understanding and accuracy.

### Multilingual evaluation reveals stark equity gaps

Nine papers explicitly address cross-linguistic and cross-cultural evaluation. The findings consistently reveal **lower performance in non-English settings**. The study on **multilingual performance of AI on physics concept inventories** — covering more than 20 languages — documented significant performance disparities that raise serious equity concerns. **IndoMMLU**, the first multi-task language understanding benchmark for Indonesian, covers 14,981 questions across 64 tasks, including content in local languages and cultural contexts. **BanglaMATH** represents the first mathematical reasoning benchmark for Bangla — a language spoken by over 230 million people.

These datasets are pioneering, but they remain exceptions. The vast majority of K-12 reasoning benchmarks assume English or Chinese as the language of instruction, leaving most LMIC languages without any systematic evaluation evidence.

## What Is Being Measured

The field has developed sophisticated approaches to measuring **answer correctness** across multiple formats — multiple-choice, fill-in-the-blank, and open-ended questions. Grade-level appropriateness is assessed through benchmarks aligned to specific curricula and educational standards. Subject-specific knowledge is evaluated across mathematics, physics, chemistry, biology, and language arts. Multi-step reasoning and problem decomposition are tested through increasingly complex problem sets, while robustness is probed through systematic problem perturbations.

Knowledge tracing systems measure **student knowledge state estimation** and future performance prediction, calibration and confidence, and difficulty level discrimination. Multimodal benchmarks assess the integration of textual and visual information. A small but growing number of studies — notably the **LearnLM-Tutor** evaluation — assess pedagogical quality metrics including helpfulness, scaffolding, clarity, and reasoning progression.

Comparison to human student performance at specific grade levels has become a standard methodology, providing intuitive benchmarks for non-technical audiences. Code generation, debugging, and programming task completion are evaluated through datasets such as **CodeWorkout** and frameworks like **TIKTOC**.

## Critical Gaps

The most striking gap in this body of research is the near-total absence of evidence on **actual learning outcomes**. The overwhelming majority of papers measure whether an AI system can answer a question correctly — not whether a student learns more effectively when supported by that system. Longitudinal studies tracking knowledge retention, transfer to novel problem types, and development of independent reasoning capabilities are essentially absent.

**Metacognitive development** — the capacity for self-assessment, strategy selection, and error detection — is unmeasured. So too is the impact of AI assistance on **student motivation, persistence, and growth mindset**. The field has no standard protocols for evaluating whether AI tools support **productive struggle** — the pedagogically valuable experience of working through difficulty — or whether they short-circuit this process by providing answers too readily.

**Equity and accessibility** across diverse student populations — including learners with disabilities, those from low socioeconomic backgrounds, and multilingual learners — receive almost no attention. **Teacher integration** challenges, pedagogical decision-making support, and the realities of deployment in resource-constrained settings are similarly neglected. For LMIC stakeholders, these gaps are not peripheral — they are central to whether AI-EdTech tools can deliver meaningful educational value.

The field also lacks robust evaluation of **cost-effectiveness and scalability**, which are critical considerations for education systems operating under severe resource constraints. Benchmark performance in laboratory conditions tells us little about real-world utility when infrastructure is unreliable, devices are shared, and connectivity is intermittent.

## Cognitive Offloading: A Growing but Underexplored Concern

Only **8 of 109 papers** meaningfully address the risk that AI assistance may encourage cognitive offloading — where students delegate thinking to the AI rather than developing their own reasoning capabilities. This is a concerning ratio given the centrality of this question to responsible educational deployment.

The papers that do engage with this issue offer important insights. The **LearnLM-Tutor** evaluation explicitly designs against "spoon-feeding" by using Socratic dialogue and refusing to provide direct answers. **ERL4SIIP** addresses the "spoon-feeding trap" in Socratic tutoring and measures strategic diversity to avoid premature answer-giving. The **CoDAE** framework tackles over-compliance in large language models (LLMs), demonstrating that models tend to reveal answers too readily without encouraging student reasoning.

Perhaps the most striking finding comes from the **"Beyond Final Answers"** study, which evaluated math tutoring quality beyond correctness. It found that only **56.6% of tutoring dialogues were entirely error-free**, despite **88.6% final answer accuracy**. This means students interacting with AI tutors are frequently exposed to flawed reasoning processes — even when the final answer is correct — potentially forming misconceptions. The **"Student Data Paradox"** paper reveals a fundamental tension: training LLMs on student data to model misconceptions causes the models to lose their own factual knowledge, creating a direct conflict between personalisation and accuracy.

These findings suggest that the risk of cognitive offloading is real and multifaceted. It is not simply about students copying answers — it extends to the subtler danger of internalising flawed reasoning from systems that appear confident but are frequently wrong in their intermediate steps.

## Notable Benchmarks and Datasets

Several benchmarks stand out for their relevance to LMIC education contexts and their methodological innovation. **MDK12-Bench** is the largest multimodal K-12 benchmark, with 141,000 questions and a dynamic evaluation framework designed to mitigate data contamination — a critical concern given evidence that models may memorise rather than reason through problems. **GSM1k**, with 1,250 newly created problems guaranteed not to appear in any training data, provides the cleanest available test of genuine mathematical reasoning versus memorisation.

**FoundationalASSIST** breaks new ground by providing complete question text and actual student responses — not just correctness indicators — enabling research on how LLMs might predict and respond to real student behaviour. **BanglaMATH** and **IndoMMLU** are significant for the LMIC community, representing rare examples of benchmarks developed for major low-resource languages. **FairytaleQA**, with 10,580 expert-created questions on children's stories validated with 120 students, offers a model for rigorous reading comprehension evaluation at the elementary level.

For multimodal evaluation, **GeoQA** (5,010 geometric problems with executable programme annotations), **VCBench** (1,720 problems requiring reasoning across multiple images), and **VisScience** (3,000 questions across mathematics, physics, and chemistry) collectively provide the most robust evidence base for visual reasoning capabilities — and their consistent finding is that current models remain far from adequate.

## Methodological Trends

The field shows several clear methodological patterns. **Large-scale benchmark construction from real examination questions** with expert annotation has become the standard approach, moving away from synthetically generated problems. **Zero-shot and few-shot prompting** evaluation assesses models' capabilities without task-specific fine-tuning, while **supervised fine-tuning** (SFT) on domain-specific educational data is used to improve pedagogical alignment.

A notable shift is underway from traditional psychometric models towards **deep learning architectures** — including LSTMs, transformers, and graph neural networks — for knowledge tracing and student modelling. However, several papers caution that this shift comes at the cost of interpretability, with the **PSI-KT** framework demonstrating that hybrid approaches can maintain both.

**Robustness testing** through systematic problem perturbations has emerged as a critical methodological innovation, exposing the gap between benchmark accuracy and genuine reasoning capability. **Dataset contamination analysis** — pioneered by the GSM1k study — is increasingly recognised as essential for valid evaluation. The growing use of **LLM-as-judge** evaluation methods for open-ended responses represents an attempt to scale assessment beyond multiple-choice formats, though questions about the reliability of this approach remain.

Most studies in this corpus evaluated models available in 2023–2024, including GPT-4, Claude 2 and 3, Gemini 1.0 and 1.5, and various open-source alternatives such as Llama 2 and 3. Given the rapid pace of model development, many findings may warrant re-evaluation with current-generation models — though the fundamental concerns about shallow reasoning and robustness are likely to persist even as raw accuracy improves.

## Recommendations

We recommend that the education and development community prioritise the following actions:

**Invest in learning outcome measurement.** The field should move urgently beyond answer accuracy to develop benchmarks that track actual student learning gains, knowledge retention, and transfer. This requires longitudinal study designs and real-world deployment research — not just offline evaluation. We recommend funders allocate dedicated resources for this purpose *(by end of 2026)*.

**Develop pedagogical quality evaluation standards.** We recommend establishing standardised protocols for assessing scaffolding quality, avoidance of over-helping, and support for productive struggle. The LearnLM-Tutor framework offers a starting point, but sector-wide standards are needed.

**Expand multilingual and LMIC-focussed benchmarks.** The current concentration on English and Chinese contexts is untenable. We recommend a coordinated effort — potentially through advanced market commitments — to develop benchmarks in at least **10 additional LMIC languages** *(by 2027)*, with particular attention to sub-Saharan African languages, Hindi, Urdu, and other high-population low-resource languages.

**Address the robustness problem systematically.** The evidence that models rely on shallow heuristics rather than genuine reasoning should inform procurement and deployment decisions. We recommend that any AI-EdTech product deployed in educational settings be required to demonstrate robustness through perturbation testing, not just accuracy on static benchmarks.

**Investigate cognitive offloading through controlled studies.** The field should commission longitudinal research examining whether sustained AI use develops or undermines student reasoning capabilities — particularly for foundational literacy and numeracy (FLN) in early primary years.

**Build dynamic, contamination-resistant benchmarks.** As models are continuously retrained on expanding corpora, static benchmarks become unreliable. We recommend investment in automatic problem variation generation and temporal controls to maintain evaluation validity *(ongoing)*.

**Prioritise interpretability alongside performance.** For educational applications — where understanding why a system makes a particular recommendation matters as much as accuracy — hybrid approaches combining psychometric theory with neural methods should be the standard, not the exception.

## Key Papers

- **"Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach" (LearnLM-Tutor)** — Establishes a comprehensive evaluation framework grounded in learning science principles with real-world deployment at scale; a model for responsible AI-EdTech development.

- **"GSM-PLUS: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers"** — Reveals that models rely on shallow heuristics, with 20% accuracy drops on slight problem variations; essential reading for anyone evaluating AI reasoning claims.

- **"A Careful Examination of Large Language Model Performance on Grade School Arithmetic" (GSM1k)** — Demonstrates data contamination in mathematical reasoning evaluation and provides methodology for creating clean benchmarks.

- **"FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing"** — First dataset enabling research on predicting actual student responses at scale, with 1.7 million K-12 interactions and Common Core alignment.

- **"Beyond Final Answers: Evaluating Large Language Models for Math Tutoring"** — Moves evaluation beyond correctness to tutoring quality, finding only 56.6% of dialogues were error-free despite high final answer accuracy.

- **"Student Data Paradox: Regressive Side Effects of Training LLMs for Personalized Learning"** — Reveals a fundamental tension between modelling student behaviour and maintaining model correctness, with direct implications for AI tutor development.

- **"MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams"** — The largest and most comprehensive K-12 multimodal benchmark, with 141,000 questions and a dynamic evaluation framework.

- **"Multilingual Performance of AI on Physics Concept Inventories"** — Comprehensive evaluation across 20+ languages revealing significant performance disparities and equity concerns relevant to global education.

- **"BanglaMATH: A Bangla Benchmark Dataset for Testing LLM Mathematical Reasoning"** — Pioneering benchmark for a major low-resource language, demonstrating both feasibility and necessity of multilingual evaluation.

- **"Are NLP Models Really Able to Solve Simple Math Word Problems?" (SVAMP)** — Demonstrates through elegant experimental design that models can achieve high accuracy without processing question content, revealing superficial pattern matching.