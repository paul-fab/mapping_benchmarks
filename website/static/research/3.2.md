# Content Alignment: How Well Do LLMs Match Curriculum Standards in K-12 Education?

## Executive Summary

A growing body of research — **37 papers** in this analysis — is examining whether large language models (LLMs) can generate and evaluate educational content that genuinely aligns with established curriculum standards, learning objectives, and age-appropriate difficulty levels. This is a foundational question for anyone considering deploying AI-EdTech at scale in classrooms across low- and middle-income countries (LMICs) and beyond. If LLM-generated content does not match what students are supposed to be learning, at the right level of challenge, the technology risks being not merely unhelpful but actively harmful to learning progression.

The research reveals a striking paradox: **models consistently perform better on higher-grade content than on elementary material**, despite the foundational importance of early-grade learning. Across multiple benchmarks — from Chinese K-12 examinations (E-EVAL, EduEval) to Indonesian multi-task assessments (IndoMMLU) to Indian mathematics curricula (MathQuest) — LLMs demonstrate strong language comprehension but struggle significantly with mathematical reasoning. Performance gaps between English and other languages remain substantial, with studies showing that models evaluated in Indonesian, for instance, only pass at a primary school level when tested on locally relevant content. These findings carry serious implications for equity: without deliberate investment, AI-generated educational materials risk widening the gap between English-dominant and multilingual learning contexts.

Critically, almost all of the measurement in this area focusses on whether generated content *looks right* — matching standards, readability levels, and taxonomic classifications — rather than whether it *works*. Not a single study in this corpus measures long-term learning outcomes when students use LLM-generated materials in real classrooms. This means the field is building increasingly sophisticated alignment tools on an untested assumption: that content which matches curriculum standards on paper will translate to effective learning in practice.

## Key Themes

### Curriculum-aligned benchmark development

The most active area of research, represented by **15 papers**, involves creating large-scale datasets that map LLM performance to specific national or international curriculum standards. These benchmarks use authentic examination materials — real questions from real exams — to test whether models can handle the kinds of tasks students actually face. **E-EVAL** offers 4,351 multiple-choice questions spanning nine subjects across China's K-12 system. **MDK12-Bench** is substantially larger, comprising **141,000 instances** across six disciplines with 6,225 knowledge points organised in a six-layer taxonomy. The **IndoMMLU** benchmark provides 14,981 questions across 64 tasks for Indonesian, with 46% focussing on Indonesian language proficiency and coverage of nine local languages and cultures.

These benchmarks consistently reveal that models tested during 2023–2024 — including GPT-4, which was the leading commercial model for much of this period — perform unevenly across grade levels and subjects. The finding that elementary content proves harder than secondary content is counterintuitive but has been replicated across multiple studies and languages. This suggests that the conceptual simplicity of early-grade material may mask linguistic and pedagogical complexities that current models handle poorly.

### Taxonomy-based assessment and classification

Nine papers focus on whether LLM outputs align with educational taxonomies such as **Bloom's cognitive levels** and **Webb's Depth of Knowledge (DOK)**. The EduEval benchmark is notable here, organising over **11,000 questions** across 24 task types along six cognitive dimensions — from memorisation through to creativity and ethics. **BloomNet** proposes a transformer-based classifier for automatically categorising learning outcomes against Bloom's taxonomy, while research from the CECIIS 2025 proceedings investigates prompt engineering strategies specifically designed to improve cognitive alignment in LLM-generated content.

A key lesson here is that surface-level accuracy — getting the right answer — does not equate to operating at the right cognitive level. Models may answer a question correctly while bypassing the reasoning process that the question was designed to assess. This distinction matters enormously for educational content generation, where the *how* of arriving at an answer is often more pedagogically important than the answer itself.

### Readability and grade-level appropriateness

Ten papers examine whether LLMs can generate or adapt content to specific reading levels. Research using established metrics — Flesch-Kincaid Grade Level, CEFR levels, Dale-Chall, and Gunning Fog indices — reveals both promise and limitation. The study "Classroom AI: Large Language Models as Grade-Specific Teachers" demonstrates a **35.64 percentage point improvement** in grade-level accuracy through fine-tuning, establishing that models can be trained for pedagogically grounded readability. The **EduAdapt** benchmark provides a question-answer dataset specifically designed to evaluate grade-level adaptability.

However, research such as "Beyond Flesch-Kincaid" and "Flesch or Fumble?" highlights that traditional readability metrics are blunt instruments. Prompt-based metrics that go beyond simple formula calculations show improved difficulty classification, suggesting that the field needs more nuanced approaches to measuring whether content is truly appropriate for a given learner — not merely readable at a given grade level.

### Multilingual and cross-cultural alignment

Six papers address how well LLMs maintain curriculum alignment across languages and cultural contexts — a question of direct relevance to LMICs. The findings are sobering. Research on multilingual mathematics problem generation reveals **consistent linguistic bias**, with English solutions rated highest and Arabic lowest across three LLMs tested. The **VNHSGE** dataset for Vietnamese high school examinations and the IndoMMLU benchmark for Indonesian both demonstrate that models trained predominantly on English data struggle to handle culturally specific content, local knowledge, and non-Latin scripts.

These performance gaps are not trivial. For countries considering AI-EdTech deployments, they represent a fundamental question of equity: can these tools serve learners in Bahasa Indonesia, Vietnamese, or Kiswahili as well as they serve learners in English? The current evidence suggests they cannot — at least not without significant investment in multilingual training data and culturally grounded benchmarks.

### Automated lesson planning and instructional design

Eight papers explore using LLMs — often in **multi-agent architectures** — to generate complete lesson plans and instructional materials aligned with specific learning objectives. The study "Enabling Multi-Agent Systems as Learning Designers" embeds the Knowledge-Learning-Instruction (KLI) framework into a collaborative agent system, demonstrating that multiple specialised agents working together produce more pedagogically sound content than single agents. Similarly, **FACET** proposes a teacher-centred multi-agent system, while **EduPlanner** offers a multiagent approach to customised instructional design.

Educator preference studies — including "Connecting Feedback to Choice" — reveal that teachers value different qualities in AI-generated versus human-created lesson plans, suggesting that AI tools should augment rather than replace teacher planning. This aligns with our broader view that effective AI-EdTech must be designed with the education workforce, not simply for them.

## What Is Being Measured

The field has developed a sophisticated measurement apparatus focussed on several dimensions. **Curriculum alignment** is assessed by testing LLMs against specific standards — Common Core State Standards (CCSS), Next Generation Science Standards (NGSS), India's NCERT framework, and various national examination syllabi. **Cognitive level classification** is measured against Bloom's taxonomy and Webb's DOK framework, using both automated classifiers and human expert validation. **Readability** is quantified through established formulae (Flesch-Kincaid, CEFR, Dale-Chall) alongside newer prompt-based metrics.

Beyond these, researchers measure **cross-difficulty performance** (easy, medium, hard question sets), **temporal robustness** (how models handle newer versus older exam questions — a proxy for data contamination), **subject-specific accuracy** across STEM and humanities, and **multilingual performance gaps**. Expert educator ratings on pedagogical quality — relevance, clarity, and appropriateness — feature in comparative studies. The **STANDARDIZE framework** demonstrates **45–100% improvement** in alignment accuracy through explicit standard-based prompting, offering a practical methodology for incorporating expert-defined standards into content generation workflows.

## Critical Gaps

The most consequential gap is the absence of **longitudinal studies measuring actual learning outcomes** when students use LLM-generated materials. Every benchmark in this corpus measures content quality as a proxy for educational effectiveness — but the link between the two has not been established. This represents a significant opportunity for funders and research organisations.

Equally concerning is the lack of research on **student misconception formation**. We do not know whether AI-generated content addresses or reinforces common misunderstandings — a question that matters acutely for mathematics and science, where misconceptions are well-documented and persistent. Nor is there meaningful research on the impact of LLM-aligned content on **metacognitive skill development**, **productive struggle**, or **transfer of learning** to novel contexts.

From an implementation perspective, the field has not addressed **teacher implementation fidelity** — how educators actually use, adapt, or override AI-generated materials in real classrooms. The **cost-benefit analysis** of AI generation versus human creation at scale remains unexplored. For LMICs specifically, **cultural appropriateness beyond surface-level translation** is a critical blind spot: matching a national curriculum standard does not guarantee that content resonates with local pedagogical traditions, community values, or students' lived experience. There is also a notable absence of research on **equity impacts** across diverse student populations — including learners with special educational needs, English language learners, and students from low socio-economic backgrounds.

## Notable Benchmarks and Datasets

**MDK12-Bench** stands out as the largest and most comprehensive benchmark, with **141,000 instances** and a structured six-layer knowledge taxonomy. Its dynamic evaluation framework — designed to test generalisation beyond static datasets — addresses the growing problem of data contamination as LLM training corpora expand. **EduEval** goes beyond multiple-choice formats to include authentic classroom scenarios such as dialogue classification and essay scoring, organised across six cognitive dimensions. This makes it one of the few benchmarks that tests pedagogical reasoning rather than merely content knowledge.

**FoundationalASSIST** is unique in providing **1.7 million K-12 mathematics interactions** with actual student responses, distractor selections, and Common Core alignment. This is the only English-language dataset that enables research on why certain items are effective — a question of direct relevance to content generation. For multilingual contexts, **IndoMMLU** remains the most important LMIC-relevant benchmark, revealing that even the most capable models available during 2023–2024 dramatically underperformed on Indonesian content compared to English equivalents. The **STANDARDIZE framework** offers a practical, generalisable approach to aligning generated content with expert-defined standards — demonstrating significant accuracy improvements through retrieval-style in-context learning.

## Methodological Trends

The dominant methodological approach involves **large-scale benchmark creation using authentic examination materials**, with datasets of 10,000 or more questions now typical. Researchers increasingly employ **multi-dimensional evaluation frameworks** that combine automated metrics with human expert judgement — recognising that neither alone is sufficient. Zero-shot and few-shot prompting strategies that explicitly reference standards or taxonomies are common, alongside fine-tuning of specialised models on domain-specific educational datasets.

**Multi-agent architectures** represent an emerging trend, with separate agents handling evaluation, optimisation, and content generation. Cross-validation using multiple LLMs as judges — including GPT-4, Claude, Gemini, and open-source models available during 2023–2024 — is increasingly standard, though concerns about bias and consistency in "LLM-as-judge" approaches remain. Pairwise preference studies with educators offer a more grounded form of evaluation. **Retrieval Augmented Generation (RAG)** integrated with curriculum knowledge bases shows promise for improving alignment without full model retraining — a particularly relevant approach for resource-constrained LMIC deployments. Given the rapid pace of model development since many of these studies were conducted, findings may need re-evaluation with current-generation models.

## Recommendations

We recommend that **funders prioritise longitudinal classroom studies** measuring actual learning outcomes — not just content quality ratings — when students use LLM-generated materials aligned to different standards. This is the single most important evidence gap in this area *(target: commissioning studies by late 2026)*.

The field should **invest specifically in elementary-level content quality**. The consistent finding that models underperform on early-grade material is deeply concerning given the centrality of foundational literacy and numeracy (FLN) to learning progression in LMICs. Alongside this, **mathematical reasoning gaps** should be addressed systematically — this remains the most consistent weakness across all benchmarks regardless of model capability.

We recommend the development of **cross-cultural validation protocols** that go beyond translation to ensure curriculum alignment frameworks transfer appropriately across educational systems. For LMIC contexts, this means building benchmarks grounded in local curricula, local languages, and local pedagogical traditions — not simply adapting English-language tools. Multilingual fairness should be treated as a first-order concern, not an afterthought.

The field should **standardise evaluation methodologies** across benchmarks to enable meaningful meta-analysis *(ongoing)*. Current diversity in metrics — while reflecting genuine complexity — prevents cross-study comparison and makes it difficult to track progress over time. We also recommend developing **dynamic benchmarks** that evolve to prevent data contamination, building on the approach pioneered by MDK12-Bench.

Finally, we recommend that **teacher-in-the-loop systems** be designed from the outset — enabling educators to efficiently validate, adapt, and customise AI-generated content rather than facing binary accept/reject decisions. This is essential for building trust and ensuring that alignment tools serve the education workforce rather than bypassing it.

## Key Papers

- **MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams** — The largest and most comprehensive K-12 benchmark (141,000 instances, six-layer knowledge taxonomy) with a dynamic evaluation framework addressing data contamination. Reveals systematic performance patterns across difficulty, temporal, and contextual dimensions.

- **EduEval: A Hierarchical Cognitive Benchmark for Evaluating Large Language Models in Chinese Education** — First benchmark to evaluate authentic teaching scenarios (classroom dialogue, essay scoring) across hierarchical cognitive dimensions grounded in Bloom's and Webb's frameworks. Reveals critical gaps between knowledge recall and contextual application.

- **FoundationalASSIST: An Educational Dataset for Foundational Knowledge Tracing and Pedagogical Grounding** — The only English dataset providing actual student responses, distractor selections, and full text with Common Core alignment. Enables research on what makes assessment items pedagogically effective — not just whether models answer correctly.

- **Classroom AI: Large Language Models as Grade-Specific Teachers** — Demonstrates a 35.64 percentage point improvement through fine-tuning for grade-level adaptation. Establishes a methodology for training models on pedagogically grounded readability integrating multiple frameworks.

- **Enabling Multi-Agent Systems as Learning Designers** — Embeds the Knowledge-Learning-Instruction framework into a multi-agent architecture, showing that collaborative agents produce more pedagogically sound content than single agents. A promising methodology for incorporating learning science into LLM systems.

- **Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs** — Reveals consistent linguistic bias with English solutions rated highest and Arabic lowest across three LLMs. Essential reading for anyone concerned with equity in AI-generated educational content for multilingual LMIC contexts.