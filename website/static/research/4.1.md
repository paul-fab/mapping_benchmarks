# Scoring and Grading: Benchmarking AI in Automated Educational Assessment

## Executive Summary

Automated scoring and grading represents one of the most mature and extensively researched areas in educational AI, with **139 papers** spanning automated essay scoring (AES), short answer grading, programming assessment, and domain-specific evaluation across multiple languages and educational contexts. The field has undergone a significant methodological evolution — from hand-crafted linguistic features and vector space models through to deep learning architectures and, most recently, large language models (LLMs) used in zero-shot and few-shot configurations. Neural network models now routinely achieve human-level agreement on established benchmarks, with quadratic weighted kappa (QWK) scores exceeding **0.80** on standard datasets such as ASAP.

However, a critical tension sits at the heart of this body of research. The overwhelming emphasis on statistical agreement with human raters — measured through QWK, Pearson correlation, and root mean square error (RMSE) — has come at the expense of deeper questions about **pedagogical validity**, **fairness**, and **real-world impact on learning**. Most systems treat human scores as perfect ground truth, yet inter-rater disagreement is well documented. LLMs frequently assign systematically different scores than humans — often harsher — and most systems remain opaque in their scoring rationales. Only **5 of 139 papers** meaningfully address cognitive offloading or learning science concerns. The field now stands at a juncture where technical capability has outpaced our understanding of whether these systems support genuine learning, serve all students equitably, or hold up under adversarial scrutiny. This represents both a significant gap and a major opportunity for the education sector.

## Key Themes

### The Rise of LLMs in Essay and Short Answer Scoring

The largest cluster of research — **28 papers** — examines the use of deep learning and LLMs for automated essay scoring. Studies such as *Are Large Language Models Good Essay Graders?* provide comprehensive empirical evaluations revealing that models including GPT-4 (which was the leading commercial model at the time of many of these studies), Claude, and Gemini can achieve near human-level performance but systematically diverge from human grading patterns. The paper *Can Large Language Models Make the Grade?* demonstrates that GPT-4 achieved a **QWK of 0.70** on K-12 short answers across subjects — close to, but below, the human baseline of **0.75** — with minimal prompt engineering. These findings suggest LLMs are capable assessors but not yet reliable replacements for human judgement, particularly in high-stakes contexts.

Alongside this, **18 papers** focus specifically on **short answer and constructed response grading** in STEM subjects. Work such as the *AMMORE dataset* study — which includes **53,000 math open-response answers from West African students** — demonstrates how chain-of-thought prompting can improve grading accuracy on difficult edge cases. This is particularly relevant for low- and middle-income countries (LMICs) where teacher capacity for individualised assessment may be constrained.

### Multi-Dimensional and Trait-Based Assessment

A substantial body of work — **18 papers** — recognises that writing quality comprises multiple dimensions. The paper *Teach-to-Reason with Scoring* introduces a novel framework that distils LLM reasoning into smaller models while generating score-justified rationales across traits such as coherence, vocabulary, grammar, and argumentation. The extended **ASAP++ benchmark** enables trait-level evaluation, moving beyond holistic scores to capture the multifaceted nature of student writing. This shift from single-score to analytic assessment aligns more closely with how teachers actually evaluate student work and represents a meaningful step toward pedagogically useful automated assessment.

### Multilingual and Low-Resource Language Scoring

Nine papers address automated scoring in languages beyond English — including Arabic, Portuguese, Chinese, Indonesian, and Hebrew. The study *How well can LLMs Grade Essays in Arabic?* reveals substantial performance gaps when models trained primarily on English data are applied to non-English contexts. *From Handwriting to Feedback* explores the use of vision-language models for assessing handwritten work in Indonesian classrooms — a context where digitised text is not available. These studies highlight a critical equity dimension: the majority of benchmarks and training data remain English-dominant, meaning that **automated scoring systems risk being least effective precisely where they could have the greatest impact** in under-resourced educational settings.

### Explainability, Feedback, and the Black Box Problem

Twelve papers address the generation of explanations and actionable feedback alongside scores. *Explainable Automated Essay Scoring: Deep Learning Really Has Pedagogical Value* demonstrates how SHAP explanations can make neural scoring models interpretable, achieving **85%+ accuracy** with detailed feature analysis. The *ExASAG* framework integrates gradient-based methods to produce natural language explanations aligned with human rationales. However, most systems still operate as black boxes — a significant barrier to teacher trust and student understanding of how to improve.

### Robustness, Adversarial Vulnerability, and Gaming

Twelve papers examine system vulnerabilities — and the findings are sobering. The *Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems* reveals that models trained on the ASAP dataset are systematically vulnerable to irrelevant content insertion, length manipulation, and stylistic gaming. *Neural Automated Essay Scoring and Coherence Modeling for Adversarially Crafted Input* demonstrates that leading neural models fail to detect grammatically correct but semantically incoherent sequences. These vulnerabilities raise serious concerns about deploying automated scoring in high-stakes contexts without robust adversarial testing.

### Human-AI Collaboration and Hybrid Systems

Fourteen papers explore frameworks where AI works alongside human evaluators rather than replacing them. *Implementation Considerations for Automated AI Grading of Student Work* is one of the few studies examining actual classroom deployment, revealing significant **trust issues** among teachers and underscoring the need for human oversight. *Generative Grading* demonstrates near-human accuracy while maintaining interpretability through decision tree extraction and human-in-the-loop validation. This theme points toward the most promising deployment model: AI as an assistive tool that handles triage and preliminary scoring while preserving human judgement for nuanced assessment decisions.

## What Is Being Measured

The field's measurement apparatus is heavily weighted toward **technical accuracy metrics**. The dominant measures include quadratic weighted kappa (QWK), Cohen's kappa, Pearson correlation coefficients, exact match accuracy, and RMSE — all of which quantify agreement between automated scores and human rater scores treated as ground truth. Cross-prompt generalisation, zero-shot and few-shot learning performance, and computational efficiency are increasingly evaluated. Trait-specific accuracy is measured across dimensions including coherence, syntax, vocabulary, phraseology, grammar, and conventions, particularly in benchmarks such as the **ELLIPSE Corpus** for English Language Learners.

Beyond scoring accuracy, some studies measure semantic similarity between student and reference answers, surface features such as essay length and vocabulary diversity, syntactic complexity, and item difficulty parameters using Item Response Theory (IRT). The **ENEM Corpus** from Brazil, for example, provides IRT-calibrated difficulty parameters across **1,031 exam questions** in Portuguese, enabling psychometric validation of automated scoring. Robustness is measured as score stability under adversarial perturbation, and a small number of studies assess subjectivity in rater comments as a proxy for bias.

## Critical Gaps

The most striking gap is the near-total absence of research on **whether automated scoring actually improves student learning**. No papers in this corpus report randomised controlled trials (RCTs) or quasi-experimental studies examining whether automated scoring or feedback leads to improved writing quality, deeper understanding, or enhanced metacognitive skills over time. The evidence base is purely correlational and cross-sectional.

Equally concerning is the limited attention to **fairness and demographic bias**. Only **4 papers** directly investigate scoring disparities across student populations, and systematic demographic fairness audits — including differential item functioning (DIF) analysis for English Language Learners, students with disabilities, and underrepresented groups — are virtually absent. The paper *Automated Essay Scoring in the Presence of Biased Ratings* is a notable exception, demonstrating that removing biased training samples improves model quality, but this work remains isolated.

Other critical gaps include: the absence of **longitudinal benchmarks** tracking individual students over time; minimal evaluation of **consequential validity** — how automated scoring changes teaching practices, curriculum design, and student writing strategies; insufficient assessment of whether systems capture **pedagogically meaningful constructs** versus surface features; and a near-complete lack of **cost-benefit analysis** comparing automated and human grading in real-world deployment contexts. The question of **construct validity** — whether automated scores actually measure intended learning outcomes — remains largely unaddressed.

## Cognitive Offloading and Learning Science Concerns

This area receives strikingly little attention. Only **5 of 139 papers** address cognitive offloading, over-reliance on automated scores, or learning science concerns about productive struggle — and even these do so tangentially. No papers directly measure whether automated scoring reduces students' development of **self-evaluation abilities** or critical thinking about their own work. No study examines whether immediate automated scores discourage the revision processes that are central to writing development.

The *FoundationalASSIST* paper mentions personalised learning and student modelling but does not evaluate cognitive effects. The *Implementation Considerations* study touches on concerns about over-reliance peripherally. One paper on adversarial robustness implicitly raises the spectre of students learning to "write to the algorithm" rather than developing genuine writing skills — but this is not analysed from a learning science perspective. This represents a fundamental blind spot: the field is building increasingly capable scoring systems without investigating whether they support or undermine the learning processes they are meant to serve.

## Notable Benchmarks and Datasets

The **ASAP (Automated Student Assessment Prize)** dataset, originating from a 2012 Kaggle competition, remains the most widely used AES benchmark, providing holistic essay scores across 8 prompts. Despite its ubiquity, it is criticised for limited prompt diversity and rubric ambiguity. Its extension, **ASAP++**, adds trait-level scores across ideas, organisation, style, and conventions, enabling multi-dimensional assessment research.

The **FoundationalASSIST** dataset represents a significant advance, offering **1.7 million K-12 mathematics interactions** with full question text, actual student responses, and Common Core alignment — the first dataset enabling LLM-based knowledge tracing with complete textual content. For LMIC contexts, the **AMMORE dataset** provides **53,000 math open-response answers from West African students**, directly addressing low-resource educational contexts and testing robustness on challenging edge cases.

**SAS-Bench** offers fine-grained evaluation of short answer scoring with **1,030 questions and 4,109 responses** annotated with step-wise scoring and error categories. **EduEval** provides **11,000+ questions** across 24 task types spanning six cognitive dimensions aligned to Bloom's Taxonomy in K-12 Chinese education. The **ENEM Corpus** from Brazil adds psychometric rigour with IRT-calibrated difficulty parameters across **1,031 high-stakes exam questions** in Portuguese.

The **Adversarial ASAP Dataset** — comprising 2,600 essays modified with perturbations at varying intensities — is essential for robustness testing. Meanwhile, the **Brazilian National Exam Essay Corpus with Rater Comments** is the first benchmark explicitly designed to study rater bias, including mandatory rater commentary that enables subjectivity analysis.

## Methodological Trends

The field has undergone a clear methodological progression. Earlier work relied on feature engineering — TF-IDF weighting, n-gram representations, readability metrics, and similarity measures such as cosine similarity and Jaccard coefficients. These approaches have been largely superseded by end-to-end neural architectures, with widespread adoption of pre-trained transformer models including BERT, RoBERTa, ELECTRA, and DeBERTa, fine-tuned on domain-specific essay datasets.

The current frontier involves LLMs — including models from the GPT, Claude, Gemini, and Llama families — applied through **prompt engineering, chain-of-thought prompting, and rubric-guided few-shot strategies** rather than fine-tuning. Multi-task learning frameworks that jointly optimise scoring and related tasks (such as feedback generation and knowledge tracing) are increasingly common. Ordinal regression approaches such as CORAL that respect the ordered nature of score levels represent a methodological refinement over standard classification.

Emerging approaches include **knowledge distillation** from large models to smaller, deployable student models; **neuro-symbolic architectures** combining neural representations with logic-based reasoning; **multimodal assessment** combining vision and language models for handwritten work; and **pairwise comparison** approaches rather than direct score generation. Post-hoc calibration layers to align AI scores with human score distributions address the systematic scoring differences observed with LLMs. However, it is worth noting that many studies in this corpus evaluated models that are now several generations old — findings based on GPT-3.5 or early GPT-4, for instance, may need re-evaluation with current-generation models.

## Recommendations

We recommend that the field undertake a **paradigm shift in evaluation**, moving beyond agreement metrics (QWK, correlation) to include construct validity, consequential validity, fairness audits, and adversarial robustness testing. Agreement with human raters is necessary but not sufficient for educational deployment.

The most urgent investment priority is **longitudinal research on learning outcomes**. The field should commission RCTs and quasi-experimental studies examining whether automated scoring and feedback improves student writing quality over time compared to human feedback or no feedback *(target: initial studies commissioned by end of 2026)*. Current evidence is purely correlational — this gap must be closed before systems are deployed at scale.

We recommend that all automated scoring systems be evaluated against **systematic adversarial attacks** before deployment, using standardised test suites such as the Adversarial ASAP toolkit. Alongside this, demographic **fairness audits** — including DIF analysis and subgroup performance evaluation — should become mandatory reporting requirements.

For LMIC contexts, we recommend prioritising investment in **multilingual and low-resource language benchmarks and training data**. The field should develop language-agnostic frameworks and expand research to morphologically complex and non-Latin script languages *(ongoing, with initial benchmarks targeted by mid-2027)*. The AMMORE and AR-AES datasets demonstrate what is possible; this work needs significant scaling.

The field should invest in **human-AI collaboration frameworks** rather than pursuing full automation. This means designing systems where AI handles triage and preliminary scoring while human educators make final judgements — and testing these hybrid systems in authentic classroom contexts in LMICs. Research on **teacher trust, adoption, and practical usability** is essential for any deployment pathway.

Finally, we recommend that cognitive and learning scientists be brought into automated scoring research programmes to investigate whether these systems support or undermine metacognitive development, productive struggle, and self-assessment skills. Without this evidence, we risk scaling systems that optimise for efficiency at the cost of learning.

## Key Papers

- **FoundationalASSIST** — First dataset providing complete question text and actual student responses at scale (1.7M K-12 maths interactions), enabling LLM-based knowledge tracing beyond binary correctness labels.

- **Are Large Language Models Good Essay Graders?** — Comprehensive empirical evaluation revealing that LLMs systematically differ from human grading patterns, identifying key limitations in current LLM-based scoring.

- **Can Large Language Models Make the Grade?** — Demonstrates GPT-4 achieves near human-level performance (QWK 0.70 vs. 0.75) on K-12 short answers across subjects with minimal prompt engineering.

- **Evaluation Toolkit For Robustness Testing Of Automatic Essay Scoring Systems** — Critical examination revealing systematic vulnerabilities in leading models to adversarial content, challenging reliability claims based solely on agreement scores.

- **Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation** — Questions fundamental assumptions about inter-rater reliability as a validation standard, proposing alternative evaluation frameworks emphasising validity and educational impact.

- **Learning to Love Edge Cases in Formative Math Assessment (AMMORE)** — Addresses the critical challenge of grading difficult-to-score responses from West African students, demonstrating that grading accuracy impacts downstream knowledge tracing.

- **How well can LLMs Grade Essays in Arabic?** — Critical evaluation of LLM scoring in low-resource language contexts, revealing substantial performance gaps and the need for language-specific approaches.

- **Automated Essay Scoring in the Presence of Biased Ratings** — First work to systematically investigate how human rater bias propagates into AES models, demonstrating that removing biased training samples improves model quality.

- **The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges** — Addresses fairness, bias, transparency, and consequential validity concerns largely absent from technical papers; provides an ethical framework for deployment.

- **Teach-to-Reason with Scoring** — Novel framework distilling LLM reasoning into smaller models while generating score-justified rationales, advancing interpretable automated assessment.