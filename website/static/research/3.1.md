# Content Knowledge: Benchmarking LLM Mastery of K-12 Subject Matter

## Executive Summary

Content knowledge — the ability of large language models (LLMs) to demonstrate mastery of K-12 subject matter across science, mathematics, humanities, and languages — is the most extensively benchmarked dimension of AI in education. Our analysis covers **211 papers** spanning a remarkable breadth of evaluation approaches, from static standardised test performance to adaptive intelligent tutoring systems that track student knowledge over time. The field has produced an impressive infrastructure of benchmarks, datasets, and evaluation frameworks. Yet a striking paradox emerges: **models that excel at solving problems consistently fail at teaching them**, and high accuracy on standard benchmarks frequently masks reliance on shallow heuristics rather than genuine understanding.

The numbers tell a compelling story of both capability and limitation. GPT-4o — which was among the leading commercial models during the period covered by most of these studies — achieves only **31% accuracy on the MM-MATH multimodal mathematics benchmark**, compared to **82% human performance**. Models experience accuracy drops of **up to 20%** when problems are only slightly rephrased in the GSM-PLUS robustness benchmark. On the TutorBench evaluation of tutoring quality, no frontier LLM exceeds **56% overall performance**, and all achieve less than 60% on criteria related to guiding, diagnosing, and supporting students. These findings carry significant implications for deployment in low- and middle-income countries (LMICs), where the promise of AI-EdTech as a scalable complement to limited teaching capacity depends on the technology doing more than simply producing correct answers.

Methodologically, the papers cluster around three approaches: static benchmark evaluation using existing K-12 test datasets, intelligent tutoring systems that assess content knowledge through dialogue and problem-solving, and adaptive learning platforms that track knowledge states over time. A dominant trend is the creation of large-scale, hierarchical datasets — such as the **STEM benchmark** (448 skills, over 1 million questions), **CMMaTH** (23,000 Chinese multimodal maths problems), and **MDK12-Bench** (141,000 multimodal exam instances) — that systematically organise questions by grade level, difficulty, and cognitive taxonomy. However, the overwhelming focus remains on evaluating what AI systems can do rather than measuring what students actually learn when using them.

---

## Key Themes

### Standardised test performance reveals uneven capabilities

A substantial body of work — approximately 15 papers — evaluates LLM performance on curriculum-aligned K-12 examinations across multiple subjects and grade levels. Benchmarks such as **E-EVAL** (4,351 questions across the Chinese K-12 curriculum), **EduEval**, and **MDK12-Bench** provide structured assessments spanning mathematics, science, language arts, and social studies. These studies consistently find that models perform well on factual recall and straightforward procedural tasks but struggle with multi-step reasoning, visual-spatial problems, and questions requiring integration of knowledge across domains. The **E-EVAL** benchmark is notable for deliberately avoiding national exam questions to prevent data contamination — a methodological concern that has become increasingly urgent as training datasets expand. Findings from these benchmarks also reveal that models trained predominantly on Chinese-language data outperform English-dominant models on Chinese educational content, underscoring the importance of **linguistically and culturally grounded evaluation**.

### Mathematics dominates but exposes deep fragility

Mathematical reasoning and problem-solving feature in over **42 papers**, making it by far the most evaluated content domain. Benchmarks range from elementary arithmetic (GSM8K, CMATH) through secondary algebra and geometry (FineMath, MM-MATH) to competition-level problems (Physics Olympiad evaluations). The foundational **GSM8K** benchmark has become the de facto standard for evaluating mathematical reasoning, though concerns about benchmark contamination prompted the creation of **GSM1k** — a parallel dataset designed to guarantee no training data overlap. Perhaps the most important finding in this cluster comes from **GSM-PLUS**, which demonstrates that models experience accuracy drops of up to 20% when problems are subjected to minor perturbations such as numerical variation, distractor insertion, or rephrasing. The paper "Are NLP Models really able to Solve Simple Math Word Problems?" goes further, demonstrating that models can achieve high scores **without reading the question text or respecting word order** — a sobering finding that fundamentally challenges claims of mathematical understanding.

### The pedagogical-performance gap is stark

Eighteen papers examine the integration of content mastery with pedagogical effectiveness — and the results are consistently troubling. **TutorBench**, the first benchmark explicitly separating content knowledge from tutoring capability, finds that frontier LLMs achieve less than 60% pass rates on criteria related to guiding, diagnosing, and supporting students. The **MathDial** dataset of 3,000 tutoring dialogues reveals that LLMs frequently "leak" solutions rather than scaffolding student reasoning, reducing the productive struggle essential for learning. Systems such as **BEETLE II**, **AutoTutor**, and **Bridge** have long been designed explicitly to avoid answer-giving, prioritising hints and Socratic questioning — yet current general-purpose LLMs struggle to replicate this pedagogical discipline. The **LearnLM-Tutor** paper, which reports deployment at scale with 67,000 students, establishes a comprehensive evaluation framework grounded in learning science and notes that students can develop over-reliance when systems provide direct answers.

### Multimodal assessment reveals critical weaknesses

Approximately 15 papers focus on benchmarks that combine text with visual information — diagrams, charts, graphs, and images — reflecting the reality that STEM education depends heavily on visual representations. The **STEM benchmark** (over 1 million questions across 448 skills) finds models achieving only approximately **55% accuracy** compared to 82% human performance. **VCBench**, which tests reasoning across multiple images per question, finds that even the best models achieve less than 50% on tasks where humans perform near-perfectly. **VisioMath** specifically tests cases where answer options are visually similar diagrams requiring fine-grained discrimination — a common scenario in geometry and spatial reasoning tasks. These findings are particularly relevant for LMIC contexts, where AI-EdTech solutions must handle diverse visual content including hand-drawn diagrams, photographed textbook pages, and culturally specific representations.

### Multilingual evaluation exposes systematic bias

Twelve papers examine content knowledge across non-English languages and culturally specific educational contexts. **BanglaMATH**, the first benchmark for Bangla mathematical reasoning, reveals a **6.5% or greater accuracy drop** compared to equivalent English problems — a significant gap for a language spoken by over 230 million people. **IndoMMLU**, covering 14,981 questions in Indonesian including nine local languages, finds that LLMs "only pass primary school exams" in that context. **EXAMS-V** provides 20,932 questions across 20 subjects in 11 languages using real school examinations rather than translations, representing a significant methodological advance. The **Multilingual Physics Concept Inventories** study evaluates performance across 32 languages and 7 physics subjects, revealing systematic weaknesses that increase with linguistic distance from English. This evidence has direct implications for the feasibility of deploying AI-EdTech at scale across diverse LMIC education systems.

### Knowledge tracing bridges AI capability and student modelling

Approximately 15 papers address knowledge tracing — the task of modelling students' evolving knowledge states over time based on their interaction histories. The foundational **Deep Knowledge Tracing** work demonstrated a 25% improvement over Bayesian Knowledge Tracing on the ASSISTments dataset, establishing the paradigm for neural approaches. More recent work, including **PSI-KT** and **LLM-KT**, explores how large language models can enhance knowledge tracing through richer representations of questions and student responses. **FoundationalASSIST** represents a significant advance by providing 1.7 million K-12 maths interactions with full question text and actual student responses — not just correctness indicators — enabling research on misconception patterns and cognitive student modelling that was previously impossible.

---

## What Is Being Measured

The field has developed a comprehensive — if somewhat narrow — infrastructure for measuring content knowledge. At the most basic level, benchmarks assess **answer correctness** on multiple-choice, short-answer, and open-ended questions across K-12 subjects, typically calibrated by grade level and difficulty tier. More sophisticated evaluations measure **subject breadth** (number of disciplines covered), **depth** (grade-level progression from primary through secondary), and **alignment with educational standards** such as Common Core, NGSS, and national curricula.

Several benchmarks employ established psychometric frameworks. **Item Response Theory (IRT)** parameters — including difficulty and discrimination indices — are used to evaluate AI-generated assessment items. Cognitive taxonomies such as **Bloom's taxonomy** and **Webb's Depth of Knowledge** provide hierarchical frameworks for categorising question complexity. Knowledge tracing systems measure **student mastery states** over time, predicting performance and identifying knowledge gaps through sequential interaction data.

Multimodal assessment is a growing frontier, with benchmarks testing the ability to integrate **text, images, diagrams, and mathematical notation**. Process evaluation — examining reasoning steps rather than just final answers — features prominently in newer benchmarks like MM-MATH and several tutoring evaluations. However, the dominant paradigm remains accuracy on closed-form questions, with relatively little attention to open-ended reasoning, creative application, or real-world problem-solving.

---

## Critical Gaps

The most significant gap is the near-complete absence of measurement of **actual student learning outcomes**. The overwhelming majority of papers evaluate whether AI systems can answer questions correctly — not whether students learn more, retain knowledge longer, or develop deeper understanding when using these systems. This represents a fundamental disconnect between the research infrastructure and the educational mission it ostensibly serves.

**Conceptual understanding versus procedural fluency** remains largely unmeasured. Most benchmarks conflate memorisation with comprehension, and the robustness studies suggest that even high accuracy may reflect pattern matching rather than genuine understanding. Closely related is the absence of **transfer assessment** — whether knowledge acquired in one context can be applied in novel situations — which is arguably the most educationally meaningful outcome.

**Longitudinal tracking** of knowledge development is rare. The field relies overwhelmingly on snapshot assessments rather than tracking how understanding evolves, is retained, or decays over time. Knowledge tracing systems model learning trajectories, but these are typically evaluated on prediction accuracy rather than their ability to support pedagogical decisions.

**Equity and demographic stratification** are strikingly absent. Few benchmarks disaggregate performance by student demographics, socioeconomic status, or learning needs. Given the significant language biases already documented, this represents both a measurement gap and an ethical concern — particularly for deployment in diverse LMIC contexts.

**Metacognitive assessment** — whether students or systems can identify what they do not know — receives almost no attention. Nor does **collaborative knowledge construction**, with individual assessment dominating entirely over measures of group learning. The humanities remain significantly underrepresented compared to STEM subjects, and real-world application beyond academic exercises is rarely evaluated.

---

## Cognitive Offloading and Over-Reliance

Although the content knowledge category focuses primarily on measuring what AI systems know rather than how they affect learning, **15 papers** raise substantive concerns about cognitive offloading — the risk that AI assistance may substitute for, rather than support, genuine learning.

The evidence converges on a persistent tension. The **LearnLM-Tutor** paper notes that students develop over-reliance when systems provide direct answers, recommending Socratic questioning as an alternative. **TutorBench** finds that LLMs frequently reveal solutions rather than scaffolding reasoning, directly reducing productive struggle. Multiple mathematics tutoring papers consistently find that **immediate correct answers reduce long-term learning** compared to guided discovery — yet students prefer and report higher satisfaction with systems that give answers directly.

The robustness literature adds a subtler dimension to the cognitive offloading concern. **GSM-PLUS** demonstrates that models can achieve high accuracy yet fail on slight problem variations, suggesting that students who rely on AI explanations may develop **false confidence** in their understanding. The SVAMP benchmark shows models can "solve" problems without processing key information, raising the question of whether students interacting with such systems absorb the superficial pattern or the underlying reasoning.

Several papers explicitly acknowledge this challenge. One systematic review notes that "overuse of ChatGPT may negatively impact innovative capacities and collaborative learning competencies among learners." A mathematics tutoring evaluation acknowledges that "less-than-perfect accuracy can harm learners" and that "students may lose trust in the tutor and develop misconceptions." These findings suggest that **content accuracy alone is an insufficient quality criterion** — the manner and timing of knowledge delivery matter enormously for learning.

---

## Notable Benchmarks and Datasets

The field has produced a rich ecosystem of evaluation tools. Among the most consequential:

**GSM8K** (8,500 grade school maths word problems) remains the most widely adopted standard for mathematical reasoning, though the companion **GSM1k** dataset — created to detect benchmark contamination — has revealed that some models' apparent capabilities may be inflated by memorisation. **GSM-PLUS** extends this with 10,552 problem variations testing eight perturbation types, exposing fragility beneath surface-level accuracy.

**MDK12-Bench** is the largest multimodal K-12 benchmark, with **141,000 exam instances** across six STEM disciplines and 6,225 structured knowledge points. Its dynamic evaluation framework introduces visual and textual perturbations, revealing a **13.7% performance drop** under modified conditions.

**AMMORE** deserves particular attention for LMIC relevance: it comprises **53,000 middle-school maths open responses** from students in Africa using a WhatsApp-based AI tutor, demonstrating that improved grading accuracy (from 98.7% to 99.9%) significantly affects Bayesian Knowledge Tracing estimates of mastery — a direct link between measurement precision and educational consequences.

**EXAMS-V** (20,932 questions, 20 subjects, 11 languages) and **IndoMMLU** (14,981 questions in Indonesian) represent important advances in multilingual evaluation. **BanglaMATH** (1,700 Bangla maths problems) is the first benchmark for a low-resource language, quantifying a bias that many had suspected but few had measured.

For tutoring evaluation, **TutorBench** and **MathDial** stand out for assessing pedagogical quality rather than just content accuracy, whilst **FoundationalASSIST** (1.7 million interactions with full text) enables a new generation of research on misconception patterns and cognitive modelling.

---

## Methodological Trends

The dominant methodology is **benchmark-centric evaluation** comparing LLM accuracy to human or student performance on standardised assessments. Most studies employ **zero-shot or few-shot prompting** rather than fine-tuning, reflecting both the practical accessibility of commercial APIs and the desire to measure general capabilities. The use of **LLM-as-judge** approaches — where models like GPT-4 evaluate generated content — is increasingly common, though its validity against expert human scoring remains insufficiently studied.

A welcome trend is the growing emphasis on **process evaluation alongside outcome evaluation**. Several newer benchmarks examine reasoning steps, solution strategies, and error patterns — not just whether the final answer is correct. **Adversarial and robustness testing** through controlled perturbations is emerging as a methodological standard, though it remains far from universal.

**Item Response Theory** serves as the gold standard for psychometric analysis of assessment quality, and several papers apply IRT to both evaluate AI-generated questions and calibrate benchmark difficulty. **Hierarchical dataset construction** — organising questions by grade level, difficulty tier, knowledge point, and cognitive taxonomy — enables fine-grained analysis of where models succeed and fail.

Notably, **real-world deployment studies remain rare**. The vast majority of evaluations occur in controlled research settings; only a handful — including the LearnLM-Tutor deployment with 67,000 students and the AMMORE study from Africa — examine performance in authentic educational contexts. Cross-lingual evaluation is growing but still concentrated in Chinese and a small number of other languages, with most LMIC languages entirely unrepresented.

---

## Recommendations

We recommend that funders and development partners **prioritise measurement of actual student learning outcomes** over AI model capabilities. The field has invested heavily in determining what models can do but remarkably little in determining whether students learn better with these tools. Randomised controlled trials comparing AI-assisted learning against traditional instruction — with measures of retention, transfer, and metacognitive development — should be a funding priority *(by 2027)*.

The field should **develop robustness evaluation as standard practice**. Every content knowledge benchmark should include perturbation and variation testing to distinguish genuine understanding from pattern matching. The GSM-PLUS approach of systematic problem variation provides an accessible model that should be applied across subjects and languages.

We recommend **significant investment in multilingual and culturally grounded benchmarks** for LMIC education systems. The documented language biases — including the 6.5%+ accuracy drop for Bangla — likely understate the challenge for the many languages with even less digital representation. Benchmarks should be developed using real local examination materials, not translations of English datasets, following the EXAMS-V methodology *(ongoing, with initial coverage of 20+ LMIC languages by 2028)*.

The field should **bridge the pedagogical-performance gap** by creating benchmarks that measure scaffolding quality, hint effectiveness, and the balance between support and productive struggle. The TutorBench and MathDial frameworks provide starting points, but these need to be extended to diverse educational contexts and validated with actual student populations.

We recommend that **equity and demographic stratification become mandatory** in benchmark design. Evaluations should systematically report performance disaggregated by student demographics, language background, and learning needs. Without this, the risk of deploying AI-EdTech systems that work well for some populations and poorly for others remains unquantified.

Finally, the field should **establish consequential validity testing** as standard practice, following the AMMORE model. Measurement accuracy should be evaluated not in isolation but in terms of its impact on downstream educational decisions — mastery estimates, intervention targeting, and progression recommendations.

---

## Key Papers

- **LearnLM-Tutor** (Towards Responsible Development of Generative AI for Education) — Comprehensive evaluation framework spanning quantitative, qualitative, automatic, and human metrics; deployed with 67,000 students, demonstrating methodological rigour for real-world educational AI evaluation.

- **TutorBench** — First benchmark explicitly separating content knowledge from tutoring effectiveness; reveals that no frontier LLM exceeds 56% overall performance on pedagogical criteria despite strong content knowledge.

- **GSM-PLUS** — Critical robustness benchmark demonstrating up to 20% accuracy drops on slight mathematical problem variations, exposing reliance on superficial patterns.

- **AMMORE Dataset** — 53,000 authentic student responses from Africa via WhatsApp AI tutor; demonstrates consequential validity by linking grading accuracy to mastery estimation in under-resourced settings.

- **FoundationalASSIST** — 1.7 million K-12 maths interactions with full question text and student responses; enables research on misconceptions and cognitive modelling previously impossible with identifier-only datasets.

- **MathDial** — 3,000 pedagogically annotated tutoring dialogues demonstrating that fine-tuned models significantly outperform GPT-4 at Socratic tutoring, addressing the gap between solving and teaching.

- **EXAMS-V** — Largest multilingual multimodal exam benchmark (20,932 questions, 11 languages) using real school examinations rather than translations, advancing equitable AI evaluation.

- **BanglaMATH** — First benchmark for low-resource language mathematical reasoning; quantifies a 6.5%+ accuracy drop that has direct implications for AI-EdTech deployment in South Asia.

- **Are NLP Models really able to Solve Simple Math Word Problems?** (SVAMP) — Foundational work revealing that models solve problems without reading questions, fundamentally challenging claims of mathematical understanding.

- **MalAlgoQA** — First systematic evaluation of counterfactual reasoning about student errors; reveals that models identify flawed reasoning at only 66.1% compared to 95.7% for correct answers, exposing a critical gap for tutoring applications.