# Ethics and Bias in AI for K-12 Education

## Executive Summary

The ethical deployment of large language models (LLMs) in K-12 education is among the most consequential — and least resolved — challenges facing the sector. Our analysis of **38 papers** in this category reveals a field that has matured rapidly in identifying risks but remains far from establishing the evidence base needed for responsible deployment at scale. The research spans child-specific safety frameworks, bias detection across demographic groups, academic integrity, transparency, and the growing concern of cognitive offloading — where AI assistance may actively undermine the learning it purports to support.

A central finding is stark: **adult-centric AI safety measures are insufficient for children**. Younger learners face unique developmental vulnerabilities — susceptibility to misinformation, emotional dependency on AI personas, and exposure to age-inappropriate content — that generic safety guardrails do not adequately address. Researchers have responded with specialised benchmarks such as **SproutBench** (1,283 developmentally grounded prompts across 20 child-safety dimensions) and **Safe-Child-LLM** (200 adversarial prompts calibrated to two developmental stages), yet these tools remain largely untested in real classroom settings, particularly in low- and middle-income countries (LMICs).

Perhaps most troubling for education funders and policymakers is the evidence on cognitive offloading. A field experiment with approximately **1,000 Turkish high school students** found that while GPT-4 assistance improved practice problem success by **48%**, students subsequently scored **17% lower on unassisted exams**. This suggests that without careful design, AI tools may create an illusion of learning while actively impeding the independent problem-solving skills that education systems exist to develop. The field urgently needs longitudinal studies, LMIC-specific evaluation frameworks, and clearer accountability structures before scaling deployment.

## Key Themes

### Child-specific safety and age-appropriate design

Twelve papers in this category focus on developing safety frameworks calibrated to children's developmental stages — a recognition that the risks children face when interacting with LLMs differ fundamentally from those facing adults. The **Safe-Child-LLM** benchmark evaluates LLM responses across two age bands (7–12 and 13–17 years), testing models including ChatGPT, Claude, and Gemini for systematic failures in age-appropriate responses. The results reveal that even models with robust general safety features frequently fail on child-specific risk scenarios.

**SproutBench** extends this work considerably, offering 1,283 prompts across three age groups and 20 child-safety dimensions — including emotional dependency prevention, privacy violation detection, and imitation of hazardous behaviours. A key finding is the **"scaling paradox"**: larger, more capable models do not necessarily perform better on child safety. Alongside this, the **EduGuardBench** framework introduced the concept of an **"Educational Transformation Effect"**, where the safest models do not merely refuse harmful requests but actively convert them into teachable moments — a qualitatively different and more pedagogically valuable response pattern.

**MinorBench** takes a different and arguably more grounded approach, drawing its prompts from actual student interactions in a real middle-school chatbot deployment. This benchmark demonstrates that LLMs struggle particularly with **novel child-specific risk scenarios** — such as questions about helium balloon dangers — even when given child-focussed system prompts. This matters. It suggests that safety cannot be achieved through prompt engineering alone.

### Bias and fairness across demographics

Eight papers examine how LLMs exhibit differential performance and treatment based on protected attributes. The paper *LLMs are Biased Teachers* provides the most systematic evaluation to date, analysing **over 17,000 educational explanations** across nine models, four thousand subjects, and multiple difficulty levels. The study introduces two novel metrics — **Mean Absolute Bias (MAB)** and **Maximum Difference Bias (MDB)** — and finds that bias is highest along **income and disability dimensions**, with AI-generated educational content reinforcing stereotypes in personalised learning contexts.

The multilingual bias pipeline developed for mathematics evaluation reveals systematic performance gaps between English and other languages, including German and Arabic. This finding is particularly relevant for LMICs where instruction may occur in languages with limited LLM training data. Research on automated essay scoring demonstrates that pre-existing human rater biases can be amplified rather than corrected when LLMs are trained on biased datasets — a compounding effect that should concern anyone deploying AI-assisted assessment at scale.

### AI-generated content detection and academic integrity

Seven papers address the detection of AI-generated student work, and the findings are sobering. The **AIG-ASAP dataset** demonstrates that relatively simple automated adversarial attacks — word and sentence substitutions — can evade current detection systems while maintaining essay quality. Research on boundary detection in hybrid human-AI essays reveals the difficulty of identifying where student writing ends and AI-generated text begins, a challenge that grows as students become more sophisticated in their use of AI tools.

The paper *Can You Trick the Grader?* examines a related concern: the vulnerability of LLM-based grading systems to adversarial persuasion, where carefully crafted submissions can manipulate automated judges. This represents a fundamental challenge for any AI-assisted assessment system deployed in high-stakes educational contexts.

### Cultural sensitivity and multilingual fairness

Five papers address the critical question of whether LLMs can serve diverse cultural and linguistic contexts equitably. **EduEval** stands out as the first comprehensive hierarchical cognitive benchmark for non-English education, comprising over **11,000 questions** across 24 task types using authentic Chinese K-12 materials organised by Bloom's Taxonomy and Webb's Depth of Knowledge. The development of **KidLM** — a language model specifically designed for children — raises important questions about whether general-purpose models can ever be sufficiently adapted for young learners, or whether purpose-built models are necessary.

Research on culturally relevant pedagogy highlights that LLMs trained predominantly on English-language, Western-centric data may inadvertently impose cultural frameworks inappropriate for diverse learning contexts — a concern of particular relevance for LMIC deployment.

## What Is Being Measured

The field has developed a surprisingly sophisticated — if fragmented — measurement infrastructure. Researchers are evaluating **age-appropriateness of LLM content** across developmental stages (0–6, 7–12, 13–18 years), **detection accuracy** for AI-generated essays under adversarial conditions, and **bias in automated scoring** across race, gender, disability, income, and language dimensions. Safety-utility tradeoffs are measured through **refusal rates** — the frequency with which models decline to engage with queries — alongside fairness metrics such as MAB and MDB for differential treatment in personalised content.

Content safety evaluation spans multiple harm categories including toxic content, misinformation, and age-inappropriate material. Researchers assess **hallucination rates** and factual accuracy in educational responses, **prompt injection and jailbreak vulnerability** in educational contexts, and linguistic performance gaps across languages. Student attitudes are measured using validated scales — the SATAI scale for attitudes toward AI and the AIAS scale for AI anxiety. Production-grade concerns including latency, throughput, and privacy compliance (FERPA, COPPA) are also being evaluated, though primarily in high-income country contexts.

## Critical Gaps

The most consequential gap is the near-total absence of **longitudinal learning outcome data**. The field has invested heavily in measuring what LLMs produce — their outputs, biases, and safety characteristics — but has invested remarkably little in measuring what happens to learners over time. We lack controlled experiments comparing knowledge retention, skill development, and academic achievement with and without LLM assistance across matched cohorts over months or years.

**Transfer effects** — whether LLM-assisted learning generalises to unaided performance on novel problems — remain almost entirely unmeasured. Similarly, **metacognitive development** — changes in students' self-assessment accuracy, help-seeking strategies, and learning sophistication — is rarely tracked. The social-emotional dimensions of AI integration, including long-term effects on collaboration skills, peer learning, and teacher-student relationships, represent another significant blind spot.

For LMIC contexts specifically, there is a critical absence of **equity-in-access data**: classroom-level evidence on which students actually use AI tools, for what purposes, and with what support. **Intersectional bias** — the combined effects of multiple demographic factors, such as the experience of low-income disabled students — is almost never evaluated, despite being among the most policy-relevant questions. The **cultural adaptation effectiveness** question — whether culturally responsive AI actually improves learning for specific communities — lacks any rigorous evidence base.

Implementation fidelity — how closely real-world deployments match safety guidelines — and false positive rates in AI-generated content detection are also neglected. Economic analysis is largely absent: we found no robust cost-effectiveness studies of safety measures or resource allocation models appropriate for under-resourced schools.

## Cognitive Offloading

This is where the evidence is most concerning — and most directly relevant to educational decision-makers. Eight papers address cognitive offloading, and the empirical findings consistently point in the same direction: **AI assistance can improve task performance while simultaneously undermining learning**.

The Turkish field experiment is the most striking example. Approximately 1,000 high school students using GPT-4 (the leading commercial model at the time) for practice problems showed **48% better practice performance** but scored **17% lower on unassisted exams**. This is not a minor effect. It suggests that AI reliance actively impeded the development of independent problem-solving capabilities.

Programming instructors reported parallel findings: students becoming dependent on ChatGPT showed **reduced independent coding ability and creative problem-solving**. Qualitative research revealed students submitting AI-generated work without understanding the content — a pattern that wastes the learning opportunities assignments are designed to create. Notably, **62% of educators surveyed** expressed concern about AI limiting critical thinking development.

Some students have developed what researchers term **"epistemic safeguarding"** — only using ChatGPT for domains where they already possess sufficient knowledge to verify AI outputs. However, participatory design studies revealed that while students are broadly aware of over-reliance risks, they **struggle to self-regulate their AI use**, preferring efficiency over deeper engagement. Students also reported anxiety about AI feedback loops suggesting their work is "never good enough", creating psychological pressure that may further undermine learning.

This evidence base, while still limited, should give pause to anyone planning large-scale AI deployment in education. The field needs standard protocols for detecting and mitigating cognitive offloading effects, including metrics for maintaining independent problem-solving when AI tools are available.

## Notable Benchmarks and Datasets

Several benchmarks merit particular attention. **SproutBench** is currently the most comprehensive child-safety benchmark, with 1,283 developmentally grounded prompts across 20 dimensions and robust inter-dimensional correlation analysis (Safety-Risk Prevention ρ=0.86). **Safe-Child-LLM** complements this with a focussed 200-prompt evaluation across two developmental stages, making it more practical for rapid evaluation. **EduGuardBench** offers a dual-component framework evaluating both teaching competence and adversarial robustness — a valuable recognition that safety and pedagogical quality must be assessed together.

**MinorBench** provides a critical reality check by grounding its evaluation in actual student usage patterns from a middle-school deployment. The **LLM Bias in Personalized Education** datasets — comprising over 17,000 educational explanations — introduce MAB and MDB metrics that could become standard fairness measures. **EduEval**, with its 11,000+ questions across Chinese K-12 materials organised by cognitive complexity, represents the most important non-English educational benchmark to date. The **AIG-ASAP dataset** remains essential for anyone working on academic integrity, demonstrating the fragility of current detection approaches.

## Methodological Trends

The field shows a clear evolution from descriptive risk taxonomies toward **actionable evaluation frameworks with measurable targets**. Red-teaming approaches using persona-based adversarial prompts have become standard, while participatory design — involving students and teachers in safety requirement specification — represents an encouraging methodological development.

Comparative evaluations across multiple LLM families on consistent benchmarks are increasingly common, enabling meaningful cross-model analysis. Researchers are employing mixed-methods designs combining quantitative metrics with qualitative stakeholder interviews, and automated evaluation using LLM-as-judge with human validation is being adopted for scalability. Production deployment case studies with real-world performance metrics are beginning to appear, though primarily from high-income country settings.

However, a significant methodological limitation persists: most benchmarks were developed and tested using models available in 2023–2024, including GPT-4 and Claude 2. Given the rapid pace of model development, findings may need re-evaluation with current-generation models. Cross-model generalisation — whether safety benchmarks predict performance on newer architectures — remains largely untested.

## Recommendations

We recommend that education funders and development partners **mandate longitudinal studies with actual learning outcome measures** — not just detection accuracy or attitude surveys — before supporting large-scale LLM deployment in schools. This is the single most important gap in the current evidence base.

The field should develop **cross-stakeholder safety frameworks** that involve students, teachers, parents, and marginalised communities in requirement specification — not just post-hoc evaluation. Building on the work of SproutBench and Safe-Child-LLM, we recommend establishing **explicit developmental stage calibration** requirements for any child-facing LLM, with measurable age-appropriateness targets validated by child development experts *(by December 2026)*.

We recommend investing in **standard protocols for detecting and mitigating cognitive offloading effects**, including metrics for independent problem-solving maintenance when AI assistance is available. Alongside this, teacher professional development programmes should focus on recognising when AI harms critical thinking, designing productive struggle opportunities, and balancing AI efficiency with learning depth.

For LMIC contexts specifically, the field should prioritise **fairness evaluation pipelines testing intersectional bias and cultural context** — not just single-attribute analysis. Mandatory pilot studies in diverse real-world classrooms — across different socioeconomic contexts, languages, and disability profiles — should be required before claiming educational effectiveness *(ongoing)*.

We aim to support the development of **open-source, community-maintained benchmarks** with transparent methodology and regular updates — framed as digital public goods — to track adversarial evolution and ensure accessibility for LMIC researchers. Clear accountability chains must be established: who is responsible when biased or harmful LLM output affects student learning or wellbeing?

## Key Papers

- **Pedagogy-driven Evaluation of Generative AI-powered Intelligent Tutoring Systems** — Comprehensive review grounded in learning science, providing empirical evidence of cognitive offloading concerns and proposing three research directions for unified evaluation methodologies.

- **SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth** — The most comprehensive child-safety benchmark (1,283 prompts, 20 dimensions), revealing the scaling paradox and introducing the Educational Transformation Effect as a new safety concept.

- **Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach** — Presents LearnLM-Tutor with seven diverse benchmarks spanning quantitative, qualitative, automatic, and human evaluation, deployed at scale at Arizona State University.

- **LLMs are Biased Teachers: Evaluating LLM Bias in Personalized Education** — First systematic evaluation of bias in the LLM-as-teacher role across nine models with 17,000+ explanations, identifying income and disability as highest-bias dimensions.

- **EduGuardBench: A Holistic Benchmark for Evaluating the Pedagogical Fidelity and Adversarial Safety of LLMs as Simulated Teachers** — Dual-component framework discovering that the safest models convert harmful requests into learning opportunities.

- **Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-LLM Interactions** — First developmental-stage-specific safety benchmark, revealing systematic age-appropriate response failures across major commercial models.

- **The Rise of Artificial Intelligence in Educational Measurement: Opportunities and Ethical Challenges** — Authoritative white paper from NCME establishing comprehensive ethical guidelines for automated scoring, item generation, and feedback systems.

- **AI in the Classroom: A Boon or a Threat to Pedagogical Practices?** — Direct empirical evidence from a sixth-grade classroom experiment measuring cognitive offloading effects, including reduced retention despite AI assistance.