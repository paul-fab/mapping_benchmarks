# Feedback with Reasoning: Evaluating AI Systems That Explain, Scaffold, and Guide K-12 Learning

## Executive Summary

The ability of AI tutoring systems to provide not just correct answers but meaningful explanations — identifying student misconceptions, scaffolding productive struggle, and generating actionable guidance — represents one of the most consequential frontiers in educational technology. Our analysis of **183 papers** in this category reveals a field undergoing a fundamental shift: from rule-based and template-driven feedback towards large language model (LLM)-powered systems capable of multi-turn dialogue, adaptive hint generation, and real-time pedagogical reasoning across subjects from elementary mathematics to high school essay writing.

The findings are both promising and sobering. On one hand, studies report meaningful learning gains — a **22.95% improvement in student outcomes** with personalised feedback in one study, and an effect size of **0.56** for immediate correctness feedback in another. Comprehensive benchmarks such as TutorBench, MRBench, and MathDial now enable systematic evaluation of AI tutoring quality across multiple pedagogical dimensions. On the other hand, even leading models fall well short of expert tutoring: GPT-4 — the dominant commercial model at the time most of these studies were conducted — achieved only **56% on expert-curated tutoring tasks** in TutorBench and was found to reveal solutions prematurely **66% of the time** in dialogue-based tutoring scenarios. The field has made striking progress in measuring what AI tutors can do in the moment, yet remains critically weak in assessing what matters most: whether AI-generated feedback with reasoning builds durable understanding, supports metacognitive development, or inadvertently fosters dependency.

Perhaps most concerning for LMIC contexts, only **8 of the 183 papers** examine non-English or cross-cultural feedback systems. The benchmarks, datasets, and evaluation frameworks that currently define quality in this space were overwhelmingly developed in high-income, English-language settings. This means the tools shaping global investment decisions about AI tutoring bear limited relevance to the linguistic, curricular, and infrastructural realities of the contexts where scalable, high-quality feedback is most urgently needed.

## Key Themes

### Multi-dimensional pedagogical evaluation frameworks

A defining trend across approximately **23 papers** is the move away from simple accuracy metrics towards comprehensive rubrics grounded in learning science. The **LearnLM-Tutor** project, developed by Google DeepMind and deployed at Arizona State University, presents what is arguably the most comprehensive evaluation framework to date — spanning seven benchmarks that combine quantitative performance, qualitative analysis, automatic scoring, and human evaluation. Alongside this, the **Unifying AI Tutor Evaluation** paper introduces MRBench, which annotates 1,596 tutor responses across eight pedagogical dimensions including mistake identification, guidance quality, and actionability. **TutorBench** provides 1,490 expert-curated samples with sample-specific rubrics, while **EduBench** offers the broadest coverage with 18,821 data points across nine educational scenarios.

These frameworks represent a significant maturation of the field. However, a key lesson here is that evaluation sophistication has outpaced deployment evidence. Most frameworks are validated against expert judgements in controlled settings rather than against actual student learning in classrooms.

### From error detection to adaptive scaffolding

Approximately **18 papers** focus on systems that automatically identify specific student misconceptions and generate targeted explanations. The **FEANEL** benchmark, for instance, annotates 1,000 K-12 English essays with fine-grained error classifications — revealing that LLMs achieve only **35.65% accuracy** on key sentence selection tasks. The **Bridge** dataset captures expert tutoring decision-making in 700 real mathematics conversations, demonstrating that GPT-4 responses improved by **76–88%** when guided by explicit expert decision frameworks. The **Stepwise Verification and Remediation** paper shows that grounding tutor responses in explicit error detection reduces hallucinations and improves feedback quality.

This theme highlights a persistent gap between what LLMs can generate and what effective tutoring requires. Generating plausible-sounding explanations is not the same as accurately diagnosing a student's specific misunderstanding and choosing the pedagogically appropriate response.

### Reinforcement learning for pedagogical alignment

A growing cluster of approximately **11 papers** applies reinforcement learning from human feedback (RLHF), direct preference optimisation (DPO), and related techniques to train LLMs towards pedagogically appropriate behaviour. The **Improving Validity of Automatically Generated Feedback via Reinforcement Learning** paper demonstrates that reward models incorporating both correctness and pedagogical alignment significantly outperform generic LLM outputs. **EduAlign** and the **Hierarchical Pedagogical Oversight** framework use multi-agent adversarial approaches to cultivate helpful yet appropriately restrained tutoring behaviour.

This is a methodologically important trend. It suggests that the gap between LLM capability and pedagogical quality is not fixed — it can be narrowed through careful alignment work. However, the pedagogical reward signals used in these systems are typically derived from expert annotations rather than from measured student learning outcomes, leaving open the question of whether aligned models actually produce better learning.

### Multilingual and cross-cultural feedback systems

Only **8 papers** in this corpus examine feedback quality across multiple languages or cultural contexts. Studies from **Indonesian classrooms**, **Brazilian high schools**, and **Chinese educational settings** (including EduEval) provide important but isolated evidence. The **Multilingual Performance Biases** paper documents systematic differences in LLM feedback quality across languages — a finding with profound implications for equitable deployment in LMICs.

This represents a critical gap. The overwhelming English-language focus of current benchmarks means the field lacks the evidence base needed to guide responsible AI tutoring deployment in the diverse linguistic environments that characterise most low- and middle-income education systems.

## What Is Being Measured

The field has developed an impressively granular measurement apparatus for immediate feedback quality. Researchers now routinely assess **accuracy of mistake identification and error localisation**, **clarity and relevance of explanations**, **scaffolding effectiveness** (whether hints guide without revealing solutions), and **alignment with pedagogical principles** such as Socratic dialogue, mastery learning, and Bloom's taxonomy.

Multi-dimensional scoring is becoming standard. Systems are evaluated for **helpfulness, coherence, tutor tone, and human-likeness** through structured rubrics. Specific failure modes are tracked — including **over-praise, answer-leaking, and hallucinations** — and avoidance of these failures is treated as a core quality criterion. Standard metrics include quadratic weighted kappa (QWK), Cohen's kappa, RMSE, and correlation with human raters for scoring accuracy, alongside BERTScore, BLEU, and cosine similarity for feedback text quality.

In programming education, researchers measure **hint effectiveness in enabling student self-correction** and distinguish between feedback that reveals solutions versus feedback that guides discovery. For essay assessment, **trait-specific scores** across dimensions such as organisation, coherence, grammar, and content are evaluated separately. Student-facing metrics include engagement proxies like **time-on-task, conversation length, and resubmission rates**, and increasingly, **student satisfaction ratings**.

What the field measures well, in short, is whether AI feedback looks and sounds like good tutoring. What it measures poorly is whether that feedback actually produces good learning.

## Critical Gaps

The most consequential gap is the near-complete absence of **long-term learning retention studies**. Across 183 papers, very few assess knowledge retention beyond immediate post-tests, and transfer to novel contexts is almost never examined. This means the field cannot currently distinguish between AI feedback that helps students answer today's question and AI feedback that builds durable conceptual understanding.

**Metacognitive development** — whether AI feedback helps students develop self-monitoring, self-explanation, and error-detection strategies — is almost entirely unmeasured. Closely related, **feedback utilisation patterns** receive scant attention: whether students actually read, process, and act on AI-generated explanations, or simply accept revisions passively, remains largely unknown. The **61A Bot** deployment study — one of the rare large-scale implementations with over **2,000 students and 100,000+ requests** — found that the AI assistant reduced homework time by **25–50%** but did not measure whether this impacted learning depth.

**Equity and bias** represent another critical blind spot. Systematic evaluation of whether AI feedback quality varies by student demographics, language background, prior knowledge, or socioeconomic status is largely absent. For an initiative focussed on LMICs, this gap is particularly concerning: the populations most likely to receive AI tutoring at scale are the least represented in the evaluation evidence.

**Cost-benefit analysis** barely features in the literature. Many effective systems depend on models like GPT-4 — which, while no longer the leading commercial model, was expensive at the time of these studies. Whether these approaches can be made affordable for under-resourced schools through fine-tuning of smaller open-source models, model compression, or hybrid architectures is an open and urgent question.

Finally, **ecological validity** remains weak. Most evaluations occur in controlled research settings rather than real classrooms, and the gap between laboratory performance and practical deployment impact is likely substantial.

## Cognitive Offloading

This category contains more substantive evidence on cognitive offloading than most areas we have reviewed, with approximately **22 papers** addressing the issue — yet the treatment remains largely descriptive rather than empirical. The findings are concerning. Bastani et al. (2025) report that **students performed worse when AI tools were removed after extended use**, suggesting genuine dependency formation. Macina et al. (2023) found that **ChatGPT reveals answers 66% of the time** in tutoring dialogues, directly undermining productive struggle. Perhaps most strikingly, Zhou et al. (2025) found that **83% of students using ChatGPT for essays could not recall any text they wrote**.

Several systems — notably **BEETLE II** and the **Virtual Physics System** — are explicitly designed to preserve cognitive engagement through Socratic questioning and withholding direct answers. The **Bridge** paper articulates the tension between guidance and discovery, showing tutors must carefully balance support against solution-leaking. Programming education researchers express particular concern, with the **Next-Step Hint Generation** paper explicitly discussing the "dangers of using LLMs as pair programmers", including overestimation of abilities and reduced learning.

However, most papers treat cognitive offloading as a **design constraint to work around** — through careful prompt engineering or system architecture — rather than as a phenomenon to rigorously measure and mitigate. No papers in this category include controlled experiments measuring actual cognitive skill development with versus without AI feedback. The **Designing Prompt Analytics Dashboard** paper notes concerns about "over-reliance on AI" and "undermining critical thinking" but focusses on detection rather than learning impact. This means the field is aware of the risk but has not yet developed the measurement infrastructure to address it systematically.

## Notable Benchmarks and Datasets

**MRBench** provides 192 mathematics tutoring conversations with 1,596 responses from seven tutors, annotated across eight pedagogical dimensions. It represents the first unified taxonomy with gold-standard human annotations enabling standardised comparison of pedagogical abilities across models.

**TutorBench** offers 1,490 expert-curated samples spanning three core tutoring skills — adaptive explanations, actionable feedback, and effective hints — for K-12 content. Its sample-specific rubrics and mixed-methods evaluation reveal that even leading models achieve only **56% overall**, establishing a clear ceiling for current capabilities.

**MathDial** is the largest dataset in this space, with **3,000 one-to-one tutoring dialogues** where human teachers guide LLM-simulated students through scaffolding questions. It demonstrates both how current models fail and how better tutors might be trained.

**EduBench** provides the broadest coverage with **18,821 data points** across nine educational scenarios, extending beyond tutoring to include grading, study planning, and psychological counselling. **SAS-Bench** contributes fine-grained evaluation of short answer scoring with 1,030 questions and 4,109 student responses. For writing, the **ASAP++ Dataset** provides trait-level annotations for argumentative essays, while **FeedEval** introduces the first framework specifically for evaluating the pedagogical quality of LLM-generated essay feedback.

Critically for LMIC relevance, the **Short Answer Feedback (SAF) Dataset** supports bilingual, multi-domain evaluation — but remains an exception. The overwhelming majority of benchmarks are English-only and developed in high-income educational contexts.

## Methodological Trends

The field has moved decisively from hand-crafted linguistic features and rule-based systems to **end-to-end deep learning with transformer-based architectures**. Most studies from 2023–2025 employ GPT-4, GPT-3.5, or Claude models — which were the leading commercial options during the study periods — alongside fine-tuned open-source alternatives such as Mistral-7B, LLaMA variants, and RoBERTa.

**Multi-agent architectures** — with separate student, tutor, and evaluator agents — have emerged as a dominant design pattern for both system development and evaluation. **Retrieval-Augmented Generation (RAG)** is increasingly used to ground feedback in course materials, rubrics, and past student work, improving specificity and reducing hallucinations.

A notable methodological tension exists between **scalable automated evaluation** (using metrics like BLEU, BERTScore, and LLM-as-judge) and **human pedagogical evaluation** (using structured rubrics rated by expert teachers). The **Beyond Agreement** paper offers a valuable critique, arguing that overreliance on inter-rater reliability as validation misses what matters educationally and proposing alternative approaches that prioritise learning impact over consensus.

**Synthetic data generation** — using LLMs to create student errors, misconceptions, and tutor responses at scale — is becoming common for training and evaluation. While this enables larger datasets, it raises questions about ecological validity that the field has not yet resolved.

Fine-tuning smaller open-source models on curated pedagogical datasets to match proprietary model performance represents an important trend for cost-sensitive deployment — and one with particular relevance for LMIC contexts where API costs may be prohibitive.

## Recommendations

We recommend that funders and development partners **prioritise investment in longitudinal learning measurement**. The field urgently needs studies that assess knowledge retention weeks and months after AI tutor use, including transfer to novel contexts. Current evaluations conflate short-term task success with durable learning, and this conflation risks directing resources towards systems that look effective but produce shallow understanding *(by end of 2026)*.

The field should **develop standardised protocols for measuring cognitive offloading**. This means creating assessment instruments that track whether students develop independence or dependency over time, including delayed assessments without AI support and measures of metacognitive strategy development. These protocols should be validated across age groups and subjects *(by mid-2027)*.

We recommend **establishing pedagogical alignment standards** that codify core principles — productive struggle thresholds, scaffolding gradients, appropriate hint levels — and develop automated metrics to detect violations. The existing benchmarks provide a strong foundation; what is needed now is consensus on minimum quality standards for deployment *(ongoing)*.

Investment in **multilingual and LMIC-representative benchmark datasets** is essential. Current benchmarks over-represent English-language mathematics in high-income settings. We recommend dedicated funding for benchmark development in low-resource languages, aligned with LMIC curricula and validated in authentic classroom contexts *(by end of 2027)*.

The field should **build cost-aware deployment pathways**. This means systematic comparison of fine-tuned open-source models against proprietary alternatives, with transparent reporting of computational costs alongside pedagogical quality metrics. For many LMIC contexts, a system that achieves 80% of GPT-4's feedback quality at 5% of the cost may be the more impactful investment.

Finally, we recommend that **feedback literacy be developed alongside feedback technology**. AI-generated feedback is only valuable if students can critically evaluate and effectively use it. Interventions teaching students how to engage with automated feedback — rather than accepting it uncritically or ignoring it entirely — should be a standard component of any deployment.

## Key Papers

- **LearnLM-Tutor** — The most comprehensive evaluation framework for AI tutoring, spanning seven benchmarks grounded in learning science with real-world deployment evidence from Arizona State University.

- **Unifying AI Tutor Evaluation: Taxonomy for Pedagogical Ability Assessment** — Introduces the first unified evaluation taxonomy (MRBench) with gold-standard human annotations across eight pedagogical dimensions, enabling standardised comparison across models.

- **MathDial: Dialogue Tutoring Dataset with Rich Pedagogical Properties** — The largest dataset (3,000 dialogues) of human teachers scaffolding LLM-simulated students, demonstrating that current models reveal solutions too early and provide incorrect feedback.

- **TutorBench: Benchmark To Assess Tutoring Capabilities** — Establishes that even leading models achieve only 56% on expert-curated K-12 tutoring tasks, quantifying the gap between current capability and effective tutoring.

- **Bridge: Novice-Expert Gap via Models of Decision-Making** — Translates expert tutoring decision-making into a framework for LLMs, showing GPT-4 responses improve by 76–88% when guided by expert reasoning.

- **Beyond Final Answers: Evaluating LLMs for Math Tutoring** — Reveals a concerning disconnect: 85.5% correct final answers but only 56.6% fully error-free tutoring dialogues, highlighting the need for holistic evaluation.

- **FeedEval: Pedagogically Aligned Evaluation of LLM-Generated Essay Feedback** — Introduces the first comprehensive framework for evaluating the pedagogical quality of AI-generated writing feedback with validated metrics.

- **Enhancing LLM-Based Feedback: Insights from Intelligent Tutoring Systems and the Learning Sciences** — A critical synthesis connecting 40 years of ITS research to modern LLM-based systems, providing theoretically grounded design principles.

- **61A Bot Report** — One of the few large-scale deployment studies (2,000+ students, 100,000+ requests), demonstrating time savings and reduced staff demands while revealing the gap in learning impact measurement.

- **Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation** — A methodologically important paper challenging overreliance on inter-rater reliability, proposing evaluation approaches that prioritise educational impact over consensus.