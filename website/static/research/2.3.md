# Pedagogical Interactions: How AI Tutoring Systems Engage Learners and Where They Fall Short

## Executive Summary

Pedagogical interactions — encompassing Socratic questioning, adaptive scaffolding, dialogue-based tutoring, and formative feedback — represent one of the most active and consequential areas of AI-in-education research. Our analysis covers **332 papers** examining how large language model (LLM)-powered systems engage K–12 and undergraduate learners in instructional dialogue, and the findings reveal a field grappling with a fundamental tension. While LLMs demonstrate impressive linguistic fluency and can generate plausible tutoring moves at scale, they frequently fail to replicate the **adaptive, theory-grounded pedagogical reasoning** that characterises effective human tutoring. Current systems tend toward cognitive shortcuts — revealing answers too early, providing overly direct feedback, and struggling to maintain coherent multi-turn scaffolding — rather than sustaining the guided questioning essential for deep learning.

Critically, the evidence on **cognitive offloading** is substantial and concerning. Across **52 papers** addressing the issue directly, researchers document that students using AI tutors without proper instructional framing develop problematic dependencies, accept AI outputs uncritically, and demonstrate reduced independent problem-solving ability when support is withdrawn. One large-scale study found that students using GPT-4 for homework solved **48% more practice problems but scored 17% lower on unassisted tests**, suggesting that AI reliance may actively impede the development of lasting competence. The most striking finding across this body of work is that most systems are not explicitly trained to maximise student learning outcomes — they are trained to mimic tutor utterances or follow surface-level pedagogical principles, leading to interactions that feel helpful but may be pedagogically suboptimal.

The research base is methodologically diverse, spanning randomised controlled trials (RCTs) in authentic classrooms, large-scale dialogue corpus analysis, reinforcement learning-based policy optimisation, and theory-driven benchmark development. However, significant gaps persist: the vast majority of studies measure **short-term performance** rather than long-term retention, domain transfer, or metacognitive development. Mathematics dominates as a subject domain, with limited coverage of humanities, creative reasoning, or open-ended inquiry. And evaluation remains heavily reliant on automated metrics or single-session experiments rather than longitudinal classroom deployments.

## Key Themes

### The gap between linguistic fluency and pedagogical competence

A central finding across **18 papers** is that generating human-like dialogue is not the same as teaching well. LLMs can produce fluent explanations, but they frequently fail to demonstrate adaptive pedagogical reasoning — the ability to diagnose a misconception in real time, calibrate the level of support, and make instructional decisions grounded in learning theory. *Problems With Large Language Models for Learner Modelling* demonstrates empirically that LLMs cannot replicate intelligent tutoring system (ITS) adaptivity even after fine-tuning, revealing fundamental limitations in **temporal coherence** and knowledge state tracking. Similarly, *Can Large Language Models Match Tutoring System Adaptivity?* shows that models such as GPT-4o — which was the leading commercial model at the time of the study — provide overly direct feedback instead of effective Socratic questioning when systematically tested through prompt variation.

The *Unifying AI Tutor Evaluation* taxonomy, which annotated **1,596 responses** across eight pedagogical dimensions, reveals that current systems score particularly poorly on **guidance provision** and **actionability** — the very qualities that distinguish effective tutoring from simple answer delivery. This gap between fluency and pedagogy has significant implications for deployment decisions, particularly in low- and middle-income countries (LMICs) where AI tutors may be positioned as substitutes for, rather than supplements to, qualified teachers.

### Answer leakage and the premature revelation of solutions

Across **15 papers**, researchers document a pervasive tendency for LLMs to reveal solutions too early or provide overly direct feedback, bypassing the sustained questioning and incremental scaffolding that characterise effective Socratic tutoring. The **MathDial** dataset — comprising 3,000 human tutor-student dialogues on maths word problems — demonstrates that current LLMs reveal solutions prematurely and fail to maintain questioning depth. *TutorBench*, which tested frontier models against expert-curated samples, found that **all models achieved less than 60% pass rates** on tutoring criteria, with answer leakage identified as a primary failure mode.

This is not merely a technical limitation — it reflects a deeper architectural challenge. LLMs are optimised for helpfulness and user satisfaction, objectives that are fundamentally misaligned with pedagogical goals requiring productive struggle and desirable difficulty. *MWPTutor meets Large Language Models* demonstrates that combining finite state transducers — providing pedagogical structure — with LLMs — providing flexibility — outperforms free-form GPT-4, offering a blueprint for controllable pedagogical AI that resists the pull toward premature solution delivery.

### Reinforcement learning and theory-driven optimisation

A promising methodological trend involves using **reinforcement learning (RL)** to optimise teaching strategies based on pedagogical reward functions rather than task accuracy alone. Across **10 papers**, researchers apply techniques including PPO, DQN, and multi-armed bandits to align LLM behaviour with learning science principles. *Rewarding How Models Think Pedagogically* introduces a novel approach that rewards pedagogical reasoning processes grounded in Pólya's problem-solving framework, demonstrating how to align LLM internal reasoning with educational theory. *Multi-Armed Bandits for Intelligent Tutoring Systems* reports a **51% normalised learning gain** using adaptive algorithms that personalise exercise sequences based on learning progress.

However, a key lesson here is that reward function design matters enormously. Systems optimised for short-term task completion may inadvertently reinforce the very cognitive shortcuts that undermine deep learning. *Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues* represents an important advance — it is the first work to train tutors using **student outcome prediction** via a simulated student model, rather than human behaviour mimicry, achieving significant improvements in pedagogical quality.

### Domain-specific challenges and the dominance of mathematics

The research base is heavily concentrated in **mathematics education**, with **13 papers** explicitly addressing domain-specific pedagogical challenges. LLMs perform better in well-structured domains where problems have clear solution paths and assessment is straightforward. However, they struggle with open-ended creative tasks, essay-based reasoning, and subjects requiring nuanced interpretation. *Beyond Final Answers* reveals a critical gap: while models achieve over **85% final answer accuracy** in maths tutoring, full dialogue correctness — maintaining pedagogical coherence across an entire conversation — drops to just **56%**. This distinction matters because pedagogical quality is not about whether the system eventually reaches the right answer but about whether the journey builds understanding.

### Human-AI hybrid architectures

Across **10 papers**, the most successful deployments combine LLM generation with human oversight, expert supervision, or structured knowledge bases. *AI tutoring can safely and effectively support students*, an exploratory RCT in UK classrooms, demonstrates that carefully designed hybrid systems can deliver meaningful learning gains. The *CLASS* design framework operationalises learning science principles — scaffolding, Socratic dialogue, problem decomposition — into practical LLM-based tutor design, while maintaining architectural constraints that prevent harmful pedagogical behaviours. This theme underscores an important point for policymakers: **full automation of tutoring is neither desirable nor achievable with current technology**, and investment should focus on systems that augment teacher capacity rather than replace it.

## What Is Being Measured

The field employs a wide array of metrics spanning learning outcomes, interaction quality, and system performance. **Learning gains** are most commonly assessed through pre/post-test designs, with some studies measuring normalised learning gain and effect sizes. **Dialogue quality** metrics include turn count, student talk time, teacher-to-student utterance ratio, and scaffolding fidelity. Researchers increasingly evaluate **answer leakage rates** — how often systems directly provide solutions — and **pedagogical alignment scores** based on expert ratings or validated rubrics.

**Knowledge tracing accuracy** is measured through prediction of student performance (using AUC, F1, and RMSE), with models ranging from traditional Bayesian knowledge tracing to deep learning approaches. More nuanced assessments capture **critical thinking skill development** mapped to Bloom's taxonomy levels, **metacognitive skill indicators** including planning, monitoring, and evaluation behaviours, and **student engagement** through completion rates, time-on-task, and voluntary interaction frequency. Several benchmarks now assess **Socratic questioning quality**, **hint specificity**, and **error diagnosis accuracy** — representing a welcome shift from surface-level metrics toward pedagogically meaningful evaluation.

## Critical Gaps

The most significant gap across this body of research is the near-total absence of **long-term retention and transfer studies**. The vast majority of evaluations rely on immediate post-tests, with very few measuring knowledge weeks or months later. This represents a critical blind spot, particularly given the cognitive offloading evidence suggesting that short-term gains may mask shallow learning.

**Metacognitive development** — whether students become better self-regulated learners through AI tutoring — is rarely measured systematically. Related to this, few studies track the **development trajectory of independent problem-solving** when AI support is gradually withdrawn. The field lacks standardised measures of **productive struggle** — quantifying the optimal level of difficulty that promotes learning without causing unproductive frustration.

**Equity analysis** is largely absent. Very few studies disaggregate results by socioeconomic status, language background, disability, or prior knowledge level, making it impossible to determine whether AI tutors narrow or widen achievement gaps — a question of paramount importance for LMIC deployment. **Cost-effectiveness analyses** comparing AI tutoring to human alternatives are similarly rare, despite being essential for investment decisions. Teacher professional development outcomes, classroom dynamic shifts, and the impact on teacher-student relationships when AI tutors are introduced remain largely unexplored.

## Cognitive Offloading

This category contains one of the most substantial evidence bases on cognitive offloading in AI-for-education research, with **52 papers** addressing the issue directly. The findings are consistent and concerning.

Students routinely accept AI-generated solutions without verification, even when they possess sufficient domain knowledge to detect errors. In physics education, **42% of students** used copy-paste strategies with ChatGPT, and nearly half failed to recognise incorrect AI solutions. A study of 299 STEM students found that trust-driven routine AI use predicted a **66% reduction in reflection**, with students of higher tech-affinity — counterintuitively — being **most vulnerable** to cognitive disengagement. Brain imaging studies show reduced neural connectivity and metacognitive monitoring in students using LLMs for essay writing compared to unaided work.

The "**Zone of No Development**" concept, introduced in one study, captures the risk that permanent AI mediation replaces productive cognitive struggle, creating a state where learners never operate independently within their zone of proximal development. Alarmingly, **short-term AI literacy interventions** — such as educational text about ChatGPT's limitations — **failed to reduce over-reliance** on incorrect suggestions and actually increased rejection of correct recommendations.

Several promising mitigations emerge from the literature. The **teachable agent paradigm** — where students teach an AI rather than being taught — reverses the dynamic that enables passive consumption. **Erroneous examples** show superior delayed retention (d = 0.33), suggesting deliberate difficulty prevents offloading. The **Extraheric AI** framework proposes that AI should pose questions and alternatives rather than provide direct answers. **Progressive autonomy** approaches — fading AI support as competence develops — show theoretical promise but require further empirical validation. These mitigations should be considered essential design requirements for any AI tutoring system intended for educational deployment, particularly in contexts where teacher oversight may be limited.

## Notable Benchmarks and Datasets

Several purpose-built benchmarks have emerged that represent significant advances in how the field evaluates pedagogical quality. **MathDial** provides 3,000 human tutor-student dialogues with annotations for scaffolding, mistake diagnosis, and Socratic questioning fidelity — the first large-scale dataset designed explicitly to evaluate pedagogical quality rather than mere correctness. **TutorBench** offers 1,490 expert-curated samples with sample-specific rubrics for adaptive explanations, actionable feedback, and hint generation, and its finding that all frontier models achieve below 60% pass rates provides a sobering baseline.

**FoundationalASSIST** — containing **1.7 million K–12 maths interactions** with full question text, student responses, and Common Core alignment — is the first English dataset enabling cognitive student modelling research with LLMs. Its demonstration that LLMs barely exceed trivial baselines on knowledge tracing is a critical finding. **MetaCLASS** introduces 11 interpretable coach moves aligned to self-regulated learning, uniquely including "no intervention" as a productive restraint action — revealing that LLMs exhibit **compulsive intervention bias**, failing to recognise when silence is pedagogically optimal.

The **ASSISTments datasets** (2009, 2012, 2015, 2017) remain the most widely used benchmarks for knowledge tracing, enabling fair comparison across methods. **ConvoLearn** provides theory-grounded annotations across six pedagogical dimensions rooted in constructivist principles. **MalAlgoQA** is the first benchmark systematically evaluating LLMs' ability to understand student misconceptions through counterfactual reasoning. The **Polygence dataset** — comprising **100,000 hours** of authentic one-on-one tutoring interactions across 150+ subjects — represents an unprecedented scale of real longitudinal tutoring data.

## Methodological Trends

The field shows a clear shift from rule-based intelligent tutoring systems toward **LLM-powered conversational agents**, with increasing methodological sophistication. Controlled classroom experiments with pre/post-test designs remain common, but researchers increasingly supplement these with large-scale dialogue corpus analysis using **LLM-as-judge** approaches combined with human expert validation. Multi-agent simulation frameworks — where LLMs role-play both tutor and student — enable scalable evaluation, though *SimStudent* raises important concerns about whether such simulations capture authentic learner behaviour with sufficient fidelity.

Theory-driven rubric development grounded in learning science principles — Bloom's taxonomy, the Socratic method, scaffolding theory, and Vygotsky's zone of proximal development (ZPD) — represents a welcome maturation of evaluation practice. Hybrid symbolic-neural approaches, combining interpretable rules or knowledge graphs with neural generation, are emerging as a strategy for maintaining pedagogical control while leveraging LLM flexibility. Knowledge tracing has evolved from Bayesian models through deep learning (DKT, SAINT) to transformer-based architectures, with graph neural networks increasingly used to model prerequisite structures.

A notable limitation is the prevalence of **small-scale, single-session studies** rather than longitudinal classroom deployments. Many evaluations rely on automated metrics (BLEU, BERTScore, ROUGE) that have limited correlation with pedagogical quality, though the trend toward expert educator ratings and multi-dimensional rubrics is encouraging.

## Recommendations

We recommend that the field **prioritise training objectives that directly optimise for student learning outcomes** rather than mimicking human tutor surface behaviours or maximising user satisfaction. The evidence is clear that systems trained to imitate tutors may learn their superficial patterns without acquiring their pedagogical reasoning.

Funders and developers should **invest in longitudinal studies measuring learning trajectories when AI support is gradually withdrawn** *(by 2027)*. The current evidence base cannot answer the most important question: do AI tutoring systems develop genuine competence or AI dependency? Studies should include delayed post-tests at intervals of weeks and months, not just immediate assessments.

We recommend that AI tutoring systems be designed with **explicit architectural constraints against answer leakage** and premature solution revelation. Systems should enforce sustained Socratic questioning before providing direct answers, and progressive autonomy mechanisms should be built into system design from the outset — not treated as optional features.

The field should **develop standardised, multi-dimensional evaluation protocols** that assess dialogue quality, scaffolding fidelity, student agency, and cognitive engagement alongside correctness metrics. The benchmarks emerging from MathDial, TutorBench, and MetaCLASS provide foundations, but open-source, community-maintained benchmark suites are needed *(ongoing)*.

For LMIC contexts specifically, we recommend **equity-focussed evaluation frameworks** that disaggregate results by language, socioeconomic background, prior knowledge, and access to teacher support. Investment in AI tutoring systems without understanding differential impacts risks widening rather than narrowing educational inequalities.

Finally, deployment should follow a **hybrid human-AI model** where expert teachers maintain oversight, particularly for pedagogically complex moments including misconception diagnosis, scaffolding withdrawal decisions, and affective support. Full automation of tutoring is not supported by the current evidence base, and systems should be designed to augment teacher capacity in contexts where qualified educators are scarce — not to replace them.

## Key Papers

- **Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach** — Establishes a comprehensive evaluation framework with seven benchmarks for LearnLM deployed at scale; sets the methodological gold standard for theory-grounded AI tutor assessment.

- **Problems With Large Language Models for Learner Modelling** — Demonstrates empirically that LLMs cannot replicate ITS adaptivity even after fine-tuning, revealing fundamental limitations in temporal coherence and knowledge state tracking.

- **MathDial** — Provides the first large-scale pedagogically rich tutoring dialogue dataset with scaffolding annotations; shows current LLMs reveal solutions prematurely and fail to maintain Socratic questioning depth.

- **Training LLM-based Tutors to Improve Student Learning Outcomes in Dialogues** — First work to train tutors using student outcome prediction rather than human behaviour mimicry, achieving significant improvements in pedagogical quality.

- **GPT-4 as a Homework Tutor can Improve Student Engagement and Learning Outcomes** — First large-scale RCT showing AI tutoring improves engagement but documents serious concern: students scored 17% lower on unassisted tests, suggesting harmful over-reliance.

- **MetaCLASS: Metacognitive Coaching for Learning with Adaptive Self-regulation Support** — Reveals that LLMs exhibit compulsive intervention bias, failing to recognise when silence is pedagogically optimal; introduces productive restraint as a tutoring action.

- **ChatGPT produces more 'lazy' thinkers** — First experimental demonstration that AI-assisted academic work significantly reduces deep cognitive engagement across multiple dimensions, with large effect sizes.

- **The Unspoken Crisis of Learning: The Surging Zone of No Development** — Introduces the concept of permanent AI mediation replacing productive struggle, with a framework demonstrating how deliberate disconnection restores learner agency.

- **Unreflected Acceptance** — Empirically demonstrates cognitive offloading risks in physics education, with 42% copy-paste usage and 57% false-positive rate on incorrect AI solutions.

- **TeachLM: Post-Training LLMs for Education Using Authentic Learning Data** — Demonstrates that fine-tuning on 100,000 hours of real tutoring data doubles student talk time and significantly improves pedagogical quality, establishing the importance of authentic learning data.